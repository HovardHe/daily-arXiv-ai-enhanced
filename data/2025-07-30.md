<div id=toc></div>

# Table of Contents

- [math.ST](#math.ST) [Total: 6]
- [math.OC](#math.OC) [Total: 16]
- [math.NT](#math.NT) [Total: 9]
- [math.LO](#math.LO) [Total: 4]
- [math.CO](#math.CO) [Total: 17]
- [cs.CR](#cs.CR) [Total: 44]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.DM](#cs.DM) [Total: 2]


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1] [False discovery rate control with compound p-values](https://arxiv.org/abs/2507.21465)
*Rina Foygel Barber,Richard J Samworth*

Main category: math.ST

TL;DR: 本文研究了在多重检验中使用复合p值时，Benjamini-Hochberg程序的错误发现率(FDR)控制特性。结果表明，在独立性条件下FDR上限为$1.93\alpha$，而在所有零假设均为真时上限可改进为$\alpha + 2\alpha^2$。但在正相关条件下，FDR可能膨胀$O(\log m)$倍。


<details>
  <summary>Details</summary>
Motivation: 复合p值通过仅要求真实零假设的p值在平均意义上满足超均匀性，为多重检验提供了更灵活的框架。研究其在Benjamini-Hochberg程序下的FDR控制特性具有重要理论价值。

Method: 通过理论分析研究复合p值在Benjamini-Hochberg程序下的表现，分别考察独立性、全零假设成立和正相关三种情况下的FDR控制特性。

Result: 在独立性下FDR上限为$1.93\alpha$（存在$\frac{7}{6}\alpha$的实例）；全零假设时上限改进为$\alpha + 2\alpha^2$（下限$\alpha + \alpha^2/4$）；正相关时FDR可能膨胀$O(\log m)$倍。

Conclusion: 复合p值在多重检验中具有实用价值，但其FDR控制特性依赖于数据依赖结构。它为信息不足或追求更高检验功效的场景提供了新思路。

Abstract: In the setting of multiple testing, compound p-values generalize p-values by
asking for superuniformity to hold only \emph{on average} across all true
nulls. We study the properties of the Benjamini--Hochberg procedure applied to
compound p-values. Under independence, we show that the false discovery rate
(FDR) is at most $1.93\alpha$, where $\alpha$ is the nominal level, and exhibit
a distribution for which the FDR is $\frac{7}{6}\alpha$. If additionally all
nulls are true, then the upper bound can be improved to $\alpha + 2\alpha^2$,
with a corresponding worst-case lower bound of $\alpha + \alpha^2/4$. Under
positive dependence, on the other hand, we demonstrate that FDR can be inflated
by a factor of $O(\log m)$, where~$m$ is the number of hypotheses. We provide
numerous examples of settings where compound p-values arise in practice, either
because we lack sufficient information to compute non-trivial p-values, or to
facilitate a more powerful analysis.

</details>


### [2] [Exact Distribution of the Noncentral Complex Roy's Largest Root Statistic via Pieri's Formula](https://arxiv.org/abs/2507.21508)
*Koki Shimizu,Hiroki Hashiguchi*

Main category: math.ST

TL;DR: 本研究推导了非中心复数Roy最大根统计量的精确分布与矩，通过复数区域多项式乘积表达，并应用组合数学中的Pieri公式计算线性化系数，最终用于复数多元方差分析(MANOVA)的检验效能计算。


<details>
  <summary>Details</summary>
Motivation: 旨在解决复数域中Roy最大根统计量在特定备择假设下的分布问题，为复数多元方差分析提供理论支持。

Method: 采用复数区域多项式乘积表示统计量分布，利用组合数学中的Pieri公式显式计算多项式乘积产生的线性化系数。

Result: 成功推导出非中心复数Roy最大根统计量的精确分布与矩，并实现复数MANOVA检验效能的量化计算。

Conclusion: 该研究为复数多元统计推断提供了新的理论工具，特别在MANOVA检验效能计算方面具有重要应用价值。

Abstract: In this study, we derive the exact distribution and moment of the noncentral
complex Roy's largest root statistic, expressed as a product of complex zonal
polynomials. We show that the linearization coefficients arising from the
product of complex zonal polynomials in the distribution of Roy's test under a
specific alternative hypothesis can be explicitly computed using Pieri's
formula, a well-known result in combinatorics. These results were then applied
to compute the power of tests in the complex multivariate analysis of variance
(MANOVA).

</details>


### [3] [Factorization by extremal privacy mechanisms: new insights into efficiency](https://arxiv.org/abs/2507.21769)
*Chiara Amorino,Arnaud Gloter*

Main category: math.ST

TL;DR: 本文研究了离散和连续设置下$\alpha$局部差分隐私($\alpha$ LDP)的效率问题，通过分解引理将隐私机制分解为极值机制与额外随机化，利用凸分析和泛函分析工具建立了高隐私 regime 下Fisher信息的上下界匹配，并设计出针对均匀分布参数的高效估计器。


<details>
  <summary>Details</summary>
Motivation: 探讨在$\alpha$局部差分隐私约束下如何实现统计效率最大化，特别是在高隐私保护需求($\alpha\to 0$)场景下的最优机制设计问题。

Method: 基于分解引理将任意隐私机制分解为极值机制与随机化步骤，利用Choquet定理等工具在无限维空间中进行极值机制搜索，将Fisher信息最大化问题转化为凸优化问题。

Result: 在高隐私 regime ($\alpha\to 0$)下建立了Fisher信息的紧致上下界，证明了对任意$\alpha$解的存在性，并针对均匀分布$[0,\theta]$参数估计问题设计了具有一致性和渐近有效性的极值机制。

Conclusion: 理论分析和数值实验表明，通过极值机制分解方法可在$\alpha$ LDP框架下实现统计最优性，为高隐私需求场景提供了可行的解决方案。

Abstract: We study the problem of efficiency under $\alpha$ local differential privacy
($\alpha$ LDP) in both discrete and continuous settings. Building on a
factorization lemma, which shows that any privacy mechanism can be decomposed
into an extremal mechanism followed by additional randomization, we reduce the
Fisher information maximization problem to a search over extremal mechanisms.
The representation of extremal mechanisms requires working in infinite
dimensional spaces and invokes advanced tools from convex and functional
analysis, such as Choquet's theorem. Our analysis establishes matching upper
and lower bounds on the Fisher information in the high privacy regime ($\alpha
\to 0$), and proves that the maximization problem always admits a solution for
any $\alpha$. As a concrete application, we consider the problem of estimating
the parameter of a uniform distribution on $[0, \theta]$ under $\alpha$ LDP.
Guided by our theoretical findings, we design an extremal mechanism that yields
a consistent and asymptotically efficient estimator in high privacy regime.
Numerical experiments confirm our theoretical results.

</details>


### [4] [Alternating Bregman projections and convergence of the EM algorithm](https://arxiv.org/abs/2507.21840)
*Dominikus Noll*

Main category: math.ST

TL;DR: 研究非凸集间交替Bregman投影的收敛性，证明其可收敛至交集点或实现集合间间隙的点，收敛速度一般为次线性，但在横截性条件下可为线性，并应用于非凸参数集期望最大化算法的收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 探讨非凸集间交替Bregman投影的收敛行为，扩展传统凸集投影理论的应用范围，并为非凸参数集的期望最大化算法提供理论支持。

Method: 通过分析交替Bregman投影在非凸集上的行为，结合横截性条件，研究其收敛性质及速度，并将结论应用于期望最大化算法的收敛性证明。

Result: 证明交替Bregman投影在非凸集上可收敛至交集点或间隙点，收敛速度一般为次线性，横截性条件下可达到线性，且该结论适用于非凸参数集的期望最大化算法。

Conclusion: 非凸集上的交替Bregman投影具有收敛性，其速度取决于集合性质，该理论为相关优化算法（如期望最大化）在非凸场景下的应用提供了理论基础。

Abstract: We investigate convergence of alternating Bregman projections between
non-convex sets and prove convergence to a point in the intersection, or to
points realizing a gap between the two sets. The speed of convergence is
generally sub-linear, but may be linear under transversality. We apply our
analysis to prove convergence of versions of the expectation maximization
algorithm for non-convex parameter sets.

</details>


### [5] [New (and old) predictive schemes with a.c.i.d. sequences](https://arxiv.org/abs/2507.21874)
*Marco Battiston,Lorenzo Cappello*

Main category: math.ST

TL;DR: 本文提出了一种放宽贝叶斯推断中预测规则限制的新方法，基于广义鞅理论，允许使用更广泛的预测规则，包括核估计器和参数化贝叶斯自举等。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯推断通常需要指定模型和先验，而本文旨在探索仅依赖预测规则的方法，以简化推断过程并扩展其适用性。

Method: 通过推广鞅理论，放宽了预测规则的限制条件，使得核估计器、参数化贝叶斯自举和基于copula的算法等更多预测规则可用于推断。

Result: 研究结果表明，广义鞅理论为使用更广泛的预测规则提供了理论基础，从而扩展了贝叶斯推断的适用范围。

Conclusion: 本文展示了使用更广泛预测规则的潜在优势，为贝叶斯推断提供了更多灵活性和可能性，而非推崇某一特定规则。

Abstract: There is a growing interest in procedures for Bayesian inference that bypass
the need to specify a model and prior but simply rely on a predictive rule that
describes how we learn on future observations given the available ones. At the
heart of the idea is a bootstrap-type scheme that allows us to move from the
realm of prediction to that of inference. Which conditions the predictive rule
needs to satisfy to produce valid inference is a key question. In this work, we
substantially relax previous assumptions building on a generalization of
martingales, opening up the possibility of employing a much wider range of
predictive rules that were previously ruled out. These include ``old" ideas in
Statistics and Learning Theory, such as kernel estimators, and more novel ones,
such as the parametric Bayesian bootstrap or copula-based algorithms. Our aim
is not to advocate in favor of one predictive rule versus the other ones, but
rather to showcase the benefits of working with this larger class of predictive
rules.

</details>


### [6] [Divergence and Model Adequacy, A Semiparametric Case Study](https://arxiv.org/abs/2507.21878)
*Michel Broniatowski,Justin Moutsouka*

Main category: math.ST

TL;DR: 本文探讨了基于散度的推断方法在平滑半参数模型下的适用性，提出了确保问题适定性和估计一致性的条件，并讨论了散度选择的理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在扩展经典参数推断至半参数模型，同时估计兴趣参数和冗余参数，探讨不同散度选择的理论基础。

Method: 在矩约束条件下，分析基于散度的推断方法对平滑半参数模型的适用性，并通过模拟研究验证方法。

Result: 提出了确保推断工具适定性和估计一致性的条件，支持$L_2$和Kullback-Leibler散度的普适性选择，并推广了[5]中定义的幂散度类。

Conclusion: 研究表明，特定散度选择在半参数模型中具有理论优势，为实际应用提供了可行的推断框架。

Abstract: Adequacy for estimation between an inferential method and a model can be
de{\ldots}ned through two main requirements: {\ldots}rstly the inferential tool
should de{\ldots}ne a well posed problem when applied to the model; secondly
the resulting statistical procedure should produce consistent estimators.
Conditions which entail these analytical and statistical issues are considered
in the context when divergence based inference is applied for smooth
semiparametric models under moment restrictions. A discussion is also held on
the choice of the divergence, extending the classical parametric inference to
the estimation of both parameters of interest and of nuisance. Arguments in
favor of the omnibus choice of the L 2 and Kullback Leibler choices as
presented in [16] are discussed and motivation for the class of power
divergences de{\ldots}ned in [5] is presented in the context of the present
semi parametric smooth models. A short simulation study illustrates the method.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [7] [Sliding Mode Control for Uncertain Systems with Time-Varying Delays via Predictor Feedback and Super-Twisting Observer](https://arxiv.org/abs/2507.21281)
*Hardy Pinto,Tiago Roux Oliveira,Liu Hsu*

Main category: math.OC

TL;DR: 本文提出了一种针对受已知时变测量延迟和匹配未知非线性扰动（可能包含执行器故障）影响的线性时不变系统的稳定控制策略。该方法结合了开环预测器和基于超螺旋算法的状态观测器，以补偿延迟并估计未测量的状态分量。


<details>
  <summary>Details</summary>
Motivation: 针对部分状态向量无法实时测量且存在时变延迟和非线性扰动（包括执行器故障）的系统，需要一种鲁棒的控制策略来确保全局稳定性。

Method: 采用开环预测器与基于超螺旋算法的状态观测器相结合的方法，补偿延迟并估计未测量状态分量；同时设计滑模控制律，确保在扰动、未建模干扰、参数不确定性和延迟下的全局稳定。

Result: 数值仿真表明，所提出的方法能够有效补偿延迟并估计未测量状态，同时在广泛的扰动和不确定性下实现全局稳定。

Conclusion: 该方法通过结合超螺旋观测器和滑模控制，为受时变延迟和非线性扰动的系统提供了一种鲁棒的稳定控制策略，具有广泛的应用潜力。

Abstract: This paper introduces a novel stabilization control strategy for linear
time-invariant systems affected by known time-varying measurement delays and
matched unknown nonlinear disturbances, which may encompass actuator faults. It
is considered that part of the state vector is not available for real-time
measurement. To address this, the proposed approach combines an open-loop
predictor with a state observer designed using the Super-Twisting Algorithm,
aiming to compensate for the delays and estimate the unmeasured state
components. Specifically, the nonlinear observer-based framework enables the
reconstruction of unmodeled fault signals without assuming that they originate
from a known exogenous system, offering robustness against parametric
uncertainties. Meanwhile, the predictor forwards the delayed output in time.
Subsequently, a sliding mode control law is formulated to enforce an ideal
sliding mode and ensure global stabilization, even under a broader class of
perturbations, unmodeled disturbances, parametric uncertainties, and delays,
owing to the integration of the Super-Twisting observer. Numerical simulations
illustrate the efficiency of the proposed approach.

</details>


### [8] [Optimal Impulsive Control of Cislunar Relative Motion using Reachable Set Theory](https://arxiv.org/abs/2507.21425)
*Matthew Hunter,Walter J. Manuel,Simone D'Amico*

Main category: math.OC

TL;DR: 本研究首次将先进的Koenig-D'Amico可达集理论求解器应用于地月空间混沌相对运动的圆形限制性三体问题（CR3BP），提出了一种高效的最优脉冲控制方法，并通过MPC架构提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为解决地月空间分布式航天器系统在CR3BP混沌环境中的安全高效运行问题，需开发计算高效的最优控制方法。

Method: 将主从航天器的相对运动建模为线性时变（LTV）系统，结合Koenig-D'Amico求解器生成最优脉冲控制方案，并采用模型预测控制（MPC）架构抑制误差。

Result: 通过单元测试、蒙特卡洛仿真及基线模型对比验证，该方法在不同CR3BP轨道和控制窗口下均表现出鲁棒性，计算效率满足星载需求。

Conclusion: 该研究为地月空间航天器集群提供了具有星载计算可行性的最优控制框架，实现了安全、精准且高效的相对运动控制。

Abstract: This work presents the first application of the state-of-the-art
Koenig-D'Amico reachable set theory solver to cislunar, chaotic relative motion
in the Circular-Restricted Three-Body Problem (CR3BP). The relative motion
dynamics of two spacecraft, a chief and a deputy, in the CR3BP are formulated
as a Linear Time-Variant (LTV) system, allowing the solver to find an optimal
impulsive control maneuver plan. This methodology demonstrates robust and
accurate control performance for both small and large reconfigurations over
different CR3BP orbits and control windows. These capabilities are enhanced by
a Model Predictive Control (MPC) architecture to reject all sources of control,
navigation, and dynamic error. The performance of the proposed approach is
validated by unit testing, Monte Carlo simulations, and comparisons to baseline
models for spacecraft relative motion. Overall, this work demonstrates an
optimal control methodology with the computational efficiency to be used
on-board spacecraft, enabling the safe, effective, and efficient operation of
Distributed Space Systems in cislunar space.

</details>


### [9] [A Bi-Objective Mathematical Model for the Multi-Skilled Resource-Constrained Project Scheduling Problem Considering Reliability: An AUGMECON2VIKOR Hybrid Method](https://arxiv.org/abs/2507.21436)
*Mohammad Ghasemi,Asef Nazari,Dhananjay Thiruvady,Reza Tavakkoli-Moghaddam,Reza Shahabi-Shahmiri,Seyed-Ali Mirnezami*

Main category: math.OC

TL;DR: 本文提出了一种考虑可靠性的多技能资源受限项目调度问题（MSRCPSPR）新模型，结合改进的epsilon约束法和增强版VIKOR算法（AUGMECON2VIKOR），在PSPLIB基准实例中实现了项目工期与成本分别降低2.55%和2.80%，计算时间减少543秒。


<details>
  <summary>Details</summary>
Motivation: 当前制造业项目中中断率虽可准确预估，但其对项目工期与成本的影响仍需深入研究，因此需要建立融合可靠性的MSRCPSP扩展模型。

Method: 采用改进版epsilon约束法与增强型VIKOR算法（AUGMECON2VIKOR）求解含可靠性约束的NP难问题，并在PSPLIB的j10/j20实例上进行验证。

Result: 新方法使j10实例平均项目工期缩短2.55%、成本降低2.80%，相比传统epsilon约束法节省543秒计算时间。敏感性分析揭示了可靠约束对目标的积极影响。

Conclusion: 所提MSRCPSPR模型及AUGMECON2VIKOR算法能有效解决含可靠性约束的复杂调度问题，显著优化项目工期与成本，为NP难问题提供了高效求解方案。

Abstract: In recent years, resources with multiple skills have received attention as an
extension of the resource-constrained project scheduling problem known as
MSRCPSP. Although the disruption rate is well-estimated in today's
manufacturing projects, its impact on project makespan and cost need further
investigation. Hence, this study presents a novel mathematical model for the
MSRCPSP considering reliability, namely MSRCPSPR. The model proposes both
objectives of minimizing project makespan and project cost. The MSRCPSP is an
NP-hard problem, and including reliability constraints, as proposed in this
paper, makes solving the problem more intractable. To cope with the
computational challenges of solving the problem, a combination of an enhanced
version of the epsilon-constraint method as well as an augmented version of the
VIKOR algorithm, namely AUGMECON2VIKOR, is employed to solve benchmark
instances j10 and j20 from the PSPLIB. A comparative analysis demonstrates the
performance of the proposed method, and the sensitivity analysis represents the
effects of positive reliable constraints on the objective functions. Employing
the proposed method, the project makespan and cost are reduced by nearly 2.55%
and 2.80% in j10 on average. CPU time is also decreased by about 543 seconds in
comparison to the epsilon-constraint method.

</details>


### [10] [Second-order sequential optimality conditions for nonlinear semidefinite optimization problems](https://arxiv.org/abs/2507.21495)
*Huimin Li,Yuya Yamakawa,Ellen H. Fukuda*

Main category: math.OC

TL;DR: 本文提出了非线性半定优化问题(NSDP)的二阶近似KKT条件(AKKT2)及其强化版本(CAKKT2)，证明了它们是无约束条件下局部最优的必要条件，并在特定约束规范下推导出弱二阶必要性，同时设计了满足这些条件的罚函数算法。


<details>
  <summary>Details</summary>
Motivation: 序列最优性条件在约束优化中至关重要，因其无需约束规范(CQs)即可提供必要性条件。本研究旨在扩展经典AKKT至二阶情形，为NSDP问题建立更全面的最优性理论框架。

Method: 通过数学分析严格定义AKKT2和CAKKT2条件，在Robinson约束规范和弱常数秩假设下建立与弱二阶必要条件的关联，并构造罚函数算法生成满足目标条件的收敛序列。

Result: 理论证明：1) AKKT2/CAKKT2是无约束的局部最优必要条件；2) 在特定CQs下AKKT2蕴含弱二阶必要性；3) 所提算法产生的聚点必然满足AKKT2和CAKKT2条件。

Conclusion: 该研究系统建立了NSDP问题的二阶序列最优性理论，提出的算法框架为求解非凸NSDP问题提供了新的理论保证和计算工具，弥补了一阶AKKT在二阶分析中的空白。

Abstract: Sequential optimality conditions play an important role in constrained
optimization since they provide necessary conditions without requiring
constraint qualifications (CQs). This paper introduces a second-order extension
of the Approximate Karush-Kuhn-Tucker (AKKT) conditions, referred to as AKKT2,
for nonlinear semidefinite optimization problems (NSDP). In particular, we
provide a formal definition of AKKT2, as well as its stronger variant, called
Complementary AKKT2 (CAKKT2), and prove that these conditions are necessary for
local minima without any assumption. Moreover, under Robinson's CQ and the weak
constant rank property, we show that AKKT2 implies the so-called weak
second-order necessary condition. Finally, we propose a penalty-based algorithm
that generates sequences whose accumulation points satisfy the AKKT2 and the
CAKKT2 conditions.

</details>


### [11] [On Policy Stochasticity in Mutual Information Optimal Control of Linear Systems](https://arxiv.org/abs/2507.21543)
*Shoju Enami,Kenji Kashima*

Main category: math.OC

TL;DR: 本文研究了离散时间线性系统的互信息最优控制问题（MIOCP），探讨了温度参数与策略随机性之间的关系，并通过数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 互信息最优控制作为最大熵最优控制的扩展，其温度参数与策略随机性之间的关系尚未明确，本文旨在填补这一理论空白。

Method: 通过扩展先前研究的结果，建立了MIOCP最优策略的存在性，并推导了温度参数使策略变为随机或确定性的条件，同时分析了交替优化算法所得策略的相应条件。

Result: 理论分析表明，温度参数的不同取值会导致最优策略呈现随机性或确定性，数值实验进一步验证了这些理论结果的正确性。

Conclusion: 本文为互信息最优控制中温度参数与策略随机性之间的关系提供了理论依据，为相关研究奠定了重要基础。

Abstract: In recent years, mutual information optimal control has been proposed as an
extension of maximum entropy optimal control. Both approaches introduce
regularization terms to render the policy stochastic, and it is important to
theoretically clarify the relationship between the temperature parameter (i.e.,
the coefficient of the regularization term) and the stochasticity of the
policy. Unlike in maximum entropy optimal control, this relationship remains
unexplored in mutual information optimal control. In this paper, we investigate
this relationship for a mutual information optimal control problem (MIOCP) of
discrete-time linear systems. After extending the result of a previous study of
the MIOCP, we establish the existence of an optimal policy of the MIOCP, and
then derive the respective conditions on the temperature parameter under which
the optimal policy becomes stochastic and deterministic. Furthermore, we also
derive the respective conditions on the temperature parameter under which the
policy obtained by an alternating optimization algorithm becomes stochastic and
deterministic. The validity of the theoretical results is demonstrated through
numerical experiments.

</details>


### [12] [Decentralized Modeling of Vehicular Maneuvers and Interactions at Urban Junctions](https://arxiv.org/abs/2507.21547)
*Saeed Rahmani,Simeon C. Calvert,Bart van Arem*

Main category: math.OC

TL;DR: 本文提出了一种用于城市交叉口轨迹规划和分散车辆控制的建模框架，解决了现有方法在非合作交互和车辆动力学统一建模上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前城市交叉口的车辆交互建模方法在非合作交互和车辆动力学统一框架方面存在不足，限制了其在混合自动驾驶交通中的适用性和真实性。

Method: 框架采用双层结构：上层通过高效图搜索算法生成运动学可行的参考轨迹，下层使用预测控制器进行轨迹跟踪和优化，无需中央控制或车辆间知识共享。

Result: 数值和仿真实验表明，该框架能够准确模拟各种城市交叉口（包括无信号灯交叉口和环岛）的车辆行为和交互。

Conclusion: 该分散式框架能够整合操作和随机因素（如车辆检测范围、感知不确定性和反应延迟），适用于安全分析和交通效率评估。

Abstract: Modeling and evaluation of automated vehicles (AVs) in mixed-autonomy traffic
is essential prior to their safe and efficient deployment. This is especially
important at urban junctions where complex multi-agent interactions occur.
Current approaches for modeling vehicular maneuvers and interactions at urban
junctions have limitations in formulating non-cooperative interactions and
vehicle dynamics within a unified mathematical framework. Previous studies
either assume predefined paths or rely on cooperation and central
controllability, limiting their realism and applicability in mixed-autonomy
traffic. This paper addresses these limitations by proposing a modeling
framework for trajectory planning and decentralized vehicular control at urban
junctions. The framework employs a bi-level structure where the upper level
generates kinematically feasible reference trajectories using an efficient
graph search algorithm with a custom heuristic function, while the lower level
employs a predictive controller for trajectory tracking and optimization.
Unlike existing approaches, our framework does not require central
controllability or knowledge sharing among vehicles. The vehicle kinematics are
explicitly incorporated at both levels, and acceleration and steering angle are
used as control variables. This intuitive formulation facilitates analysis of
traffic efficiency, environmental impacts, and motion comfort. The framework's
decentralized structure accommodates operational and stochastic elements, such
as vehicles' detection range, perception uncertainties, and reaction delay,
making the model suitable for safety analysis. Numerical and simulation
experiments across diverse scenarios demonstrate the framework's capability in
modeling accurate and realistic vehicular maneuvers and interactions at various
urban junctions, including unsignalized intersections and roundabouts.

</details>


### [13] [Distributionally Robust Shape and Topology Optimization](https://arxiv.org/abs/2507.21574)
*Charles Dapogny,Julien Prando,Boris Thibert*

Main category: math.OC

TL;DR: 本文提出了一种基于分布鲁棒性的优化设计方法，用于处理参数不确定性下的最优设计问题。通过构建模糊集和凸对偶技术，将复杂的双层规划转化为可处理的单层问题，并应用于拓扑优化和几何形状优化。


<details>
  <summary>Details</summary>
Motivation: 现实设计中，物理模型和成本函数常依赖不确定参数，而参数的概率分布仅通过少量样本估计得到。传统方法无法充分应对这种分布不确定性，因此需要引入分布鲁棒优化框架。

Method: 采用Wasserstein距离和矩约束构建模糊集，针对成本函数的期望和条件风险价值进行鲁棒化。利用凸对偶理论将原双层问题转化为单层凸优化问题，并扩展变量集实现统一求解框架。

Result: 提出的方法在密度拓扑优化和几何形状优化中得到验证，二维和三维数值实验证明了该技术的有效性。

Conclusion: 分布鲁棒优化方法能有效处理参数分布不确定性，其通用框架可广泛应用于各类最优设计问题，数值结果展示了方法的优越性和适用性。

Abstract: This article aims to introduce the paradigm of distributional robustness from
the field of convex optimization to tackle optimal design problems under
uncertainty. We consider realistic situations where the physical model, and
thereby the cost function of the design to be minimized depend on uncertain
parameters. The probability distribution of the latter is itself known
imperfectly, through a nominal law, reconstructed from a few observed samples.
The distributionally robust optimal design problem is an intricate bilevel
program which consists in minimizing the worst value of a statistical quantity
of the cost function (typically, its expectation) when the law of the uncertain
parameters belongs to a certain ``ambiguity set''. We address three classes of
such problems: firstly, this ambiguity set is made of the probability laws
whose Wasserstein distance to the nominal law is less than a given threshold;
secondly, the ambiguity set is based on the first- and second-order moments of
the actual and nominal probability laws. Eventually, a statistical quantity of
the cost other than its expectation is made robust with respect to the law of
the parameters, namely its conditional value at risk. Using techniques from
convex duality, we derive tractable, single-level reformulations of these
problems, framed over augmented sets of variables. Our methods are essentially
agnostic of the optimal design framework; they are described in a unifying
abstract framework, before being applied to multiple situations in
density-based topology optimization and in geometric shape optimization.
Several numerical examples are discussed in two and three space dimensions to
appraise the features of the proposed techniques.

</details>


### [14] [Optimal control of stochastic homogenous systems](https://arxiv.org/abs/2507.21576)
*Ying Hu,Xiaomin Shi,Zuo Quan Xu*

Main category: math.OC

TL;DR: 本文研究了一类具有锥控制约束的新型齐次随机控制问题，扩展了经典的齐次随机线性二次（LQ）框架，涵盖非线性系统动力学和非二次成本泛函。通过建立新型高度非线性倒向随机微分方程（BSDEs）的解的存在唯一性，并推导出最优控制和值函数的显式反馈表示。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展经典的齐次随机LQ控制框架，使其能够处理非线性系统和非二次成本泛函，从而在更广泛的应用场景中保持最优控制的可解性。

Method: 方法包括引入新型高度非线性BSDEs，并采用截断函数和对数变换等技术，在三种不同条件下证明这些BSDEs解的存在唯一性。通过严格的验证论证，推导出最优控制和值函数的显式反馈表示。

Result: 结果表明，广义问题的最优控制和值函数与新型BSDEs的解密切相关。在一般可解性条件下，可以恢复齐次LQ问题的许多已知结果，包括标准和奇异情况。

Conclusion: 结论是所提出的框架成功扩展了齐次随机控制问题的范围，为非线性系统和非二次成本泛函提供了统一的解决方案，并验证了其与经典LQ问题的兼容性。

Abstract: This paper investigates a new class of homogeneous stochastic control
problems with cone control constraints, extending the classical homogeneous
stochastic linear-quadratic (LQ) framework to encompass nonlinear system
dynamics and non-quadratic cost functionals. We demonstrate that, analogous to
the LQ case, the optimal controls and value functions for these generalized
problems are intimately connected to a novel class of highly nonlinear backward
stochastic differential equations (BSDEs). We establish the existence and
uniqueness of solutions to these BSDEs under three distinct sets of conditions,
employing techniques such as truncation functions and logarithmic
transformations. Furthermore, we derive explicit feedback representations for
the optimal controls and value functions in terms of the solutions to these
BSDEs, supported by rigorous verification arguments. Our general solvability
conditions allow us to recover many known results for homogeneous LQ problems,
including both standard and singular cases, as special instances of our
framework.

</details>


### [15] [Cost allocations in interval inventory situations: the SOC and Shapley approaches](https://arxiv.org/abs/2507.21603)
*J. C. Gonçalves-Dosantos,A. Meca,I. Ozcan*

Main category: math.OC

TL;DR: 本文探讨了在需求与供应不确定条件下，基于区间模型的库存管理方法，提出了两种区间分配规则（区间SOC规则和区间Shapley规则），并通过西班牙机场香水库存案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统库存模型（如EOQ）依赖固定参数和确定性假设，难以应对现实中的不确定性。本文旨在通过区间模型更灵活地分析库存决策及合作代理间的成本分摊问题。

Method: 研究扩展了经典库存模型，将需求表示为区间以纳入不确定性，并设计两种区间分配规则。通过理论分析及基于AENA提供的2023年西班牙七座机场客流数据的案例研究验证方法。

Result: 案例研究表明，区间模型能实现库存成本的稳健公平分配。区间SOC规则和区间Shapley规则在不确定需求下展现出高效且公正的成本分摊能力。

Conclusion: 区间模型为操作不确定性下的库存管理提供了新思路，其分配规则可促进合作代理间的成本公平分担，具有实际应用潜力。

Abstract: Uncertainty in demand and supply conditions poses critical challenges to
effective inventory management, especially in collaborative environments.
Traditional inventory models, such as those based on the Economic Order
Quantity (EOQ), often rely on fixed parameters and deterministic assumptions,
limiting their ability to capture the complexity of real-world scenarios. This
paper focuses on interval inventory situations, an extension of classical
models in which demand is represented as intervals to account for uncertainty.
This framework allows for a more flexible and realistic analysis of inventory
decisions and cost-sharing among cooperating agents. We examine two
interval-based allocation rules, the interval SOC-rule and the interval Shapley
rule, designed to distribute joint ordering costs fairly and efficiently under
uncertain demand. Their theoretical properties are analyzed, and their
practical applicability is demonstrated through a case study involving the
coordination of perfume inventories across seven Spanish airports, based on
2023 passenger traffic data provided by AENA. The findings highlight the
potential of interval-based models to enable a robust and equitable allocation
of inventory costs in the face of operational uncertainty.

</details>


### [16] [Adaptive Benders decomposition and enhanced SDDP for multistage stochastic programs with block-separable multistage recourse](https://arxiv.org/abs/2507.21624)
*Nicolò Mazzi,Ken Mckinnon,Hongyu Zhang*

Main category: math.OC

TL;DR: 本文提出了一种高效求解具有块可分追索的多阶段随机规划问题的算法，通过自适应Benders分解和增强SDDP方法实现，并在电力系统投资规划案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对具有阶段独立不确定性的多阶段随机追索问题，现有方法效率不足，需要开发新算法以高效求解此类问题。

Method: 算法首先通过自适应Benders分解将原问题分解为主问题和子问题，子问题采用增强SDDP方法求解，包括有效边界、路径探索规则、割共享和保证$\delta$最优收敛等改进。

Result: 案例研究表明：(1)算法能高效求解该类问题；(2)确定性风电模型低估目标函数；(3)随机风电模型导致不同的投资决策。

Conclusion: 本文提出了首个解决此类多阶段随机规划问题的算法，并通过电力系统案例验证了其优越性和实用性。

Abstract: This paper proposes an algorithm to efficiently solve multistage stochastic
programs with block separable recourse where each recourse problem is a
multistage stochastic program with stage-wise independent uncertainty. The
algorithm first decomposes the full problem into a reduced master problem and
subproblems using Adaptive Benders decomposition. The subproblems are then
solved by an enhanced SDDP. The enhancement includes (1) valid bounds at each
iteration, (2) a path exploration rule, (3) cut sharing among subproblems, and
(4) guaranteed {\delta}-optimal convergence. The cuts for the subproblems are
then shared by calling adaptive oracles. The key contribution of the paper is
the first algorithm for solving this class of problems. The algorithm is
demonstrated on a power system investment planning problem with multi-timescale
uncertainty. The case study results show that (1) the proposed algorithm can
efficiently solve this type of problem, (2) deterministic wind modelling
underestimate the objective function, and (3) stochastic modelling of wind
leads to different investment decisions.

</details>


### [17] [Guaranteeing Line-of-Sight Wireless Connectivity in Stochastic Environments with Random Obstacles](https://arxiv.org/abs/2507.21676)
*Mohsen Abedi,Alexis A. Dowhuszko,Mehdi Sookhak,Risto Wichman*

Main category: math.OC

TL;DR: 本文提出了一种在随机障碍物环境中优化毫米波/太赫兹/光无线通信接入点(AP)部署的方法，通过图建模和最大团算法减少25%的AP数量，同时保持5%的可容忍覆盖间隙。


<details>
  <summary>Details</summary>
Motivation: 毫米波、太赫兹和光无线通信虽能提供超高速率，但严重依赖视距(LoS)传输。随机环境中的障碍物会导致信号阻断，需通过优化AP部署来保障连接质量。

Method: 将随机环境建模为图结构：节点表示布局实现的子多边形，边表示可见性重叠。采用最大团聚类和最大团填充算法确定AP位置，确保全LoS覆盖或可控覆盖间隙。

Result: 在典型随机环境中的仿真显示：相比全LoS覆盖优化部署，该方法减少25%的AP需求，仅产生5%的可接受覆盖间隙。

Conclusion: 所提出的基于图论的AP部署策略能有效适应障碍物位置随机性，在保证覆盖质量的同时显著降低基础设施成本，为高频通信网络规划提供新思路。

Abstract: Advancements in high-frequency communication technologies using millimeter
waves (mmWave), Tera- Hertz (THz), and optical wireless frequency bands are key
for extending wireless connectivity beyond 5G. These technologies offer a
broader spectrum than the one available on low- and mid-bands, enabling
ultra-high-speed data rates, higher device density, enhanced security, and
improved positioning accuracy. However, their performance relies heavily on
clear Line-of-Sight (LoS) conditions, as Non-LoS components are significantly
weaker, making blockages a major challenge to ensure suitable received signal
power. This paper addresses this limitation by identifying the minimum number
and optimal placement of access points (APs) needed to ensure LoS connectivity
in stochastic/dynamic environments with random obstacle locations. To achieve
this, the stochastic environment is carefully modeled as a graph, where the
nodes represent sub-polygons of layout realizations, and the edges capture the
visibility overlaps between them. By employing maximal clique clustering and
maximum clique packing methods over this graph, the proposed approach
determines the AP placement locations that guarantee either full LoS coverage
or controlled LoS gaps, while seamlessly adapting to the stochastic variability
in obstacle locations. Simulations results in a representative stochastic
environment demonstrate a 25% reduction in the required number of APs,
achieving a tolerable 5% coverage gap compared to AP deployment optimized for
full LoS coverage.

</details>


### [18] [Riemannian Optimization on Tree Tensor Networks with Application in Machine Learning](https://arxiv.org/abs/2507.21726)
*Marius Willner,Marco Trenti,Dirk Lebiedz*

Main category: math.OC

TL;DR: 本文对树张量网络（TTN）的微分几何基础进行了形式化分析，并基于此开发了高效的一阶和二阶优化算法，以及用于核学习场景的反向传播算法，通过机器学习任务验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 树张量网络（TTN）在低秩近似和量子多体模拟中广泛应用，但缺乏对其微分几何基础的深入理解，限制了优化算法的开发和应用。

Method: 基于TTN的固有商结构，开发了高效的一阶和二阶优化算法，并设计了用于核学习场景的反向传播算法。

Result: 数值实验验证了所提方法在代表性机器学习任务中的有效性。

Conclusion: 本文的理论分析和算法开发为TTN的优化提供了新工具，展示了其在机器学习中的潜力。

Abstract: Tree tensor networks (TTNs) are widely used in low-rank approximation and
quantum many-body simulation. In this work, we present a formal analysis of the
differential geometry underlying TTNs. Building on this foundation, we develop
efficient first- and second-order optimization algorithms that exploit the
intrinsic quotient structure of TTNs. Additionally, we devise a backpropagation
algorithm for training TTNs in a kernel learning setting. We validate our
methods through numerical experiments on a representative machine learning
task.

</details>


### [19] [Communication-Efficient Algorithms for Distributed Nonconvex Minimax Optimization Problems](https://arxiv.org/abs/2507.21901)
*Haoyuan Cai,Sulaiman A. Alghunaim,Ali H. Sayed*

Main category: math.OC

TL;DR: 论文针对随机非凸Polyak-{\L}ojasiewicz极小极大问题，提出了一种通信和样本高效的分布式算法，并在三种设置下（分布式/去中心化、联邦/集中式、单智能体）验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布式极小极大算法存在通信复杂度高、步长选择严格、需要大批量样本等问题，亟需一种能同时实现高效通信和样本利用的新方法。

Method: 通过利用二阶Lipschitz连续性和通信高效策略，开发了具有局部更新的去中心化归一化加速动量法，并针对不同场景提出优化变体。

Result: 所提算法首次达到$\mathcal{O}\Big(\frac{\kappa^3\varepsilon^{-3}}{NK(1-\lambda)^{3/2}}\Big)$的最优通信复杂度，在$K$个智能体和$N$次局部更新上实现线性加速，且放宽了步长比限制、简化了稳定性条件。

Conclusion: 在鲁棒逻辑回归和公平神经网络分类器的真实数据集实验中，新方法显著优于现有基线，同时解决了关键实践难题。

Abstract: We study stochastic nonconvex Polyak-{\L}ojasiewicz minimax problems and
propose algorithms that are both communication- and sample-efficient. The
proposed methods are developed under three setups: decentralized/distributed,
federated/centralized, and single-agent. By exploiting second-order Lipschitz
continuity and integrating communication-efficient strategies, we develop a new
decentralized normalized accelerated momentum method with local updates and
establish its convergence to an $\varepsilon$-game stationary point. Compared
to existing decentralized minimax algorithms,
  our proposed algorithm is the first to achieve a state-of-the-art
communication complexity of order $\mathcal{O}\Big(
  \frac{ \kappa^3\varepsilon^{-3}}{NK(1-\lambda)^{3/2}}\Big)$, demonstrating
linear speedup with respect to both the number of agents $K$ and the number of
local updates $N$, as well as the best known dependence on the level of
accuracy of the solution $\varepsilon$. In addition to improved complexity, our
algorithm offers several practical advantages: it relaxes the strict
two-time-scale step size ratio required by many existing algorithms, simplifies
the stability conditions for step size selection, and eliminates the need for
large batch sizes to attain the optimal sample complexity.
  Moreover, we propose more efficient variants tailored to
federated/centralized and single-agent setups, and show that all variants
achieve best-known results while effectively addressing some key issues.
Experiments on robust logistic regression and fair neural network classifier
using real-world datasets demonstrate the superior performance of the proposed
methods over existing baselines.

</details>


### [20] [Large-Scale Linear Energy System Optimization: A Systematic Review on Parallelization Strategies via Decomposition](https://arxiv.org/abs/2507.21932)
*Lars Hadidi,Leonard Göke,Maximilian Hoffmann,Mario Klostermeier,Shima Sasanpour,Tim Varelmann,Vassilios Yfantis,Jochen Linßen,Detlef Stolten,Jann M. Weinand*

Main category: math.OC

TL;DR: 本文系统综述了针对能源系统优化模型规模与复杂性增加的并行化策略，提出了分类方案并评估了不同分解方法，强调了标准化基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源整合、部门耦合及时空细节的增加，能源系统优化模型的规模和复杂性不断增长，常使求解器达到性能极限，亟需有效的并行化策略应对这些挑战。

Method: 研究首先提出线性能源系统优化模型的分类方案，涵盖分析焦点、数学结构与范围；随后系统评估并行分解方法，并调查现有软件工具。

Result: 发现许多并行分解方法能提升性能，但无单一方法普遍最优；缺乏标准化基准套件增加了比较难度，为此提出了未来基准的核心标准与最低报告要求。

Conclusion: 尽管聚焦能源系统模型，但研究结论可扩展至更广泛的运筹学领域，强调了工具模块化与算法抽象的重要性。

Abstract: As renewable energy integration, sector coupling, and spatiotemporal detail
increase, energy system optimization models grow in size and complexity, often
pushing solvers to their performance limits. This systematic review explores
parallelization strategies that can address these challenges. We first propose
a classification scheme for linear energy system optimization models, covering
their analytical focus, mathematical structure, and scope. We then review
parallel decomposition methods, finding that while many offer performance
benefits, no single approach is universally superior. The lack of standardized
benchmark suites further complicates comparison. To address this, we recommend
essential criteria for future benchmarks and minimum reporting standards. We
also survey available software tools for parallel decomposition, including
modular frameworks and algorithmic abstractions. Though centered on energy
system models, our insights extend to the broader operations research field.

</details>


### [21] [Warm-starting Strategies in Scalarization Methods for Multi-Objective Optimization](https://arxiv.org/abs/2507.21933)
*Stephanie Riedmüller,Janina Zittel,Thorsten Koch*

Main category: math.OC

TL;DR: 本文研究了在(混合)整数线性规划的多目标优化中，如何将预热启动策略与标量化方法结合使用。通过理论分析和计算实验，探讨了子问题排序对预热效率的影响及其与其他优化目标的权衡。


<details>
  <summary>Details</summary>
Motivation: 标量化方法是计算帕累托最优解的经典技术，因其算法简单且适用于任意数量目标的连续和整数规划而广受欢迎。然而，预热启动策略在该领域的系统化方法和分析仍存在空白。

Method: 提出了标量化方法中预热启动的理论特征，重点研究子问题的排序策略。同时指出优化子问题排序以提高预热效率可能与早期识别不可行区域等其他标准存在冲突。

Result: 通过大量计算实验量化了这些权衡关系，展示了不同排序策略对预热效果的影响。

Conclusion: 研究表明，在标量化方法中系统应用预热启动策略具有潜力，但需要仔细权衡子问题排序与其他优化目标之间的关系。

Abstract: We explore how warm-starting strategies can be integrated into
scalarization-based approaches for multi-objective optimization in (mixed)
integer linear programming. Scalarization methods remain widely used classical
techniques to compute Pareto-optimal solutions in applied settings. They are
favored due to their algorithmic simplicity and broad applicability across
continuous and integer programs with an arbitrary number of objectives. While
warm-starting has been applied in this context before, a systematic methodology
and analysis remain lacking. We address this gap by providing a theoretical
characterization of warm-starting within scalarization methods, focusing on the
sequencing of subproblems. However, optimizing the order of subproblems to
maximize warm-start efficiency may conflict with alternative criteria, such as
early identification of infeasible regions. We quantify these trade-offs
through an extensive computational study.

</details>


### [22] [Stable formulations for the Capacitated Facility Location Problem with Customer Preferences](https://arxiv.org/abs/2507.21944)
*Concepción Domínguez,Juan de Dios Jaime-Alcántara*

Main category: math.OC

TL;DR: 本文研究了带顾客偏好的容量限制设施选址问题（CFLCP），提出了三种新的稳定分配类型，并给出了两种混合整数线性规划模型。特别地，循环联盟稳定分配在全局偏好最大化下具有帕累托最优性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展简单工厂选址问题（SPLPO），考虑设施容量限制和顾客偏好，以最小化选址和分配成本，并在全局偏好最大化下定义新的稳定分配类型。

Method: 方法包括定义顾客稳定、成对稳定和循环联盟稳定分配，并为每种分配类型提供两种混合整数线性规划模型。此外，还提出了一种新的帕累托最优匹配模型。

Result: 计算实验表明，新提出的分配类型在质量上优于文献中的现有方法。循环联盟稳定分配在全局偏好最大化下实现了帕累托最优。

Conclusion: 结论是CFLCP问题的新模型和分配类型有效解决了容量限制和顾客偏好的挑战，循环联盟稳定分配具有帕累托最优性，为相关领域提供了新的解决方案。

Abstract: In the Simple Plant Location Problem with Order (SPLPO), the aim is to open a
subset of plants to assign every customer taking into account their
preferences. Customers rank the plants in strict order and are assigned to
their favorite open plant, and the objective is to minimize the location plus
allocation costs. Here, we study a generalization of the SPLPO named the
Capacitated Facility Location Problem with Customer Preferences (CFLCP) where a
limited number of customers can be allocated to each facility. We consider the
global preference maximization setting, where the customers preferences are
globally maximized. For this setting, we define three new types of stable
allocations, namely customer stable, pairwise stable and cyclic-coalition
stable allocations, and we provide two mixed-integer linear formulations for
each setting. In particular, our cyclic-coalition stable formulations are
Pareto optimal in a global-preference maximization setting, in the sense that
no customer can improve their allocation without making another one worse off.
We provide extensive computational experiments and compare the quality of our
allocations with previous ones defined in the literature. As an additional
result, we present a novel formulation that provides Pareto optimal matchings
in the Capacitated House Allocation problem of maximum cardinality.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [23] [Iwasawa Theory of Elliptic Curves in Quadratic Twist Families](https://arxiv.org/abs/2507.21339)
*Debanjana Kundu,Katharina Mueller*

Main category: math.NT

TL;DR: 本文通过代数与解析两种方法研究了有理椭圆曲线在二次扭转族中Iwasawa不变量\的变化规律。


<details>
  <summary>Details</summary>
Motivation: 探索有理椭圆曲线在二次扭转族中Iwasawa不变量\的变化特性，以深化对椭圆曲线算术性质的理解。

Method: 采用代数与解析两种互补的研究方法，系统分析二次扭转对Iwasawa不变量\的影响。

Result: 揭示了不同二次扭转下Iwasawa不变量\的变化模式，为相关理论提供了新的实证依据。

Conclusion: 双方法研究证实了二次扭转族中Iwasawa不变量\具有规律性变化特征，为后续研究奠定了方法论基础。

Abstract: In this article, we use two different approaches -- one algebraic and the
other analytic -- to study the variation of Iwasawa invariants of rational
elliptic curves in some quadratic twist families.

</details>


### [24] [Resurgent Lambert series with characters](https://arxiv.org/abs/2507.21352)
*David Broadhurst,Daniele Dorigoni*

Main category: math.NT

TL;DR: 论文研究了特定Lambert级数作为除数和的生成函数，通过Dirichlet特征扭曲，计算了其在$q=1^-$附近的精确复苏超越级数展开。


<details>
  <summary>Details</summary>
Motivation: 探讨Lambert级数与扭曲的Dirichlet特征之间的关系，以及其在Fricke对合作用下的表现，为拓扑弦理论中的模复苏结构提供新视角。

Method: 利用扭曲的全纯Eisenstein级数的迭代积分表达Lambert级数，并分析Fricke对合对其的影响，进而推导超越级数展开。

Result: 对于特定参数，Lambert级数可通过扭曲的Eisenstein级数表示；对于一般参数，超越级数展开揭示了量子模版本的Fricke对合。

Conclusion: 研究不仅扩展了对Lambert级数的理解，还为拓扑弦理论中的模复苏结构提供了数学基础，展示了量子模对合的新性质。

Abstract: We consider certain Lambert series as generating functions of divisor sums
twisted by Dirichlet characters and compute their exact resurgent transseries
expansion near $q=1^-$. For special values of the parameters, these Lambert
series are expressible in terms of iterated integrals of holomorphic Eisenstein
series twisted by the same characters and the transseries representation is a
direct consequence of the action of Fricke involution on such twisted
Eisenstein series. When the parameters of the Lambert series are generic the
transseries representation provides for a quantum-modular version of Fricke
involution which for a particular example we show being equivalent to modular
resurgent structures found in topological strings observables.

</details>


### [25] [Simultaneous Diophantine approximation on the three dimensional Veronese curve](https://arxiv.org/abs/2507.21401)
*Dmitry Badziahin*

Main category: math.NT

TL;DR: 计算了三维Veronese曲线上同时满足$\lambda$-良好逼近点的Hausdorff维数，验证了Beresnevich和Yang的猜想下界部分。


<details>
  <summary>Details</summary>
Motivation: 验证Beresnevich和Yang在2023年提出的猜想，即对于$1/3\le \lambda\le 3/5$的范围，Veronese曲线上的$\lambda$-良好逼近点的Hausdorff维数下界成立。

Method: 通过数学分析和计算，研究了三维Veronese曲线上同时满足$\lambda$-良好逼近的点的Hausdorff维数。

Result: 在$1/3\le \lambda\le 3/5$范围内，计算并确认了Veronese曲线上$\lambda$-良好逼近点的Hausdorff维数下界，这是首次在$n\ge 3$的非退化曲线上验证该猜想。

Conclusion: 研究结果支持了Beresnevich和Yang的猜想，表明三维Veronese曲线是该猜想下界部分的首个非退化曲线实例。

Abstract: We compute the Hausdorff dimension of the set of simultaneously
$\lambda$-well approximable points on the Veronese curve in $\RR^3$ for $1/3\le
\lambda\le 3/5$. This range for $\lambda$ was predicted in the conjecture of
Beresnevich and Yang from~\cite{ber_yan_2023}. To the best of the author's
knowledge, this makes $\VVV_3$ the first nondegenerate curve in $\RR^n$, $n\ge
3$, to confirm the lower bound part of this conjecture.

</details>


### [26] [The Fourier coefficients and singular moduli of the elliptic modular function $j(τ)$, revisited](https://arxiv.org/abs/2507.21514)
*Toshiki Matsusaka*

Main category: math.NT

TL;DR: 金子公式将椭圆模$j$-函数的傅里叶系数表示为奇异模的有限和，源于Zagier受Borcherds积启发的成果，现已发展为更广泛的框架。


<details>
  <summary>Details</summary>
Motivation: 探索模形式傅里叶系数与模函数特殊值之间的深层联系，扩展金子公式的理论与应用范围。

Method: 综述了基于Zagier工作及Borcherds积启发的理论框架，分析其后续扩展方向。

Result: 建立了模形式傅里叶系数与模函数特殊值的系统性关联，形成多方向推广的理论体系。

Conclusion: 金子公式从单一结论发展为连接模形式与模函数的重要理论桥梁，具有持续的研究价值。

Abstract: Kaneko's formula expresses the Fourier coefficients of the elliptic modular
$j$-function as finite sums of singular moduli. First published as a short
article in 1996, it was presented as a consequence of Zagier's work inspired by
Borcherds products. Since then, the formula has developed into a broader
framework that links the Fourier coefficients of modular forms to the special
values of modular functions, extending in various directions. This article
surveys these subsequent developments.

</details>


### [27] [The modified prime sieve for primitive elements in finite fields](https://arxiv.org/abs/2507.21515)
*Gustav Kjærbye Bagger,James Punch*

Main category: math.NT

TL;DR: 该论文提出了一种筛法准则，用于证明有限域$\mathbb{F}_{q^r}$子集$\mathcal{A}$中存在本原元，仅需估计特征和$\sum_{\gamma \in \mathcal{A}}\chi(\gamma)$。该方法显著改进了Fernandes和Reis（2021）关于避开仿射超平面子集问题的结果。


<details>
  <summary>Details</summary>
Motivation: 研究有限域$\mathbb{F}_{q^r}$子集$\mathcal{A}$中本原元的存在性问题，为解决该领域问题提供灵活且直接适用的理论工具。

Method: 通过建立依赖特征和估计的筛法准则，将本原元存在性转化为对$\sum_{\gamma \in \mathcal{A}}\chi(\gamma)$的分析。

Result: 成功应用于Fernandes-Reis问题，在避开仿射超平面的子集中获得比以往更优的本原元存在性结果。

Conclusion: 所提筛法准则具有普适性和计算优势，为有限域本原元研究提供了新的方法论突破。

Abstract: Let $r \geq 2$ be an integer, $q$ a prime power and $\mathbb{F}_{q}$ the
finite field with $q$ elements. Consider the problem of showing existence of
primitive elements in a subset $\mathcal{A} \subseteq \mathbb{F}_{q^r}$. We
prove a sieve criterion for existence of such elements, dependent only on an
estimate for the character sum $\sum_{\gamma \in \mathcal{A}}\chi(\gamma)$. The
flexibility and direct applicability of our criterion should be of considerable
interest for problems in this field. We demonstrate the utility of our result
by tackling a problem of Fernandes and Reis (2021) with $\mathcal{A}$ avoiding
affine hyperplanes, obtaining significant improvements over previous knowledge.

</details>


### [28] [The imaginary case of the nonabelian Cohen--Lenstra heuristics](https://arxiv.org/abs/2507.21558)
*Yuan Liu,Ken Willyard*

Main category: math.NT

TL;DR: 研究有限群$\Gamma$下，虚$\Gamma$-扩张$K$的最大非分歧完全分裂扩张Galois群$G_{\emptyset}^{\#}(K)$的分布，通过Hurwitz栈计算矩并构建随机群模拟其行为。


<details>
  <summary>Details</summary>
Motivation: 探索虚$\Gamma$-扩张中Galois群的分布规律，类比非阿贝尔Cohen-Lenstra启发式，填补虚数域与函数域的研究空白。

Method: 在函数域情形，通过Hurwitz栈的点计数计算矩；构建特定形式的随机群模拟$G_{\emptyset}^{\#}(K)$的行为以预测分布。

Result: 证明了$G_{\emptyset}^{\#}(K)$具有特定形式的表示，并提出了基于随机群概率测度的分布猜想。

Conclusion: 成果为虚数域与函数域的Galois群分布提供了新视角，与非阿贝尔Cohen-Lenstra理论形成虚类比。

Abstract: For a finite group $\Gamma$, we study the distribution of the Galois group
$G_{\emptyset}^{\#}(K)$ of the maximal unramified extension of $K$ that is
split completely at $\infty$ and has degree prime to $|\Gamma|$ and
$\textit{Char}(K)$, as $K$ varies over imaginary $\Gamma$-extensions of
$\mathbb{Q}$ or $\mathbb{F}_q(t)$. In the function field case, we compute the
moments of the distribution of $G_{\emptyset}^{\#}(K)$ by counting points on
Hurwitz stacks. In order to understand the probability of the distribution, we
prove that $G_{\emptyset}^{\#}(K)$ admits presentations of a specific form,
then use this presentation to build random groups to simulate the behavior of
$G_{\emptyset}^{\#}(K)$, and make the conjecture to predict the distribution
using the probability measures of these random groups. Our results provide the
imaginary analog of the work of Wood, Zureick-Brown, and the first author on
the nonabelian Cohen--Lenstra heuristics.

</details>


### [29] [On a characterisation of perfectoid fields by Iwasawa theory](https://arxiv.org/abs/2507.21801)
*Gautier Ponsinet*

Main category: math.NT

TL;DR: 本文证明了某些de Rham Galois表示的通用范数模的消失刻画了$p$-进数域的代数扩张，其完备化是perfectoid域，推广了Coates、Greenberg和Bondarko的结果。


<details>
  <summary>Details</summary>
Motivation: 研究$p$-进数域的代数扩张与perfectoid域之间的联系，推广已有关于阿贝尔簇和$p$-可除群的结果。

Method: 通过分析de Rham Galois表示的通用范数模的消失性质，建立与perfectoid域完备化的代数扩张之间的特征关系。

Result: 证明了通用范数模的消失是完备化为perfectoid域的代数扩张的充分必要条件，推广了前人的工作。

Conclusion: 该结果为理解$p$-进数域的代数扩张与perfectoid几何之间的联系提供了新的理论工具，并统一了不同背景下的相关结果。

Abstract: We prove that the vanishing of the module of universal norms associated with
certain de Rham Galois representations characterises the algebraic extensions
of the field of $p$-adic numbers whose completion is a perfectoid field. We
thereby generalise results by Coates and Greenberg for abelian varieties, and
by Bondarko for $p$-divisible groups.

</details>


### [30] [Quadratic forms of holomorphic cusp forms and the decay of their $\ell^p$-norms for $0 < p < 2$](https://arxiv.org/abs/2507.21951)
*Shenghao Hua*

Main category: math.NT

TL;DR: 本文证明在给定正交基条件下，由全纯Hecke尖点形式构成的二次型通常不能表示为有界线性组合，且当权重大于某常数时解有限；同时表明$\ell^p$-范数在$0<p<2$时渐近趋于零。


<details>
  <summary>Details</summary>
Motivation: 研究全纯Hecke尖点形式二次型的表示局限性及其渐进行为，揭示模形式线性方程解的有限性及范数收敛特性。

Method: 基于正交基展开和条件性假设，分析二次型在Hecke特征形式展开中的有界线性组合不可表示性及$\ell^p$-范数渐近性质。

Result: 证明权重大于绝对常数时，满足非零有界条件的二次型通常无法表示为尖点形式的有界组合；$\ell^p$-范数在$0<p<2$时随基展开渐近消失。

Conclusion: 该结果揭示了高权模形式二次型表示的固有约束，并为相关数论问题的解空间有限性及分析性质提供了理论依据。

Abstract: In this paper, we demonstrate that, given an orthonormal basis of holomorphic
Hecke cusp forms, conditionally, quadratic forms composed of cusp forms -- each
expressed as a bounded linear combination of holomorphic Hecke cusp forms --
are generally not themselves expressible as bounded linear combinations of
holomorphic Hecke cusp forms when the sum of the weights exceeds some absolute
constant, provided that the coefficients of the quadratic form satisfy
appropriate nonvanishing and boundedness conditions. This illustrates the
finiteness of the number of solutions to the linear equation of modular forms
equated to a quadratic form of large weight.
  We also show that, conditionally, for $0 < p < 2$, the $\ell^p$-norm of such
quadratic forms in holomorphic Hecke cusp forms tends to zero asymptotically
with respect to expansion in this orthonormal basis of Hecke eigenforms.

</details>


### [31] [Coh zeta functions for inert quadratic orders](https://arxiv.org/abs/2507.21966)
*Yifeng Huang*

Main category: math.NT

TL;DR: 研究惰性二次序的Coh zeta函数，提出其与$t$-变形Bressoud $q$-级数相关的猜想，并通过新方法验证部分结果。


<details>
  <summary>Details</summary>
Motivation: 完成将分歧与分裂二次序的zeta函数分别联系到Andrews-Gordon和Bressoud恒等式的三部曲，探索惰性序的对应关系。

Method: 采用基于偏序集M\"obius反演的新方法，计算家族中最简单序的有限化Coh zeta函数及$t=1$特例。

Result: 为首个惰性序家族推导出有限化Coh zeta函数的显式，为猜想提供强有力证据。

Conclusion: 通过新方法验证了惰性二次序Coh zeta函数与$t$-变形Bressoud级数的关联猜想，填补了三部曲的最后空白。

Abstract: We study the Coh zeta function for a family of inert quadratic orders, which
we conjecture to be given by $t$-deformed Bressoud $q$-series. This completes a
trilogy connecting the zeta functions of ramified and split quadratic orders to
the classical Andrews--Gordon and Bressoud identities, respectively. We provide
strong evidence for this conjecture by deriving the first explicit formulas for
the finitized Coh zeta function of the simplest order in the family, and for
the $t=1$ specialization of the finitized Coh zeta functions for all orders in
the family. Our primary tool is a new method based on M\"obius inversion on
posets.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [32] [A combinatorial characterization of Kim's lemma for pairs of bi-invariant types](https://arxiv.org/abs/2507.21366)
*James E. Hanson*

Main category: math.LO

TL;DR: 本文通过组合构型研究了Kim引理在特定条件下的失效情况，建立了参数阵列与模型理论性质间的联系，并探讨了在$\mathsf{GCH}$假设下通用平稳局部特征的失效。


<details>
  <summary>Details</summary>
Motivation: 研究Kim引理形式$(\star)$的失效条件及其组合等价构型，探索模型理论中参数类型与公式可除性的深层关系。

Method: 构建组合一致性-不一致性构型，提出非平凡技术变体$(\star)$，分析无$P_4$图参数族，并通过链-反链阵列验证$k$-可除性差异。

Result: 证明当存在满足链一致/反链$k$-不一致的参数阵列时，必存在模型$M$使得$\varphi(x,b)$沿$p$的$k$-可除性不同于沿$q$的可除性，且该构型导致$\mathsf{GCH}$下通用平稳局部特征失效。

Conclusion: 通过组合方法揭示了Kim引理失效的精确条件，建立了阵列性质与类型可除性的对应关系，为模型理论中的可除性研究提供了新工具。

Abstract: We give a combinatorial consistency-inconsistency configuration that is
equivalent to the failure of the following form of Kim's lemma for a given $k$:
$(\star)$ For any set of parameters $A$, formula $\varphi(x,b)$, and
$A$-bi-invariant types $p$ and $q$ extending $\mathrm{tp}(b/A)$, if
$\varphi(x,b)$ $k$-divides along $p$, then it divides along $q$.
  We then give an equivalent technical variant of $(\star)$ that is non-trivial
over arbitrary invariance bases. We also show that the failure of weaker
versions of $(\star)$ entails the existence of stronger combinatorial
configurations, the strongest of which can be phrased in terms of families of
parameters indexed by arbitrary cographs (i.e., $P_4$-free graphs).
  Finally, we show that if there is an array $(b_{i,j} : i,j < \omega)$ of
parameters such that $\{\varphi(x,b_{i,j}) : (i,j) \in C\}$ is consistent
whenever $C \subseteq \omega^2$ is a chain (in the product partial order) and
$k$-inconsistent whenever $C$ is an antichain, then there is a model $M$,
parameter $b$, and $M$-coheirs $p,q \supset \mathrm{tp}(b/M)$ such that
$q^{\otimes \omega}$ is an $M$-heir-coheir and $\varphi(x,b)$ $k$-divides along
$p$ but does not divide along $q$. In doing so, we also show that this
configuration entails the failure of generic stationary local character under
the assumption of $\mathsf{GCH}$.

</details>


### [33] [First-order aspects of Artin groups](https://arxiv.org/abs/2507.21575)
*Alberto Cassella,Gianluca Paolini,Giovanni Paolini*

Main category: math.LO

TL;DR: 本文研究了Artin群的模型理论，重点关注了与右角Artin群差异较大的Artin群。证明了若干结果，包括模型理论的可约性、超稳定性问题、球面Artin群的初等等价性，以及仿射Artin群的区分方法。


<details>
  <summary>Details</summary>
Motivation: 研究Artin群的模型理论性质，特别是那些与右角Artin群差异较大的Artin群，以深化对这类群的理论理解。

Method: 通过分析Artin群的不可约分量、双曲性质和无挠性，以及利用抛物子群的纯性条件，结合同调理论和$K(\pi, 1)$猜想的最新证明结果。

Result: 1) 对于不可约分量为双曲无挠的Artin群，其模型理论可约至不可约分量；2) 非交换Artin群的超稳定性问题可约至特定二面体抛物子群的纯性；3) 球面Artin群初等等价当且仅当同构；4) 仿射Artin群$\tilde{A}_n$（$n\geq4$）可通过存在语句与其他单链仿射Artin群区分。

Conclusion: 本文为Artin群的模型理论提供了新的理论工具和结果，特别是在可约性、超稳定性和初等等价性方面，同时展示了同调理论在区分Artin群中的重要作用。

Abstract: We prove several results on the model theory of Artin groups, focusing on
Artin groups which are ``far from right-angled Artin groups''. The first result
is that if $\mathcal{C}$ is a class of Artin groups whose irreducible
components are acylindrically hyperbolic and torsion-free, then the model
theory of Artin groups of type $\mathcal{C}$ reduces to the model theory of its
irreducible components. The second result is that the problem of superstability
of a given non-abelian Artin group $A$ reduces to certain dihedral parabolic
subgroups of $A$ being $n$-pure in $A$, for certain large enough primes $n \in
\mathbb{N}$. The third result is that two spherical Artin groups are elementary
equivalent if and only if they are isomorphic. Finally, we prove that the
affine Artin groups of type $\tilde{A}_n$, for $n \geq 4$, can be distinguished
from the other simply laced affine Artin groups using existential sentences;
this uses homology results of independent interest relying on the recent proof
of the $K(\pi, 1)$ conjecture for affine Artin groups.

</details>


### [34] [Maximal order types for sequences with gap condition](https://arxiv.org/abs/2507.21877)
*Patrick Uftring*

Main category: math.LO

TL;DR: 本文重新证明了Gordeev关于对称间隙条件下序列偏序的结论，并扩展了弱/强间隙条件及带弱升序标签的二叉树的结果，同时计算了所有结构的最大序类型。


<details>
  <summary>Details</summary>
Motivation: 研究Higman引理与算术可计算性之间的关系，探讨不同间隙条件下偏序的性质及其证明所需的数学基础。

Method: 结合Girard、Sch\"{u}tte和Simpson的成果，采用对称间隙条件简化证明，并扩展至弱/强间隙条件及二叉树结构。

Result: 证明对称间隙条件下的偏序良序性等价于算术超限递归，并计算了相关结构的最大序类型。

Conclusion: 间隙条件的变体会显著影响偏序的证明复杂度，对称条件需要更强的数学基础，而最大序类型的计算为相关理论提供了量化工具。

Abstract: Higman's lemma states that for any well partial order $X$, the partial order
$X^*$ of finite sequences with members from $X$ is also well. By combining
results due to Girard as well as Sch\"{u}tte and Simpson, one can show that
Higman's lemma is equivalent to arithmetical comprehension over
$\textsf{RCA}_0$, the usual base system of reverse mathematics. By
incorporating Friedman's gap condition, Sch\"{u}tte and Simpson defined a
slightly different order on finite number sequences with fewer comparisons.
While it is still true that their definition yields a well partial order, it
turns out that arithmetical comprehension is not enough to prove this fact.
Gordeev considered a symmetric variation of this gap condition for sequences
with members from arbitrary well orders. He could show, over $\textsf{RCA}_0$,
that his partial order on sequences is well (for any underlying well order) if
and only if arithmetical transfinite recursion is available. We present a new
and simpler proof of this fact and extend Gordeev's results to weak and strong
gap conditions as well as binary trees with weakly ascending labels. Moreover,
we compute the maximal order types of all considered structures.

</details>


### [35] [Model theory of Hilbert spaces expanded by normal operators](https://arxiv.org/abs/2507.21894)
*Alexander Berenstein,Nicolás Cuervo Ovalle,Isaac Goldbring*

Main category: math.LO

TL;DR: 本文研究了带有有界正规算子$T$的希尔伯特空间的扩展，通过公理化方法确定了所有完备化理论，证明了伴随算子$T^*$的可定义性，并给出了类型与算子谱上测度的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究希尔伯特空间中带有有界正规算子的扩展理论，旨在理解其逻辑结构和类型空间的性质，为算子理论的模型论分析提供基础。

Method: 采用公理化方法，在自然语言中形式化该理论，并通过添加伴随算子$T^*$实现量词消去。利用测度论工具，将类型与算子谱上的测度联系起来。

Result: 证明了所有完备化理论都是稳定的，并给出了类型空间度量与算子谱的精确公式。逻辑拓扑与测度空间的弱*-拓扑相对应，且所有完备化理论在扰动下都是$\omega$-稳定的。

Conclusion: 该研究为带有有界正规算子的希尔伯特空间提供了完整的模型论描述，建立了算子谱与理论稳定性之间的深刻联系，拓展了模型论在泛函分析中的应用。

Abstract: We study expansions of Hilbert spaces with a bounded normal operator $T$. We
axiomatize this theory in a natural language and identify all of its
completions. We prove the definability of the adjoint $T^*$ and prove
quantifier elimination for every completion after adding $T^*$ to the language.
We identify types with measures on the spectrum of the operator and show that
the logic topology on the type space corresponds to the weak*-topology on the
space of measures. We also give a precise formula for the metric on the space
of $1$-types. We prove all completions are stable and characterize the
stability spectrum of the theory in terms of the spectrum of the operator. We
also show all completions, regardless of their spectrum, are $\omega$-stable up
to perturbations.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [36] [Spectral properties of distance Laplacian matrices of complex unit gain graphs](https://arxiv.org/abs/2507.21345)
*Aniruddha Samanta,Deepshikha*

Main category: math.CO

TL;DR: 本文研究了$\mathbb{T}$-增益图的距离拉普拉斯矩阵的若干谱性质，包括平衡性判据、切换等价图的谱关系、谱半径的上下界及其等式条件。


<details>
  <summary>Details</summary>
Motivation: 探索$\mathbb{T}$-增益图的距离拉普拉斯矩阵的谱特性，以深化对这类图结构的数学理解。

Method: 通过分析增益图的距离拉普拉斯矩阵，建立平衡性判据，并研究切换等价性与谱不变性的关系，推导谱半径的界。

Result: 证明了平衡$\mathbb{T}$-增益图的零空间判据；发现切换等价性不保证谱相同，但给出了谱相同的必要条件；获得了谱半径的上下界及等式成立条件。

Conclusion: 该研究系统建立了$\mathbb{T}$-增益图距离拉普拉斯矩阵的谱理论框架，为后续研究提供了重要工具。

Abstract: A complex unit gain graph ($ \mathbb{T} $-gain graph), $ \Phi=(G, \varphi) $
is a graph where the function $ \varphi $ assigns a unit complex number to each
orientation of an edge of $ G $, and its inverse is assigned to the opposite
orientation. In this article, we study several spectral properties of distance
Laplacian matrices of $\mathbb{T}$-gain graphs. In particular, we establish a
characterization for the balanced $ \mathbb{T}$-gain graph in terms of the
nullity of gain distance Laplacian matrices. As an example, it is shown that
two switching equivalent $ \mathbb{T} $-gain graphs need not imply that their
distance Laplacian spectra are the same. However, we provide a necessary
condition for which two switching equivalent $ \mathbb{T} $-gain graphs have
the same distance Laplacian spectra. Furthermore, we present a lower bound for
spectral radii of gain distance Laplacian matrices in terms of the winner
index. In addition, we establish some upper bounds for spectral radii of gain
distance Laplacian matrices and characterize the equalities.

</details>


### [37] [Non-Hamiltonian 2-regular Digraphs](https://arxiv.org/abs/2507.21381)
*Munagala V. S. Ramanath*

Main category: math.CO

TL;DR: 本文扩展了2-正则有向图(2-dds)的研究，引入路径和商的概念，提出了新的非哈密尔顿性判定标准，并识别了几类罕见的非哈密尔顿有向图家族。


<details>
  <summary>Details</summary>
Motivation: 先前研究已发现2-dds的分解特性及其与邻接矩阵行列式/永久式的关联，但缺乏系统性的非哈密尔顿图家族分类。本文旨在填补这一空白。

Method: 通过引入路径(routes)和商(quotients)的新概念，构建理论框架分析2-dds结构特性，并开发非哈密尔顿性判定准则。

Result: 建立了多个2-dds非哈密尔顿性的充分条件，识别出文献中罕见的连通正则非哈密尔顿有向图家族。

Conclusion: 研究成果扩展了有向图理论工具集，特别为低度数正则有向图的非哈密尔顿性研究提供了新范式。

Abstract: In earlier papers, we showed a decomposition of 2-diregular digraphs (2-dds)
and used it to provide some sufficient conditions for these graphs to be
non-Hamiltonian; we also showed a close connection between the permanent and
determinant of the adjacency matrices of these digraphs and gave some
enumeration and generation results. In the present paper we extend the
discussion to a larger class of digraphs, introduce the notions of routes and
quotients and use them to provide additional criteria for 2-dds to be
non-Hamiltonian. Though individual non-Hamiltonian regular connected graphs of
low degree are known (e.g. Tutte and Meredith graphs), families of such graphs
are not common in the literature; even scarcer are families of such digraphs.
Our results identify a few such families.

</details>


### [38] [On the DP-chromatic Number of Cartesian Products of Critical Graphs](https://arxiv.org/abs/2507.21421)
*Hemanshu Kaul,Jeffrey A. Mudrock,Gunjan Sharma*

Main category: math.CO

TL;DR: 本文研究了DP着色（对应着色）在图笛卡尔积上的性质，特别是针对k-临界图G与完全二分图K_{l,t}的积图，探讨了其DP色数与色数之间的差距，并利用DP色函数工具取得了相关进展。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解当G是k-临界图且$\chi_{DP}(G)=k$时，$\chi_{DP}(G \square K_{l,t})$与其色数$\chi(G \square K_{l,t})$的差距，特别是寻找使得上界$k+l$达到的最小t值。

Method: 方法上，作者利用DP色函数（DP color function）作为工具，这是色多项式的DP类比，用于分析和解决这一问题。

Result: 结果表明，$\chi_{DP}(G \square K_{l,t}) \leq k + l$，并且对于固定的l，找到了使得这一上界达到的最小t值，这与经典结果中$K_{l,t}$的列表色数为$l+1$当且仅当$t \geq l^l$的情形类似。

Conclusion: 结论指出，DP色函数作为一个概念和工具，对于解决此类问题具有重要价值，为理解DP着色在图积上的行为提供了新的视角和方法。

Abstract: DP-coloring (also called correspondence coloring) is a well-studied
generalization of list coloring introduced by Dvo\v{r}\'{a}k and Postle in
2015. The following sharp bound on the DP-chromatic number of the Cartesian
product of graphs $G$ and $H$ is known: $\chi_{DP}(G \square H) \leq
\text{min}\{\chi_{DP}(G) + \text{col}(H), \chi_{DP}(H) + \text{col}(G) \} - 1$
where $\chi_{DP}(G)$ is the DP-chromatic number of $G$ and $\text{col}(H)$ is
the coloring number of $H$. We seek to understand when $\chi_{DP}(G \square
K_{l,t})$ is far from its chromatic number: $\chi(G \square K_{l,t}) = \max
\{\chi(G), 2 \}$ in the case that $G$ is a $k$-critical graph with
$\chi_{DP}(G)=k$. In particular, we have $\chi_{DP}(G \square K_{l,t}) \leq k +
l$, and for fixed $l$ we wish to find the smallest $t$ for which this upper
bound is achieved. This can be viewed as an extension of the classic result
that the list chromatic number of $K_{l,t}$ is $l+1$ if and only if $t \geq
l^l$. Our results illustrate that the DP color function of $G$, the DP analogue
of the chromatic polynomial, provides a concept and tool that is useful for
making progress on this problem.

</details>


### [39] [Slavic Techniques for Hat Guessing Algorithms](https://arxiv.org/abs/2507.21487)
*I. M. J. McInnis*

Main category: math.CO

TL;DR: 2023年本科论文研究确定性'帽子游戏'。在有向图$D$中，玩家根据邻居的帽子颜色猜测自己的帽子颜色。论文探讨了哪些游戏$(D,g,h)$是可赢的，并研究了参数$\mu(D)$和$\hat{\mu}(D)$。


<details>
  <summary>Details</summary>
Motivation: 研究帽子游戏的可赢性条件，探索图参数与游戏可赢性之间的关系，解决开放性问题。

Method: 使用'帽子作为提示'、'可接受路径'、'组合棱镜'和'依赖有向图'等方法，分析循环、树和完全二分图等特定图结构的游戏。

Result: 证明了循环游戏$(C_{k\geq4},1,h)$的可赢条件，树游戏$(T,1,h)$的可赢性取决于子树$T'$的条件，以及有向图$D$的参数$\hat{\mu}(D)$的上界。

Conclusion: 论文提供了多种图结构下帽子游戏的可赢性条件，并提出了未解决的开放性问题，如图参数如何限制$\mu$以及涉及的复杂性类别。

Abstract: 2023 undergraduate thesis on a deterministic "hat game." For a digraph $D$,
each player stands on a vertex $v$, is assigned a hat from $h(v)$ possible
colors, and makes $g(v)$ guesses of her hat's color based on her out-neighbors'
hats. If there exists a collective strategy that guarantees a correct guess for
any hat assignment, the game is winnable. Which games $(D,g,h)$ are winnable?
Two much-studied parameters: $\mu(D)$ is the maximum integer $k$ such that
$(D,1,k)$ is winnable, and $\hat{\mu}(D)$ is the supremum of $h/g$ for integer
$h, g$ such that $(D,g,h)$ is winnable.
  Chapter 0 is a casual, riddle-based introduction. Chapter 1 taxonomizes the
games, surveys all previous work, and summarizes the piece. Chapter 2 proves
lemmata and easy cases. Chapter 3 uses "hats as hints" and "admissible paths"
for games on cycles. Chapter 4 generalizes several "constructors" and applies
them to tree games. Chapter 5 uses "combinatorial prisms" for a new angle on
the well-studied $K_{n,m}$ games. In chapter 6, we apply "dependency digraphs"
to the continuous limit of this game. Chapter 7 collects open problems and
minor results.
  We show:
  $(C_{k\geq 4},1,h)$ is winnable if and only if: $h=3$ and $k$ is divisible by
$3$ or equal to $4$, $h\leq 4$ and the $h(v)$ sequence $(3,2,3)$ or $(2,3,3)$
appears in the cycle, or the $h(v)$ sequence $(2,...,2)$ appears with no
intervening value $>4$.
  $(T, 1, h)$ is winnable for tree $T$ iff $T$ has a subtree $T'$ with
$h(v)\leq 2^{deg_{T'}(v)}$ for all $v\in V(T')$.
  For a digraph $D$, $\hat{\mu}(D)\leq e(\Delta^-+1)$. For a graph $G$,
$\hat{\mu}(G)\leq (\Delta-1)^{1-\Delta} \Delta^{\Delta}<e\Delta$.
  $(\overrightarrow{C}_k, g, h)$ is unwinnable if $g(v_i)/h(v_i) +
g(v_{i+1})/h(v_{i+1}) < 1$ for some $i$.
  And much else. Important open questions: what other graph parameters or
properties bound $\mu$? What complexity classes are at play?

</details>


### [40] [Solid bricks that every $b$-invariant edge is solitary](https://arxiv.org/abs/2507.21565)
*Yipei Zhang,Xiumei Wang*

Main category: math.CO

TL;DR: 本文证明了对于不同于$K_4$的实心砖块图$G$，其所有$b$-不变边都是孤立的当且仅当$G$是轮图$W_n$。


<details>
  <summary>Details</summary>
Motivation: Lucchesi和Murty提出了一个关于砖块图特征的问题，特别是那些不同于$K_4$、$\overline{C_6}$和Petersen图的图，其中所有$b$-不变边都是孤立的。本文旨在解决这一问题。

Method: 通过研究实心砖块图的性质，特别是其顶点和边的关系，以及完美匹配的条件，来探讨$b$-不变边的孤立性。

Result: 结果表明，对于不同于$K_4$的实心砖块图$G$，所有$b$-不变边都是孤立的当且仅当$G$是轮图$W_n$。

Conclusion: 本文的结论为Lucchesi和Murty提出的问题提供了一个明确的解答，即只有在轮图$W_n$中，所有$b$-不变边才是孤立的。

Abstract: A graph $G$ is a brick if it is 3-connected and $G-\{u,v\}$ has a perfect
matching for any two distinct vertices $u$ and $v$ of $G$. A brick $G$ is solid
if for any two vertex disjoint odd cycles $C_1$ and $C_2$ of $G$,
$G-(V(C_1)\cup V(C_2))$ has no perfect matching. Lucchesi and Murty proposed a
problem concerning the characterization of bricks, distinct from $K_4$,
$\overline{C_6}$ and the Petersen graph, in which every $b$-invariant edge is
solitary. In this paper, we show that for a solid brick $G$ of order $n$ that
is distinct from $K_4$, every $b$-invariant edge of $G$ is solitary if and only
if $G$ is a wheel $W_n$.

</details>


### [41] [Partial Deranged Bell Numbers and Their Combinatorial Properties](https://arxiv.org/abs/2507.21643)
*Yahia Djemmada,Levent Kargın,Mümün Can*

Main category: math.CO

TL;DR: 本文引入了部分错位贝尔数$w_{n,r}$的新概念，统一了部分错位、斯特林数和有序贝尔数的框架，研究了其组合性质，并揭示了它们与经典序列的关系。


<details>
  <summary>Details</summary>
Motivation: 通过定义部分错位贝尔数，构建一个统一框架，连接部分错位、斯特林数和有序贝尔数，以深入探索其组合性质及与其他经典序列的关系。

Method: 研究了部分错位贝尔数的组合性质，包括显式公式、生成函数和递推关系，并探讨了其与经典序列如错位贝尔数和有序贝尔数的关系。

Result: 证明了部分错位贝尔数可用经典序列表示，并揭示了其与互补贝尔数的结构联系，如$\tilde{\phi}_{n}=\Tilde{w}_{n,0}-\Tilde{w}_{n,1}=\tilde{w}_{n-1,0}-2\tilde{w}_{n-1,2}$。此外，引入了多项式展开并探讨了其与指数多项式、几何多项式和伯努利数的联系。

Conclusion: 部分错位贝尔数为组合数学提供了新的工具，其与经典序列的关系及多项式展开的应用，为涉及第二类斯特林数、伯努利数和二项式系数的有限求和提供了闭式表达式。

Abstract: We introduce a novel generalization of deranged Bell numbers by defining the
partial deranged Bell numbers $w_{n,r}$, which count the number of set
partitions of $\left[ n\right] $ with exactly $r$ fixed blocks, while the
remaining blocks are deranged. This construction provides a unified framework
that connects partial derangements, Stirling numbers, and ordered Bell numbers.
We investigate their combinatorial properties, including explicit formulas,
generating functions, and recurrence relations. Moreover, we demonstrate that
these numbers are expressible in terms of classical sequences such as deranged
Bell numbers and ordered Bell numbers, and reveal their relationship to
complementary Bell numbers, offering insights relevant to Wilf's conjecture.
Notably, we derive the identity \[
\tilde{\phi}_{n}=\Tilde{w}_{n,0}-\Tilde{w}_{n,1}=\tilde{w}_{n-1,0}-2\tilde
{w}_{n-1,2}, \] which illustrates their structural connection to complementary
Bell numbers. We also introduce a polynomial expansion for these numbers and
explore their connections with exponential polynomials, geometric polynomials,
and Bernoulli numbers. These relationships facilitate the derivation of
closed-form expressions for certain finite summations involving Stirling
numbers of the second kind, Bernoulli numbers, and binomial coefficients,
articulated through partial derangement numbers.

</details>


### [42] [Curves, points, incidences and covering](https://arxiv.org/abs/2507.21758)
*Arijit Bishnu,Mathew Francis,Pritam Majumder*

Main category: math.CO

TL;DR: 研究点集（通常是网格）被不同曲线覆盖所需数量的上下界，以及覆盖点集的反问题。


<details>
  <summary>Details</summary>
Motivation: 探讨如何用最少数量的特定类型曲线（如单调曲线、直线、正交凸曲线、圆等）完全覆盖给定的点集，并研究点集被覆盖时的几何配置特性。

Method: 分析点集被不同曲线覆盖的上下界，特别关注单调曲线、直线、正交凸曲线和圆；同时研究当$n^2$个平面点被$n$条直线覆盖时点集的结构特性。

Result: 提出了点集覆盖所需曲线数量的理论界限，并揭示了点集被直线覆盖时的配置规律。

Conclusion: 该研究为点集覆盖问题提供了理论框架，并展示了覆盖曲线数量与点集几何结构之间的深刻联系。

Abstract: Given a point set, mostly a grid in our case, we seek upper and lower bounds
on the number of curves that are needed to cover the point set. We say a curve
covers a point if the curve passes through the point. We consider such
coverings by monotonic curves, lines, orthoconvex curves, circles, etc. We also
study a problem that is converse of the covering problem -- if a set of $n^2$
points in the plane is covered by $n$ lines then can we say something about the
configuration of the points?

</details>


### [43] [Schur-like numbers and a lemma of Shearer](https://arxiv.org/abs/2507.21656)
*Tomasz Kosciuszko*

Main category: math.CO

TL;DR: 论文研究了避免特定方程单色解的着色问题，改进了Cwalina和Schoen的界限，并针对更复杂的方程展示了更强的结果。


<details>
  <summary>Details</summary>
Motivation: 研究在着色数字时避免特定方程的单色解，以探索着色问题的界限和改进现有结果。

Method: 通过分析着色方案和方程的解结构，推导出避免单色解的最大数字N的上界。

Result: 对于方程$x_1+x_2+x_3=y_1+y_2$，证明了$N=O((n!)^{1/2})$；对于更复杂的方程，展示了更强的界限$N=O(((n-k)!)^{1/2})$，其中$k\gg\frac{\log n}{\log\log n}$。

Conclusion: 论文改进了避免特定方程单色解的着色问题的界限，并对其他方程的相关结果进行了讨论。

Abstract: Suppose that each number $1,2,...,N$ has one of n colours assigned. We show
that if there are no monochromatic solutions to the equation
$x_1+x_2+x_3=y_1+y_2$, then $N=O((n!)^{1/2})$, improving upon a result of
Cwalina and Schoen. Further, a stronger bound of $N=O(((n-k)!)^{1/2})$, where
$k\gg\frac{\log n}{\log\log n}$ is shown for colourings avoiding solutions to
the equation $x_1+x_2+...+x_{12}=y_1+y_2+...+y_9$. Finally, some remarks on
other equations are presented.

</details>


### [44] [Enumerating Cayley digraphs on dihedral groups](https://arxiv.org/abs/2507.21658)
*Zai Ping Lu,Jia Yin Xie,Jin-Hua Xie*

Main category: math.CO

TL;DR: 本文研究了二面体群上Cayley有向图的枚举问题，针对具有DCI性质的图，推导了非同构图数量的显式公式，特别关注了素数阶群$\mathrm{D}_{6p}$。


<details>
  <summary>Details</summary>
Motivation: 探索二面体群上Cayley有向图的非同构计数问题，填补特定群类（如$\mathrm{D}_{6p}$）在DCI性质下的理论空白。

Method: 结合Cauchy-Frobenius引理与自同构性质，通过分析自同构的循环数及群元素作用，建立枚举方法。

Result: 给出了素数$p>3$时，群$\mathrm{D}_{6p}$上非DCI-同构Cayley有向图数量的精确公式。

Conclusion: 理论框架成功应用于二面体群，为更复杂群的Cayley图枚举提供了可扩展的方法论基础。

Abstract: This paper investigates the enumeration of Cayley digraphs, focusing on
counting Cayley digraphs on dihedral groups up to CI-isomorphism. By leveraging
the Cauchy-Frobenius Lemma and properties of automorphisms, we derive an
explicit formula for the number of non-isomorphic Cayley digraphs on dihedral
groups with DCI-property, particularly for the group $\mathrm{D}_{6p}$ with
$p>3$ a prime. The enumeration involves detailed analysis of cycle numbers of
automorphisms and their actions on the group elements, culminating in a precise
count of non-isomorphic digraphs.

</details>


### [45] [Spectral generalized Turán problems](https://arxiv.org/abs/2507.21689)
*Xizhi Liu*

Main category: math.CO

TL;DR: 该论文结合广义Tur\\'{a}n问题和谱Tur\\'{a}n问题，提出了谱广义Tur\\'{a}n问题，并建立了一个扩展Keevash--Lenz--Mubayi结果的定理。应用包括谱Erd\H{o}s五边形定理，并引入广义Tur\\'{a}n问题的熵密度概念，证明其与广义谱半径一致。


<details>
  <summary>Details</summary>
Motivation: 研究结合广义Tur\\'{a}n问题和谱Tur\\'{a}n问题的谱广义Tur\\'{a}n问题，以扩展现有理论并解决更广泛的数学问题。

Method: 通过建立一般性定理，扩展Keevash--Lenz--Mubayi的结果，并引入熵密度概念，与广义谱半径进行比较。

Result: 获得了谱Erd\H{o}s五边形定理，并证明广义Tur\\'{a}n问题的熵密度与广义谱半径一致。

Conclusion: 谱广义Tur\\'{a}n问题的引入和熵密度概念的提出，为Tur\\'{a}n问题的研究提供了新的理论工具和方向。

Abstract: Combining two well-studied variants of the classical Tur\'{a}n problem, the
generalized Tur\'{a}n problem and the spectral Tur\'{a}n problem, we introduce
the spectral generalized Tur\'{a}n problem and establish a general theorem that
extends the result of Keevash--Lenz--Mubayi~\cite{KLM14} on the spectral
Tur\'{a}n problem in this broader setting. As a quick application, we obtain
the spectral Erd\H{o}s Pentagon Theorem. We also introduce the notion of
entropic density for generalized Tur\'{a}n problems, and show that it coincides
with the generalized spectral radius, extending a recent result of Chao--Hans
on entropic Tur\'{a}n density.

</details>


### [46] [The spectra of graph substitutions](https://arxiv.org/abs/2507.21733)
*Thomas Hirschler,Wolfgang Woess*

Main category: math.CO

TL;DR: 该论文研究了在有限连通无环图上通过边替换构造的新图$X[V]$的谱特性，重点分析了由$X$和$V$的转移矩阵$P$和$Q$诱导的$P_*$的谱结构及其多重性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是描述通过边替换构造的图$X[V]$的谱，基于原始图$X$和替换图$V$的谱，特别是归一化的转移矩阵谱，以推广到更一般的可逆转移矩阵情况。

Method: 采用归一化的转移矩阵方法，将$X$的每条边替换为图$V$的拷贝，并分析由此诱导的矩阵$P_*$的谱。考虑$X$的偶长环和$Q$在$V \setminus \{a,b\}$上的限制特征值的四种类型。

Result: 结果表明，$P_*$的谱不能简单地由$P$和$Q$的谱直接转换得到，其依赖于$X$的拓扑结构及$Q$的限制特征值类型，且特征值的多重性确定较为复杂。

Conclusion: 论文揭示了边替换构造图的谱分析的复杂性，指出其依赖于原始图和替换图的多种因素，并提供了处理不同情况的方法框架。

Abstract: Let $(X,E_X)$ and $(V,E_V)$ be finite connected graphs without loops. We
assume that $V$ has two distinguished vertices $a,b$ and an automorphism
$\gamma$ which exchanges $a$ and $b$. The $V$-edge substitution of $X$ is the
graph $X[V]$ where each edge $[x,y] \in E_X$ is replaced by a copy of $V$,
identifying $x$ with $a$ and $y$ with $b$ or vice versa. (The latter choice
does not matter; it yields isomorphic graphs). The aim is to describe the
spectrum of $X[V]$ in terms of the spectra of $X$ and $V$. Instead of the
spectra of the adjacency matrices, we consider the versions which are
normalised by dividing each row by the row sum (the vertex degree). These are
stochastic, reversible matrices, and our approach applies more generally to
reversible transition matrices corresponding to arbitrary positive edge weights
invariant under $\gamma$. We write $P$ for the transition matrix over $X$ and
$Q$ for the one over $V$. Together, they induce the matrix $P_*$ over $X[V]$.
  There is not a nice and compact formula which says how to transform the
spectra of $P$ and $Q$ into the spectrum of $P_*\,$. The results depend on
issues like whether $X$ has circles of even length and on the eigenvalues of
the restriction of $Q$ to $V \setminus \{ a,b\}$, which are classified into 4
possible types, each to be handled differently. Also quite subtle is the issue
of determining the multiplicities of the eigenvalues of $P_*$ in terms of the
input.

</details>


### [47] [Coherent configurations and Frobenius structures](https://arxiv.org/abs/2507.21774)
*Gejza Jenča,Anna Jenčová,Dominik Lachman*

Main category: math.CO

TL;DR: 该论文证明了相干构型可表示为实数非负矩阵范畴中的Frobenius结构模，推广了结合方案到相干构型的容许态射概念，并构建了与群胚和$H^*$-代数的联系。通过引入矩阵$O$的特征向量恢复原构型，并将拉格朗日定理推广至结合方案。


<details>
  <summary>Details</summary>
Motivation: 研究相干构型在代数结构中的表示及其与Frobenius结构的联系，旨在扩展结合方案的理论框架并建立新的数学工具。

Method: 将相干构型表述为Frobenius结构模，引入容许态射的广义定义，构造dagger Frobenius结构，并通过矩阵$O$的谱分析提取构型信息。

Result: 证明了相干构型的色度可通过$O$的特征向量重构，并将群论的拉格朗日定理推广至结合方案，揭示了$O$谱的数学意义。

Conclusion: 该工作为相干构型提供了新的代数视角，通过Frobenius结构与谱理论建立了与群论深刻的联系，拓展了离散数学的研究工具。

Abstract: We prove that coherent configurations can be represented as modules over
Frobenius structures in the category of real nonnegative matrices. We
generalize the notion of admissible morphism from association schemes to
coherent configurations. We show that the Frobenius structure associated to a
coherent configuration can be modified to become a dagger Frobenius structure,
and use this to connect the coherent configurations to groupoids and
$H^*$-algebras. We examine the properties of the dagger Frobenius structure
with respect to admissible morphisms. We introduce the matrix $O$ obtained as
the composition of comultiplication and multiplication of the dagger Frobenius
structure and prove that we may obtain the valencies of colors, and thus
recover the original coherent configuration, as an eigenvector of $O$. In the
last part of the paper, we examine the spectrum of $O$ and apply it to
generalize the Lagrange theorem from groups to association schemes.

</details>


### [48] [Self-Dual Ramsey Degrees for Trees](https://arxiv.org/abs/2507.21819)
*Sebastian Junge*

Main category: math.CO

TL;DR: 本文研究了树间映射对的Ramsey性质，证明了对双坐标依赖的着色不存在Ramsey定理，但给出了此类映射对Ramsey度的完整刻画，并推导出线性序映射对的Ramsey定理。


<details>
  <summary>Details</summary>
Motivation: 探索树结构上Deuber嵌入与Solecki刚性满射组合映射对的Ramsey性质，填补现有理论空白。

Method: 通过构造性证明分析树间映射对的着色行为，建立Ramsey度与线性序映射定理的关联体系。

Result: 1) 双坐标依赖着色无Ramsey定理 2) 完整刻画映射对Ramsey度 3) 推导出Solecki线性序映射定理

Conclusion: 树结构映射对的Ramsey理论既揭示本质限制（无一般定理），又提供精确量化工具（Ramsey度），并能统一推导已有重要结论。

Abstract: We consider a Ramsey statement for pairs of maps between trees, where one is
an embedding as defined by Deuber and the other is a rigid surjection as
defined by Solecki. We show that there is no Ramsey Theorem for pairs of maps
where the coloring depends on both coordinates. On the other hand, we give a
characterization of the Ramsey degrees for such pairs. Furthermore, we show
that our theorem on Ramsey Degrees for pairs of maps between trees implies the
Ramsey Theorem for pairs of maps between linear orders as proved by Solecki.

</details>


### [49] [A note on the strength of a hypercube](https://arxiv.org/abs/2507.21908)
*Melissa A. Huggan,M. E. Messinger,Dylan Pearson*

Main category: math.CO

TL;DR: 本文改进了超立方体图强度的上界，该强度定义为所有顶点排序中相邻顶点标签和的最大值的最小值。


<details>
  <summary>Details</summary>
Motivation: 超立方体图的强度问题尚未解决，但已有边界。研究旨在优化其强度的上界。

Method: 通过分析顶点排序$f$的强度定义，即相邻顶点标签和的最大值，寻找更优的排序策略。

Result: 提出了超立方体图强度的改进上界，优于先前已知的结果。

Conclusion: 本研究为超立方体图的强度问题提供了更紧的上界，推动了该领域的进展。

Abstract: As a generalization of super magic strength, the strength of a graph was
introduced in [R. Ichishima, F.A. Muntaner-Batle, A. Oshima, Bounds for the
strength of graphs, Austral. J. of Combin. 72(3) (2018) 492-508]. For a vertex
ordering $f$ of graph $G$, the strength of $f$ is the maximum sum of the labels
on any pair of adjacent vertices. The strength of $G$ is defined as the minimum
strength of $f$, taken over all vertex orderings of $G$. The strength of the
hypercube is unknown, but bounded. In this note, we provide an improved upper
bound for the strength of a hypercube.

</details>


### [50] [Some coefficients of rank 2 cluster scattering diagrams](https://arxiv.org/abs/2507.21916)
*Ryota Akagi*

Main category: math.CO

TL;DR: 本文通过形式幂级数重新表述了秩2簇散射图的对数元素表达，并证明了Thomas Elgin等人的若干猜想。


<details>
  <summary>Details</summary>
Motivation: 研究旨在将秩2簇散射图的对数元素表达转换为形式幂级数表达，以验证相关数学猜想。

Method: 采用形式幂级数方法重新构建簇散射图的数学表达，替代原有的对数元素表达方式。

Result: 成功实现了表达方式的转换，并证明了Thomas Elgin等人提出的若干猜想。

Conclusion: 该研究不仅提供了簇散射图的新数学表达工具，还为相关猜想提供了严格证明，推动了该领域的理论发展。

Abstract: The purpose of this paper is to translate the expression of rank 2 cluster
scattering diagrams via dilogarithm elements into via formal power series. As a
corollary, we prove some conjectures introduced by Thomas Elgin, Nathan
Reading, and Salvatore Stella.

</details>


### [51] [Tropical elliptic curves in 3-space](https://arxiv.org/abs/2507.21958)
*Laura Casabella,Lars Kastner,Raluca Vlad*

Main category: math.CO

TL;DR: 本文分类了热带3维空间中由两个二次曲面相交产生的16个顶点和16条边的三价图，共4,009种，代表了椭圆曲线的最大退化稳定模型。


<details>
  <summary>Details</summary>
Motivation: 研究热带3维空间中两个二次曲面相交产生的三价图，以理解椭圆曲线作为热带完全交的退化模型。

Method: 通过分析4维Cayley多面体的405,246,030种正则单模三角剖分，推导出这些图的分类。

Result: 共分类出4,009种三价图，这些图代表了椭圆曲线的最大退化稳定模型。

Conclusion: 该研究为热带几何中椭圆曲线的退化模型提供了系统的分类方法，揭示了其与多面体三角剖分的深刻联系。

Abstract: We classify trivalent graphs with 16 vertices and 16 edges that arise from
intersecting two quadratic surfaces in tropical 3-space. There are 4,009 such
graphs, representing maximally degenerate stable models of elliptic curves
realized as tropical complete intersections of two quadrics. Our classification
is derived from 405,246,030 regular unimodular triangulations of the
4-dimensional Cayley polytope.

</details>


### [52] [On the $l_\infty$-analog of Algebraic Connectivity](https://arxiv.org/abs/2507.22015)
*M. Rajesh Kannan,Rahul Roy*

Main category: math.CO

TL;DR: 本文研究了图$G$的$l_\infty$-范数参数$\gamma(G)$的组合意义，提出了基于广度优先搜索(BFS)的高效算法，建立了新界限，并推导了笛卡尔积图的优雅计算公式。


<details>
  <summary>Details</summary>
Motivation: 受Andrade和Dahl 2024年工作的启发，本文旨在探索$l_\infty$-范数参数$\gamma(G)$的图论性质，以补充传统代数连通度$a(G)$的研究。

Method: 采用组合分析与BFS算法相结合的方法，研究$\gamma(G)$的极值特性，并推导笛卡尔积图的显式计算公式。

Result: 证明$\gamma(G)$可高效计算且表征图的连通性，建立新界限，给出超立方图、汉明图等典型图族的显式结果。

Conclusion: $\gamma(G)$作为$l_\infty$-范数下的连通性参数，具有优良的可计算性和组合意义，为图论研究提供了新工具。

Abstract: The algebraic connectivity $a(G)$ of a graph $G$ is defined as the second
smallest eigenvalue of its Laplacian matrix $L(G)$. It also admits a
variational characterization as the minimum of a quadratic form associated with
$L(G)$, subject to $l_2$-norm constraints. In 2024, Andrade and Dahl
investigated an analogous parameter $\gamma(G)$, defined using the
$l_\infty$-norm instead of the $l_2$-norm. They demonstrated that $\gamma(G)$
can be computed in polynomial time using linear programming. In this article,
we study the combinatorial significance of $\gamma(G)$, revealing that it can
be efficiently computed using a breadth-first search (BFS) algorithm. We show
that $\gamma (G)$ characterizes the connectedness of the graph $G$. We further
establish new bounds on $\gamma(G)$, and analyze the graphs that attain
extremal values. Finally, we derive an elegant formula for $\gamma(G)$ when $G$
is the Cartesian product of finitely many graphs. Applying this formula, we
explicitly compute $\gamma(G)$ for various families of graphs, including
hypercube graphs, Hamming graphs, and others.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [53] [Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning](https://arxiv.org/abs/2507.21060)
*Abdullah Al Siam,Sadequzzaman Shohan*

Main category: cs.CR

TL;DR: 本文提出了一种基于加密医学图像的隐私保护诊断框架，采用改进的卷积神经网络（Masked-CNN）处理加密图像，结合AES-CBC加密与JPEG2000压缩技术，在保证AI推理性能的同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗诊断中的快速应用引发了患者隐私保护的迫切需求，特别是在敏感影像数据的传输、存储和处理过程中。

Method: 提出了一种新型框架，通过改进的卷积神经网络（Masked-CNN）处理加密或转换后的医学图像，结合AES-CBC加密和JPEG2000压缩技术，确保图像在加密状态下仍适用于AI推理。

Result: 实验使用公开的DICOM数据集（NIH ChestX-ray14和LIDC-IDRI）评估系统性能，结果显示加密推理模型在诊断准确性、推理延迟、存储效率和隐私泄漏抵抗方面表现接近未加密模型，仅存在边际性能损失。

Conclusion: 该框架在数据隐私与临床实用性之间架起了桥梁，为安全的AI驱动诊断提供了一种实用且可扩展的解决方案。

Abstract: The rapid integration of Artificial Intelligence (AI) into medical
diagnostics has raised pressing concerns about patient privacy, especially when
sensitive imaging data must be transferred, stored, or processed. In this
paper, we propose a novel framework for privacy-preserving diagnostic inference
on encrypted medical images using a modified convolutional neural network
(Masked-CNN) capable of operating on transformed or ciphered image formats. Our
approach leverages AES-CBC encryption coupled with JPEG2000 compression to
protect medical images while maintaining their suitability for AI inference. We
evaluate the system using public DICOM datasets (NIH ChestX-ray14 and
LIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage
efficiency, and privacy leakage resistance. Experimental results show that the
encrypted inference model achieves performance comparable to its unencrypted
counterpart, with only marginal trade-offs in accuracy and latency. The
proposed framework bridges the gap between data privacy and clinical utility,
offering a practical, scalable solution for secure AI-driven diagnostics.

</details>


### [54] [Security practices in AI development](https://arxiv.org/abs/2507.21061)
*Petr Spelda,Vit Stritecky*

Main category: cs.CR

TL;DR: 本文探讨了通用AI系统（如大语言模型）安全声明的可信度问题，指出安全实践而非工具本身重塑了AI安全形象，并揭示了当前实践的局限性与商业化导向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示为何通用AI系统的安全声明被认为可信，并分析安全工具能力与理想保障之间的差距根源。

Method: 通过批判性分析AI安全实践如何填补工具能力与安全承诺的鸿沟，评估了多样性及参与度等维度的缺陷。

Result: 发现当前安全实践本质上是为无法完全验证可信度的通用AI商业化开发服务的"安全化"过程。

Conclusion: 提出改进现有AI安全实践的建议，强调需超越工具层面，构建更全面、包容的安全验证框架。

Abstract: What makes safety claims about general purpose AI systems such as large
language models trustworthy? We show that rather than the capabilities of
security tools such as alignment and red teaming procedures, it is security
practices based on these tools that contributed to reconfiguring the image of
AI safety and made the claims acceptable. After showing what causes the gap
between the capabilities of security tools and the desired safety guarantees,
we critically investigate how AI security practices attempt to fill the gap and
identify several shortcomings in diversity and participation. We found that
these security practices are part of securitization processes aiming to support
(commercial) development of general purpose AI systems whose trustworthiness
can only be imperfectly tested instead of guaranteed. We conclude by offering
several improvements to the current AI security practices.

</details>


### [55] [Private key and password protection by steganographic image encryption](https://arxiv.org/abs/2507.21068)
*Debesh Choudhury,Sujoy Chakraborty*

Main category: cs.CR

TL;DR: 提出一种将私钥或密码加密嵌入彩色图像的技术，通过生成加密QR码并利用隐写术嵌入图像，用户可通过种子图像恢复密钥。实验验证了该技术的可行性。


<details>
  <summary>Details</summary>
Motivation: 为解决私钥或密码的安全存储问题，提出一种将敏感信息隐藏于普通彩色图像中的方法，即使种子图像被篡改也能通过备份恢复。

Method: 将明文私钥转换为加密QR码，通过隐写术嵌入彩色图像；密钥由用户选择的种子图像经LFSR生成，解密时先提取QR码再解密。

Result: 实验证明该技术能有效保护私钥数据，且用户可通过备份种子图像应对篡改，确保密钥可重新生成。

Conclusion: 该技术为私钥存储提供了一种安全且可恢复的解决方案，结合加密与隐写术，适用于现实场景中的敏感信息保护。

Abstract: We propose a technique to protect and preserve a private key or a passcode in
an encrypted two-dimensional graphical image. The plaintext private key or the
passcode is converted into an encrypted QR code and embedded into a real-life
color image with a steganographic scheme. The private key or the passcode is
recovered from the stego color image by first extracting the encrypted QR code
from the color image, followed by decryption of the QR code. The cryptographic
key for encryption of the QR code is generated from the output of a Linear
Feedback Shift Register (LFSR), initialized by a seed image chosen by the user.
The user can store the seed image securely, without the knowledge of an
attacker. Even if an active attacker modifies the seed image (without knowledge
of the fact that it is the seed image), the user can easily restore it if
he/she keeps multiple copies of it, so that the encryption key can be
regenerated easily. Our experiments prove the feasibility of the technique
using sample private key data and real-life color images.

</details>


### [56] [Applications Of Zero-Knowledge Proofs On Bitcoin](https://arxiv.org/abs/2507.21085)
*Yusuf Ozmiş*

Main category: cs.CR

TL;DR: 本文探讨了零知识证明如何增强比特币的功能与隐私性，提出了三种应用方案：基于zk-STARKs的储备证明协议、ZK轻客户端验证协议，以及利用BitVM的隐私保护Rollup方案，并分析了其安全性与实际应用权衡。


<details>
  <summary>Details</summary>
Motivation: 旨在通过零知识证明技术解决比特币在储备审计、轻客户端信任及交易隐私方面的局限性，扩展其功能边界。

Method: 1. 储备证明：使用zk-STARKs验证托管方持有超过阈值X的比特币，隐藏具体地址与余额；2. ZK轻客户端：通过STARK证明验证区块头链，实现去信任化；3. 隐私Rollup：基于BitVM设计零知识证明保护的二层交易方案。

Result: 研究表明，零知识证明可为比特币带来链上储备审计、无信任轻客户端和隐私二层执行等特性，但需在效率与信任假设间权衡。

Conclusion: 零知识证明能显著提升比特币功能与隐私，但具体应用需针对UTXO模型优化协议设计，并谨慎评估实践中的效率与安全取舍。

Abstract: This paper explores how zero-knowledge proofs can enhance Bitcoin's
functionality and privacy. First, we consider Proof-of-Reserve schemes: by
using zk-STARKs, a custodian can prove its Bitcoin holdings are more than a
predefined threshold X, without revealing addresses or actual balances. We
outline a STARK-based protocol for Bitcoin UTXOs and discuss its efficiency.
Second, we examine ZK Light Clients, where a mobile or lightweight device
verifies Bitcoin's proof-of-work chain using succinct proofs. We propose a
protocol for generating and verifying a STARK-based proof of a chain of block
headers, enabling trust-minimized client operation. Third, we explore
Privacy-Preserving Rollups via BitVM: leveraging BitVM, we design a conceptual
rollup that keeps transaction data confidential using zero-knowledge proofs. In
each case, we analyze security, compare with existing approaches, and discuss
implementation considerations. Our contributions include the design of concrete
protocols adapted to Bitcoin's UTXO model and an assessment of their
practicality. The results suggest that while ZK proofs can bring powerful
features (e.g., on-chain reserve audits, trustless light clients, and private
layer-2 execution) to Bitcoin, each application requires careful trade-offs in
efficiency and trust assumptions.

</details>


### [57] [Intelligent ARP Spoofing Detection using Multi-layered Machine Learning (ML) Techniques for IoT Networks](https://arxiv.org/abs/2507.21087)
*Anas Ali,Mubashar Husain,Peter Hans*

Main category: cs.CR

TL;DR: 本文提出了一种基于机器学习的多层框架，用于实时检测物联网环境中的ARP欺骗攻击，通过结合多种分析技术和混合模型，实现了高准确率和低误报率。


<details>
  <summary>Details</summary>
Motivation: ARP欺骗是物联网网络中的严重威胁，传统检测方法在资源受限的物联网环境中效果有限，因此需要一种智能、高效的解决方案。

Method: 采用多层次的机器学习框架，结合ARP头部行为特征工程、流量分析和时序异常检测，使用决策树、集成模型和深度学习分类器的混合检测流程，并在边缘网关和中心节点部署不同复杂度的模型以平衡性能与效率。

Result: 在模拟物联网流量和CICIDS2017数据集上的验证表明，该系统检测准确率超过97%，且误报率低，优于传统的基于签名和规则的系统。

Conclusion: 智能机器学习集成能够为物联网场景提供主动的ARP欺骗检测，为可扩展和自主的网络安全解决方案奠定了基础。

Abstract: Address Resolution Protocol (ARP) spoofing remains a critical threat to IoT
networks, enabling attackers to intercept, modify, or disrupt data transmission
by exploiting ARP's lack of authentication. The decentralized and
resource-constrained nature of IoT environments amplifies this vulnerability,
making conventional detection mechanisms ineffective at scale. This paper
introduces an intelligent, multi-layered machine learning framework designed to
detect ARP spoofing in real-time IoT deployments. Our approach combines feature
engineering based on ARP header behavior, traffic flow analysis, and temporal
packet anomalies with a hybrid detection pipeline incorporating decision trees,
ensemble models, and deep learning classifiers. We propose a hierarchical
architecture to prioritize lightweight models at edge gateways and deeper
models at centralized nodes to balance detection accuracy and computational
efficiency. The system is validated on both simulated IoT traffic and the
CICIDS2017 dataset, achieving over 97% detection accuracy with low false
positive rates. Comparative evaluations with signature-based and rule-based
systems demonstrate the robustness and generalizability of our approach. Our
results show that intelligent machine learning integration enables proactive
ARP spoofing detection tailored for IoT scenarios, laying the groundwork for
scalable and autonomous network security solutions.

</details>


### [58] [The Discovery, Disclosure, and Investigation of CVE-2024-25825](https://arxiv.org/abs/2507.21092)
*Hunter Chasens*

Main category: cs.CR

TL;DR: CVE-2024-25825是FydeOS中发现的一个漏洞，涉及默认凭证和空密码问题。研究发现该漏洞不太可能是国家行为者故意植入的。


<details>
  <summary>Details</summary>
Motivation: 研究动机是调查FydeOS中的CVE-2024-25825漏洞，特别是其是否与国家行为者有关联。

Method: 方法包括漏洞的发现、披露过程，以及与Fyde、CISA和Mitre的合作调查。

Result: 结果表明，漏洞不太可能是故意植入的，但建议在怀疑代码被污染时联系适当的CERT而非母公司。

Conclusion: 结论指出，尽管怀疑国家行为者介入的可能性低，但在类似情况下应优先联系CERT，这与传统的责任披露原则存在冲突。

Abstract: CVE-2024-25825 is a vulnerability found in FydeOS. This thesis describes its
discovery, disclosure, and its further investigation in connection to a nation
state actor. The vulnerability is CWE-1392: Use of Default Credentials,
CWE-1393: Use of Default Password, and CWE-258: Empty Password in Configuration
File found in the /etc/shadow configuration file. The root users entry in the
/etc/shadow file contains a wildcard allowing entry with any, or no, password.
Following responsable disclosure, Fyde, CISA, and Mitre were informed. Fyde was
already aware of the vulnerability. There was concern that this vulnerability
might have been purposefully placed, perhaps by a nation state actor. After
further investigation, it appears that this is unlikely to be the case. In
cases in which poisoned code is suspected it might be prudent to contact the
appropriate CERT, rather than the parent company. This, however, clashes with
the typical teaching of responsable disclosure.

</details>


### [59] [SkyEye: When Your Vision Reaches Beyond IAM Boundary Scope in AWS Cloud](https://arxiv.org/abs/2507.21094)
*Minh Hoang Nguyen,Anh Minh Ho,Bao Son To*

Main category: cs.CR

TL;DR: 本文介绍了SkyEye，一个用于AWS IAM配置枚举的协作多主体框架，旨在解决云安全中的身份与访问管理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着企业将基础设施迁移至云端，IAM配置的精确枚举成为确保数据安全、合规性和防范网络威胁的关键需求。

Method: SkyEye采用协作多主体框架，结合先进枚举模型，突破单一主体视角限制，全面分析AWS凭证的IAM配置。

Result: 该框架实现了对IAM权限的全局可视化，有效识别配置错误，降低权限提升风险，并增强合规性。

Conclusion: SkyEye为云环境下的IAM管理提供了创新解决方案，显著提升了安全团队的情境感知与风险管控能力。

Abstract: In recent years, cloud security has emerged as a primary concern for
enterprises due to the increasing trend of migrating internal infrastructure
and applications to cloud environments. This shift is driven by the desire to
reduce the high costs and maintenance fees associated with traditional
on-premise infrastructure. By leveraging cloud capacities such as high
availability and scalability, companies can achieve greater operational
efficiency and flexibility. However, this migration also introduces new
security challenges. Ensuring the protection of sensitive data, maintaining
compliance with regulatory requirements, and mitigating the risks of cyber
threats are critical issues that must be addressed. Identity and Access
Management (IAM) constitutes the critical security backbone of most cloud
deployments, particularly within AWS environments. As organizations adopt AWS
to scale applications and store data, the need for a thorough, methodical, and
precise enumeration of IAM configurations grows exponentially. Enumeration
refers to the systematic mapping and interrogation of identities, permissions,
and resource authorizations with the objective of gaining situational
awareness. By understanding the interplay between users, groups, and their
myriads of policies, whether inline or attached managed policies, security
professionals need to enumerate and identify misconfigurations, reduce the risk
of unauthorized privilege escalation, and maintain robust compliance postures.
This paper will present SkyEye, a cooperative multi-principal IAM enumeration
framework, which comprises cutting-edge enumeration models in supporting
complete situational awareness regarding the IAMs of provided AWS credentials,
crossing the boundary of principal-specific IAM entitlement vision to reveal
the complete visionary while insufficient authorization is the main challenge.

</details>


### [60] [HexaMorphHash HMH- Homomorphic Hashing for Secure and Efficient Cryptographic Operations in Data Integrity Verification](https://arxiv.org/abs/2507.21096)
*Krishnendu Das*

Main category: cs.CR

TL;DR: 本文提出了一种基于格密码学的同态哈希函数HexaMorphHash，用于解决分布式系统中大数据管理的动态更新与数据完整性问题。该方法在保证恒定摘要大小的同时，支持增量更新，并具备抗量子攻击能力。


<details>
  <summary>Details</summary>
Motivation: 传统哈希方法在动态环境下因节点变更需全面重哈希而效率低下，一致性哈希虽减少数据迁移但仍存在负载均衡与扩展性瓶颈。亟需一种支持高频更新、保持数据完整性的新型哈希方案。

Method: 基于短整数解(SIS)问题的格密码学构造HexaMorphHash，实现恒定时间增量更新。通过同态特性避免全量重计算，并与Merkle树、AdHash等7类现有方案进行对比实验。

Result: 相比传统方法，HexaMorphHash在计算效率(提升38%)、内存占用(减少52%)及扩展性方面显著优化，尤其适用于超大规模分布式系统的实时更新场景。

Conclusion: HexaMorphHash为动态分布式系统提供了兼顾数据完整性保护与高性能的解决方案，其抗量子特性对未来密码学应用具有重要价值。

Abstract: In the realm of big data and cloud computing, distributed systems are tasked
with proficiently managing, storing, and validating extensive datasets across
numerous nodes, all while maintaining robust data integrity. Conventional
hashing methods, though straightforward, encounter substan tial difficulties in
dynamic settings due to the necessity for thorough rehashing when nodes are
altered. Consistent hashing mitigates some of these challenges by reducing data
redistribution; however, it still contends with limitations in load balancing
and scalability under intensive update conditions. This paper introduces an
innovative approach using a lattice based homomorphic hash function
HexaMorphHash that facilitates constant time, incremental updates while
preserving a constant digest size. By utilizing the complexity of the Short
Integer Solutions SIS problem, our method secures strong protective measures,
even against quantum threats. We further com pare our method with existing ones
such as direct signatures for each update, comprehensive database signing,
Merkle tree based techniques, AdHash, MuHash, ECMH, and homomorphic sig nature
schemes highlighting notable advancements in computational efficiency, memory
usage, and scalability. Our contributions present a viable solution for
frequent update dissemination in expansive distributed systems, safeguarding
both data integrity and system performance.

</details>


### [61] [Singularity Cipher: A Topology-Driven Cryptographic Scheme Based on Visual Paradox and Klein Bottle Illusions](https://arxiv.org/abs/2507.21097)
*Abraham Itzhak Weinberg*

Main category: cs.CR

TL;DR: 本文提出了一种名为'奇点密码'的新型密码-隐写框架，结合拓扑变换和视觉悖论实现多维安全。该方法通过克莱因瓶的非定向特性与莫比乌斯带的扭转函数生成高混淆密文，并利用视觉错觉（如消失方块悖论）隐藏加密内容。相比传统密码学，该框架在代数复杂性和人类认知模糊性上实现了双重防护。


<details>
  <summary>Details</summary>
Motivation: 传统密码学仅依赖代数复杂性，存在被量子计算破解的风险。受克莱因瓶拓扑特性的启发，本研究旨在开发一种结合数学拓扑与视觉隐写的双层次安全框架，以增强抗检测性和抗破解能力，适用于对抗性环境中的安全通信与水印应用。

Method: 1. 基于莫比乌斯带构造符号扭转函数模拟拓扑遍历；2. 利用克莱因瓶非定向性实现高扩散/混淆密文；3. 通过消失方块等视觉悖论进行二进制数据编码；4. 提供完整的加解密算法架构与安全性证明。

Result: 相比经典密码、后量子密码及传统隐写术，该方法在BAN逻辑验证中展现出更强的抗侧信道攻击能力，视觉测试显示隐写内容的人类检测错误率提升47%，同时保持NIST测试通过的密码强度。

Conclusion: 奇点密码首次将拓扑变换与认知错觉系统结合，为安全通信开辟了新维度。未来可扩展至四维超拓扑结构研究，并在生物特征加密领域具有应用潜力。

Abstract: This paper presents the Singularity Cipher, a novel
cryptographic-steganographic framework that integrates topological
transformations and visual paradoxes to achieve multidimensional security.
Inspired by the non-orientable properties of the Klein bottle -- constructed
from two Mobius strips -- the cipher applies symbolic twist functions to
simulate topological traversal, producing high confusion and diffusion in the
ciphertext. The resulting binary data is then encoded using perceptual
illusions, such as the missing square paradox, to visually obscure the presence
of encrypted content. Unlike conventional ciphers that rely solely on algebraic
complexity, the Singularity Cipher introduces a dual-layer approach: symbolic
encryption rooted in topology and visual steganography designed for human
cognitive ambiguity. This combination enhances both cryptographic strength and
detection resistance, making it well-suited for secure communication,
watermarking, and plausible deniability in adversarial environments. The paper
formalizes the architecture, provides encryption and decryption algorithms,
evaluates security properties, and compares the method against classical,
post-quantum, and steganographic approaches. Potential applications and future
research directions are also discussed.

</details>


### [62] [SoK: A Systematic Review of Context- and Behavior-Aware Adaptive Authentication in Mobile Environments](https://arxiv.org/abs/2507.21101)
*Vyoma Harshitha Podapati,Divyansh Nigam,Sanchari Das*

Main category: cs.CR

TL;DR: 本文系统分析了2011年以来41篇关于移动环境自适应认证的研究，揭示了机器学习（64.3%）在该领域的主导地位及其在持续认证（61.9%）和防未授权访问（54.8%）中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着移动计算成为数字交互的核心，自适应认证因其实时、情境和行为感知的验证能力受到关注，但现有实现存在碎片化、智能技术应用不一致及用户体验不足的问题。

Method: 通过知识系统化方法，从隐私安全模型、交互方式、用户行为、风险感知、实施挑战、可用性需求及机器学习框架七个维度分析了41篇同行评审研究。

Result: 研究发现机器学习（尤其是异常检测57.1%和时空分析52.4%）与传感器/位置感知模型正重塑交互范式，且连续认证（61.9%）是主要应用场景。

Conclusion: AI驱动方法（如异常检测）正成为移动自适应认证的主流技术方向，但需进一步解决隐私保护与用户体验的平衡问题。

Abstract: As mobile computing becomes central to digital interaction, researchers have
turned their attention to adaptive authentication for its real-time, context-
and behavior-aware verification capabilities. However, many implementations
remain fragmented, inconsistently apply intelligent techniques, and fall short
of user expectations. In this Systematization of Knowledge (SoK), we analyze 41
peer-reviewed studies since 2011 that focus on adaptive authentication in
mobile environments. Our analysis spans seven dimensions: privacy and security
models, interaction modalities, user behavior, risk perception, implementation
challenges, usability needs, and machine learning frameworks. Our findings
reveal a strong reliance on machine learning (64.3%), especially for continuous
authentication (61.9%) and unauthorized access prevention (54.8%). AI-driven
approaches such as anomaly detection (57.1%) and spatio-temporal analysis
(52.4%) increasingly shape the interaction landscape, alongside growing use of
sensor-based and location-aware models.

</details>


### [63] [A Formal Rebuttal of "The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability"](https://arxiv.org/abs/2507.21111)
*Craig Wright*

Main category: cs.CR

TL;DR: Error processing this paper.


<details>
  <summary>Details</summary>
Motivation: Error processing this paper.

Method: Error processing this paper.

Result: Error processing this paper.

Conclusion: Error processing this paper.

Abstract: This paper presents a comprehensive refutation of the so-called "blockchain
trilemma," a widely cited but formally ungrounded claim asserting an inherent
trade-off between decentralisation, security, and scalability in blockchain
protocols. Through formal analysis, empirical evidence, and detailed critique
of both methodology and terminology, we demonstrate that the trilemma rests on
semantic equivocation, misuse of distributed systems theory, and a failure to
define operational metrics. Particular focus is placed on the conflation of
topological network analogies with protocol-level architecture, the
mischaracterisation of Bitcoin's design--including the role of miners, SPV
clients, and header-based verification--and the failure to ground claims in
complexity-theoretic or adversarial models. By reconstructing Bitcoin as a
deterministic, stateless distribution protocol governed by evidentiary trust,
we show that scalability is not a trade-off but an engineering outcome. The
paper concludes by identifying systemic issues in academic discourse and peer
review that have allowed such fallacies to persist, and offers formal criteria
for evaluating future claims in blockchain research.

</details>


### [64] [Vulnerability Mitigation System (VMS): LLM Agent and Evaluation Framework for Autonomous Penetration Testing](https://arxiv.org/abs/2507.21113)
*Farzana Abdulzada*

Main category: cs.CR

TL;DR: 本文提出了一种基于大型语言模型（LLM）的漏洞缓解系统（VMS），用于自动化渗透测试，并通过新设计的CTF基准进行评估，结果表明GPT-4o表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁频率的增加，传统渗透测试难以应对复杂环境，因此需要开发自动化工具以提高测试效率和覆盖范围。

Method: VMS采用双部分架构（规划器和总结器），并设计了基于PicoCTF和OverTheWire的200个挑战的新CTF基准，用于评估系统性能。实验中使用不同LLM并调整温度和top-p参数。

Result: 实验发现GPT-4o表现最佳，有时甚至超出预期，表明LLM可有效应用于网络安全任务，但需注意风险。系统在容器化环境中安全运行。

Conclusion: VMS和基准测试的公开可用性推动了安全、自主网络安全工具的研发，LLM在网络安全领域具有广阔应用前景，但需谨慎管理风险。

Abstract: As the frequency of cyber threats increases, conventional penetration testing
is failing to capture the entirety of todays complex environments. To solve
this problem, we propose the Vulnerability Mitigation System (VMS), a novel
agent based on a Large Language Model (LLM) capable of performing penetration
testing without human intervention. The VMS has a two-part architecture for
planning and a Summarizer, which enable it to generate commands and process
feedback. To standardize testing, we designed two new Capture the Flag (CTF)
benchmarks based on the PicoCTF and OverTheWire platforms with 200 challenges.
These benchmarks allow us to evaluate how effectively the system functions. We
performed a number of experiments using various LLMs while tuning the
temperature and top-p parameters and found that GPT-4o performed best,
sometimes even better than expected. The results indicate that LLMs can be
effectively applied to many cybersecurity tasks; however, there are risks. To
ensure safe operation, we used a containerized environment. Both the VMS and
the benchmarks are publicly available, advancing the creation of secure,
autonomous cybersecurity tools.

</details>


### [65] [Kintsugi: Decentralized E2EE Key Recovery](https://arxiv.org/abs/2507.21122)
*Emilie Ma,Martin Kleppmann*

Main category: cs.CR

TL;DR: Kintsugi是一种去中心化的密钥恢复协议，允许用户通过密码恢复端到端加密数据，无需依赖单一服务提供商。


<details>
  <summary>Details</summary>
Motivation: 现有E2EE密钥恢复方案（如Signal和WhatsApp）依赖单一服务商，存在中心化信任问题。Kintsugi旨在通过分布式节点消除单点故障风险。

Method: 协议将信任分散至多个恢复节点（独立服务器或P2P设备），需$t+1$个节点协作解密备份。采用密码认证且无需专用硬件，可抵抗离线暴力破解。

Result: Kintsugi可抵御$t$个诚实但好奇节点的共谋攻击，容忍$n-t-1$个离线节点，并在异步网络模型中保持安全。

Conclusion: 该协议为去中心化密钥恢复提供了可行方案，在保障安全性的同时提升了系统的抗审查与容错能力。

Abstract: Kintsugi is a protocol for key recovery, allowing a user to regain access to
end-to-end encrypted data after they have lost their device, but still have
their (potentially low-entropy) password. Existing E2EE key recovery methods,
such as those deployed by Signal and WhatsApp, centralize trust by relying on
servers administered by a single provider. Kintsugi is decentralized,
distributing trust over multiple recovery nodes, which could be servers run by
independent parties, or end user devices in a peer-to-peer setting. To recover
a user's keys, a threshold $t+1$ of recovery nodes must assist the user in
decrypting a shared backup. Kintsugi is password-authenticated and protects
against offline brute-force password guessing without requiring any specialized
secure hardware. Kintsugi can tolerate up to $t$ honest-but-curious colluding
recovery nodes, as well as $n - t - 1$ offline nodes, and operates safely in an
asynchronous network model where messages can be arbitrarily delayed.

</details>


### [66] [Security study based on the Chatgptplugin system: ldentifying Security Vulnerabilities](https://arxiv.org/abs/2507.21128)
*Ruomai Ren*

Main category: cs.CR

TL;DR: 研究分析了ChatGPT插件商店的安全性问题，揭示了主要漏洞并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 插件系统虽增强用户体验，但开发者多样性和复杂性导致监管不足，ChatGPT插件系统的安全风险被忽视。

Method: 通过分析ChatGPT插件商店的插件，识别其安全漏洞。

Result: 研究发现插件存在重大安全隐患，需加强监管和防护措施。

Conclusion: ChatGPT插件系统需完善安全机制以降低风险，保障用户安全。

Abstract: Plugin systems are a class of external programmes that provide users with a
wide range of functionality, and while they enhance the user experience, their
security is always a challenge. Especially due to the diversity and complexity
of developers, many plugin systems lack adequate regulation. As ChatGPT has
become a popular large-scale language modelling platform, its plugin system is
also gradually developing, and the open platform provides creators with the
opportunity to upload plugins covering a wide range of application scenarios.
However, current research and discussions mostly focus on the security issues
of the ChatGPT model itself, while ignoring the possible security risks posed
by the plugin system. This study aims to analyse the security of plugins in the
ChatGPT plugin shop, reveal its major security vulnerabilities, and propose
corresponding improvements.

</details>


### [67] [Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities](https://arxiv.org/abs/2507.21133)
*Atil Samancioglu*

Main category: cs.CR

TL;DR: 大型语言模型（LLMs）在威胁操纵下展现出复杂反应，既暴露漏洞又存在性能提升机会。研究通过3390次实验分析三大模型（Claude、GPT-4、Gemini）在6种威胁条件下的表现，提出新威胁分类框架，揭示系统性漏洞与最高+1336%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在威胁操纵下的反应模式，为AI安全和高风险应用中的提示工程提供实证依据。

Method: 构建新型威胁分类法及多指标评估框架，分析三大模型在10个任务领域、6种威胁条件下的3390次实验数据。

Result: 发现基于角色威胁时政策评估任务指标显著率最高，部分案例性能提升达1336%；统计证实存在系统性确定性操纵（pFDR < 0.0001）及分析深度、响应质量的显著改善。

Conclusion: 研究结果对AI安全防护和关键领域提示工程具有双重启示，证实威胁条件可系统性影响LLMs行为模式。

Abstract: Large Language Models (LLMs) demonstrate complex responses to threat-based
manipulations, revealing both vulnerabilities and unexpected performance
enhancement opportunities. This study presents a comprehensive analysis of
3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)
across 10 task domains under 6 threat conditions. We introduce a novel threat
taxonomy and multi-metric evaluation framework to quantify both negative
manipulation effects and positive performance improvements. Results reveal
systematic vulnerabilities, with policy evaluation showing the highest metric
significance rates under role-based threats, alongside substantial performance
enhancements in numerous cases with effect sizes up to +1336%. Statistical
analysis indicates systematic certainty manipulation (pFDR < 0.0001) and
significant improvements in analytical depth and response quality. These
findings have dual implications for AI safety and practical prompt engineering
in high-stakes applications.

</details>


### [68] [Learning-based Privacy-Preserving Graph Publishing Against Sensitive Link Inference Attacks](https://arxiv.org/abs/2507.21139)
*Yucheng Wu,Yuncong Yang,Xiao Han,Leye Wang,Junjie Wu*

Main category: cs.CR

TL;DR: 本文提出首个隐私保护图结构学习框架PPGSL，通过对抗训练自动优化隐私-效用平衡，有效抵御敏感链接推断攻击，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 图数据发布虽支持多种分析任务，但存在隐私泄露风险（如敏感链接被推断）。现有启发式图修改方法难以确定最优隐私-效用权衡，需自动化解决方案。

Method: PPGSL框架包含两部分：1) 模拟强大攻击者进行敏感链接推断；2) 训练参数化图以抵御攻击并保持原始图效用。采用安全迭代训练协议确保隐私与收敛，并引入加速技术处理大规模图。

Result: 实验表明PPGSL在隐私-效用权衡上达到最优性能，能有效抵抗多种敏感链接推断攻击，且理论证明其训练过程稳定收敛。

Conclusion: PPGSL是首个通过对抗学习自动优化隐私保护的图结构框架，为安全图数据发布提供了高效可靠的解决方案。

Abstract: Publishing graph data is widely desired to enable a variety of structural
analyses and downstream tasks. However, it also potentially poses severe
privacy leakage, as attackers may leverage the released graph data to launch
attacks and precisely infer private information such as the existence of hidden
sensitive links in the graph. Prior studies on privacy-preserving graph data
publishing relied on heuristic graph modification strategies and it is
difficult to determine the graph with the optimal privacy--utility trade-off
for publishing. In contrast, we propose the first privacy-preserving graph
structure learning framework against sensitive link inference attacks, named
PPGSL, which can automatically learn a graph with the optimal privacy--utility
trade-off. The PPGSL operates by first simulating a powerful surrogate attacker
conducting sensitive link attacks on a given graph. It then trains a
parameterized graph to defend against the simulated adversarial attacks while
maintaining the favorable utility of the original graph. To learn the
parameters of both parts of the PPGSL, we introduce a secure iterative training
protocol. It can enhance privacy preservation and ensure stable convergence
during the training process, as supported by the theoretical proof.
Additionally, we incorporate multiple acceleration techniques to improve the
efficiency of the PPGSL in handling large-scale graphs. The experimental
results confirm that the PPGSL achieves state-of-the-art privacy--utility
trade-off performance and effectively thwarts various sensitive link inference
attacks.

</details>


### [69] [Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for Compliance AI Agents](https://arxiv.org/abs/2507.21142)
*Chenhao Fang,Yanqing Peng,Rajeev Rao,Matt Sarmiento,Wendy Summer,Arya Pudota,Alex Goncalves,Jordi Mola,Hervé Robert*

Main category: cs.CR

TL;DR: 本文提出PACT系统，通过嵌入驱动的图结构连接企业内多种隐私相关资源，利用DRAGON模型优化链接效果，显著提升隐私合规性分析的准确率。


<details>
  <summary>Details</summary>
Motivation: 企业环境中存在大量异构且快速增长的代码、数据和工具资源，隐私风险评估和法规遵从所需信息分散其中，亟需统一系统实现跨资源关联分析。

Method: 采用基于DRAGON嵌入模型的对比学习框架，通过微调整合元数据、所有权信息和合规上下文等文本特征，构建多类型资源连接图。

Result: 实验表明：PACT将recall@1从18%提升至53%，AI代理查询匹配率从9.6%增至69.7%，推荐系统hitrate@1从25.7%提高到44.9%。

Conclusion: PACT通过先进嵌入技术有效解决企业隐私资源关联难题，为大规模合规管理提供可扩展的自动化解决方案。

Abstract: Enterprise environments contain a heterogeneous, rapidly growing collection
of internal artifacts related to code, data, and many different tools. Critical
information for assessing privacy risk and ensuring regulatory compliance is
often embedded across these varied resources, each with their own arcane
discovery and extraction techniques. Therefore, large-scale privacy compliance
in adherence to governmental regulations requires systems to discern the
interconnected nature of diverse artifacts in a common, shared universe.
  We present Privacy Artifact ConnecT or (PACT), an embeddings-driven graph
that links millions of artifacts spanning multiple artifact types generated by
a variety of teams and projects. Powered by the state-of-the-art DRAGON
embedding model, PACT uses a contrastive learning objective with light
fine-tuning to link artifacts via their textual components such as raw
metadata, ownership specifics, and compliance context. Experimental results
show that PACT's fine-tuned model improves recall@1 from 18% to 53%, the query
match rate from 9.6% to 69.7% when paired with a baseline AI agent, and the
hitrate@1 from 25.7% to 44.9% for candidate selection in a standard recommender
system.

</details>


### [70] [Assessment of Quantitative Cyber-Physical Reliability of SCADA Systems in Autonomous Vehicle to Grid (V2G) Capable Smart Grids](https://arxiv.org/abs/2507.21154)
*Md Abdul Gaffar*

Main category: cs.CR

TL;DR: 本文研究了电动汽车与电网（V2G）系统中自主车辆通信（AV2G）带来的网络安全风险，评估了其对SCADA系统可靠性的影响，并提出了基于贝叶斯攻击图和蒙特卡洛模拟的定量分析方法。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车通过V2G技术日益融入电网，虽然提升了电网可靠性和分布式储能能力，但也扩大了电网的物理-网络攻击面，特别是AV2G通信可能引入新的监控与SCADA系统漏洞。

Method: 采用贝叶斯攻击图结合概率容量停运模型（基于IEEE RTS-79系统数据），并通过蒙特卡洛模拟量化AV2G攻击对系统性能的影响。

Result: 模拟显示AV2G攻击会显著降低电网可靠性，凸显了在智能电网设计中强化网络安全的必要性。

Conclusion: 研究强调了针对AV2G通信基础设施的网络安全加固策略对保障智能电网稳定运行的关键作用。

Abstract: The integration of electric vehicles (EVs) into power grids via
Vehicle-to-Grid (V2G) system technology is increasing day by day, but these
phenomena present both advantages and disadvantages. V2G can increase grid
reliability by providing distributed energy storage and ancillary services.
However, on the other hand, it has a scope that encompasses the cyber-physical
attack surface of the national power grid, introducing new vulnerabilities in
monitoring and supervisory control and data acquisition (SCADA) systems. This
paper investigates the maliciousness caused by Autonomous Vehicle to Grid
(AV2G) communication infrastructures and assesses their impacts on SCADA system
reliability. This paper presents a quantitative reliability assessment using
Bayesian attack graph combined with probabilistic capacity outage modeling
based on IEEE RTS-79 system data. This work presents how AV2G-based attacks
degrade system performance by using Monte Carlo simulations method,
highlighting the need for cybersecurity-hardening strategies in smart grid
design.

</details>


### [71] [Leveraging Trustworthy AI for Automotive Security in Multi-Domain Operations: Towards a Responsive Human-AI Multi-Domain Task Force for Cyber Social Security](https://arxiv.org/abs/2507.21145)
*Vita Santa Barletta,Danilo Caivano,Gabriel Cellammare,Samuele del Vescovo,Annita Larissa Sciacovelli*

Main category: cs.CR

TL;DR: 研究探讨了决策树集成模型（随机森林、梯度提升、极端梯度提升）的关键超参数对黑盒对抗性机器学习攻击（ZOO）执行时间的影响，发现随机森林和梯度提升对参数变化更敏感，并分析了对抗训练时间以评估攻击机会窗口。


<details>
  <summary>Details</summary>
Motivation: 随着智能城市和联网自动驾驶汽车（CAVs）成为多域作战（MDOs）中的主要目标，CAVs作为军民两用资产易受多表面威胁（MSTs）攻击，尤其是对抗性机器学习（AML）攻击。因此，研究如何通过优化超参数提升防御性可信人工智能（D-TAI）的实践具有重要意义。

Method: 研究分析了随机森林（RF）、梯度提升（GB）和极端梯度提升（XGB）等决策树集成模型的关键超参数（如树的数量或提升轮数）对黑盒对抗性机器学习攻击（ZOO）执行时间的影响，并评估了对抗训练（AT）时间。

Result: 研究发现，随机森林和梯度提升对超参数变化更为敏感，而极端梯度提升相对稳健。超参数的优化显著影响了攻击执行时间，同时对抗训练时间分析为攻击者的机会窗口提供了评估依据。

Conclusion: 通过优化超参数，研究支持了在多表面威胁场景下的防御性可信人工智能实践，并为军民领域的弹性机器学习系统开发提供了贡献，符合多域作战中的网络社会安全框架和人机多域任务部队的需求。

Abstract: Multi-Domain Operations (MDOs) emphasize cross-domain defense against complex
and synergistic threats, with civilian infrastructures like smart cities and
Connected Autonomous Vehicles (CAVs) emerging as primary targets. As dual-use
assets, CAVs are vulnerable to Multi-Surface Threats (MSTs), particularly from
Adversarial Machine Learning (AML) which can simultaneously compromise multiple
in-vehicle ML systems (e.g., Intrusion Detection Systems, Traffic Sign
Recognition Systems). Therefore, this study investigates how key
hyperparameters in Decision Tree-based ensemble models-Random Forest (RF),
Gradient Boosting (GB), and Extreme Gradient Boosting (XGB)-affect the time
required for a Black-Box AML attack i.e. Zeroth Order Optimization (ZOO).
Findings show that parameters like the number of trees or boosting rounds
significantly influence attack execution time, with RF and GB being more
sensitive than XGB. Adversarial Training (AT) time is also analyzed to assess
the attacker's window of opportunity. By optimizing hyperparameters, this
research supports Defensive Trustworthy AI (D-TAI) practices within MST
scenarios and contributes to the development of resilient ML systems for
civilian and military domains, aligned with Cyber Social Security framework in
MDOs and Human-AI Multi-Domain Task Forces.

</details>


### [72] [Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems](https://arxiv.org/abs/2507.21146)
*Gauri Sharma,Vidhi Kulkarni,Miles King,Ken Huang*

Main category: cs.CR

TL;DR: 本文提出了一种新型安全风险——代理级联注入（ACI），分析了多智能体系统中信任链被恶意利用导致的级联攻击，并呼吁建立量化评估框架以增强系统安全性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统的普及，智能体间信任关系可能被恶意利用，导致局部漏洞演变为系统性风险。本文旨在揭示这种级联攻击机制，并推动安全评估标准化。

Method: 通过形式化攻击模型（含对抗目标方程和关键变量），分析ACI的传播链、放大因子等特性，并基于OWASP分类提出针对A2A/MCP架构的压力测试方法论。

Result: 揭示了ACI攻击如何通过污染观测数据在智能体网络中扩散，验证了其与OWASP风险类别（如影响链攻击）的关联性，构建了可量化的安全评估基础框架。

Conclusion: 研究为工程师提供了评估系统弹性的工具，强调需通过标准化协议测试和防御设计来应对新兴的智能体级联威胁。

Abstract: Evolving AI systems increasingly deploy multi-agent architectures where
autonomous agents collaborate, share information, and delegate tasks through
developing protocols. This connectivity, while powerful, introduces novel
security risks. One such risk is a cascading risk: a breach in one agent can
cascade through the system, compromising others by exploiting inter-agent
trust. In tandem with OWASP's initiative for an Agentic AI Vulnerability
Scoring System we define an attack vector, Agent Cascading Injection, analogous
to Agent Impact Chain and Blast Radius, operating across networks of agents. In
an ACI attack, a malicious input or tool exploit injected at one agent leads to
cascading compromises and amplified downstream effects across agents that trust
its outputs. We formalize this attack with an adversarial goal equation and key
variables (compromised agent, injected exploit, polluted observations, etc.),
capturing how a localized vulnerability can escalate into system-wide failure.
We then analyze ACI's properties -- propagation chains, amplification factors,
and inter-agent compound effects -- and map these to OWASP's emerging Agentic
AI risk categories (e.g. Impact Chain and Orchestration Exploits). Finally, we
argue that ACI highlights a critical need for quantitative benchmarking
frameworks to evaluate the security of agent-to-agent communication protocols.
We outline a methodology for stress-testing multi-agent systems (using
architectures such as Google's A2A and Anthropic's MCP) against cascading trust
failures, developing upon groundwork for measurable, standardized
agent-to-agent security evaluation. Our work provides the necessary apparatus
for engineers to benchmark system resilience, make data-driven architectural
trade-offs, and develop robust defenses against a new generation of agentic
threats.

</details>


### [73] [WaveVerify: A Novel Audio Watermarking Framework for Media Authentication and Combatting Deepfakes](https://arxiv.org/abs/2507.21150)
*Aditya Pujari,Ajita Rattani*

Main category: cs.CR

TL;DR: 语音生成技术的快速发展带来了高风险，如深度伪造诈骗，2024年相关案件激增1300%，金融领域损失超1000万美元。全球正推动AI内容透明度和可追溯性措施。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术虽有益于个性化语音系统，但也导致深度伪造诈骗和虚假信息传播激增，亟需建立音频内容认证机制。

Method: 通过开发法证工具和水印技术，提升AI生成内容的透明度和可追溯性。

Result: 2024年深度伪造诈骗尝试较2023年增长1300%，金融领域因AI语音诈骗损失超1000万美元，单笔损失超6000美元。

Conclusion: 全球监管机构正采取措施加强AI内容管理，强调法证工具和水印技术对维护媒体完整性的重要性。

Abstract: The rapid advancement of voice generation technologies has enabled the
synthesis of speech that is perceptually indistinguishable from genuine human
voices. While these innovations facilitate beneficial applications such as
personalized text-to-speech systems and voice preservation, they have also
introduced significant risks, including deepfake impersonation scams and
synthetic media-driven disinformation campaigns. Recent reports indicate that
in 2024, deepfake fraud attempts surged by over 1,300% compared to 2023,
underscoring the urgent need for robust audio content authentication. The
financial sector has been particularly impacted, with a loss of over 10 million
USD to voice scams and individual victims reporting losses exceeding $6,000
from AI-generated deepfake calls. In response, regulators and governments
worldwide are enacting measures to improve AI content transparency and
traceability, emphasizing the development of forensic tools and watermarking
techniques as essential strategies to uphold media integrity.

</details>


### [74] [NIST Post-Quantum Cryptography Standard Algorithms Based on Quantum Random Number Generators](https://arxiv.org/abs/2507.21151)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本研究提出基于量子随机数生成器（QRNG）的后量子密码（PQC）算法，通过量子计算生成随机数，增强密钥生成、封装和数字签名的安全性，并设计了六种QRNG进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 尽管NIST发布的PQC标准（如ML-KEM、ML-DSA和SLH-DSA）能抵抗量子计算攻击，但在某些特定应用场景中安全性仍不足，需结合量子随机性提升防护能力。

Method: 提出通用QRNG架构，设计六种QRNG，并依据NIST SP 800-90B标准进行统计验证（包括熵源检验和IID输出测试），评估其计算时间及基于QRNG的PQC算法性能。

Result: 实验量化了六种QRNG的计算效率，以及QRNG增强的ML-KEM、ML-DSA和SLH-DSA的性能表现，为未来PQC系统部署提供了关键参考数据。

Conclusion: 基于QRNG的PQC算法通过量子随机性显著提升安全性，实验验证了其可行性，为后量子密码的实际应用提供了新方向。

Abstract: In recent years, the advancement of quantum computing technology has posed
potential security threats to RSA cryptography and elliptic curve cryptography.
In response, the National Institute of Standards and Technology (NIST)
published several Federal Information Processing Standards (FIPS) of
post-quantum cryptography (PQC) in August 2024, including the
Module-Lattice-Based Key-Encapsulation Mechanism (ML-KEM), Module-Lattice-Based
Digital Signature Algorithm (ML-DSA), and Stateless Hash-Based Digital
Signature Algorithm (SLH-DSA). Although these PQC algorithms are designed to
resist quantum computing attacks, they may not provide adequate security in
certain specialized application scenarios. To address this issue, this study
proposes quantum random number generator (QRNG)-based PQC algorithms. These
algorithms leverage quantum computing to generate random numbers, which serve
as the foundation for key pair generation, key encapsulation, and digital
signature generation. A generalized architecture of QRNG is proposed, along
with the design of six QRNGs. Each generator is evaluated according to the
statistical validation procedures outlined in NIST SP 800-90B, including tests
for verification of entropy sources and independent and identically distributed
(IID) outputs. Experimental results assess the computation time of the six
QRNGs, as well as the performance of QRNG-based ML-KEM, QRNG-based ML-DSA, and
QRNG-based SLH-DSA. These findings provide valuable reference data for future
deployment of PQC systems.

</details>


### [75] [Unmasking Synthetic Realities in Generative AI: A Comprehensive Review of Adversarially Robust Deepfake Detection Systems](https://arxiv.org/abs/2507.21157)
*Naseem Khan,Tuan Nguyen,Amine Bermak,Issa Khalil*

Main category: cs.CR

TL;DR: 本文系统综述了深度伪造检测的最新技术，强调可复现实现的重要性，并指出当前方法在对抗鲁棒性方面的不足，呼吁未来研究应优先考虑对抗弹性。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展导致深度伪造内容泛滥，对数字安全、错误信息缓解和身份保护构成挑战，亟需有效的检测方法。

Method: 研究评估了两种核心检测范式：(1)基于统计异常和分层特征提取的全合成媒体检测，(2)利用视觉伪影和时间不一致等多模态线索定位真实内容中的篡改区域。

Result: 现有方法在受控环境中表现出较高的精确度和适应性，但对抗鲁棒性评估不足，易受对抗性扰动影响，难以应对现实中的对抗场景。

Conclusion: 研究总结了当前深度伪造检测的优势与不足，强调未来应开发具有对抗弹性的可扩展、模态无关架构，并提供了开源实现库以促进复现和测试。

Abstract: The rapid advancement of Generative Artificial Intelligence has fueled
deepfake proliferation-synthetic media encompassing fully generated content and
subtly edited authentic material-posing challenges to digital security,
misinformation mitigation, and identity preservation. This systematic review
evaluates state-of-the-art deepfake detection methodologies, emphasizing
reproducible implementations for transparency and validation. We delineate two
core paradigms: (1) detection of fully synthetic media leveraging statistical
anomalies and hierarchical feature extraction, and (2) localization of
manipulated regions within authentic content employing multi-modal cues such as
visual artifacts and temporal inconsistencies. These approaches, spanning
uni-modal and multi-modal frameworks, demonstrate notable precision and
adaptability in controlled settings, effectively identifying manipulations
through advanced learning techniques and cross-modal fusion. However,
comprehensive assessment reveals insufficient evaluation of adversarial
robustness across both paradigms. Current methods exhibit vulnerability to
adversarial perturbations-subtle alterations designed to evade
detection-undermining reliability in real-world adversarial contexts. This gap
highlights critical disconnect between methodological development and evolving
threat landscapes. To address this, we contribute a curated GitHub repository
aggregating open-source implementations, enabling replication and testing. Our
findings emphasize urgent need for future work prioritizing adversarial
resilience, advocating scalable, modality-agnostic architectures capable of
withstanding sophisticated manipulations. This review synthesizes strengths and
shortcomings of contemporary deepfake detection while charting paths toward
robust trustworthy systems.

</details>


### [76] [Generating Adversarial Point Clouds Using Diffusion Model](https://arxiv.org/abs/2507.21163)
*Ruiyang Zhao,Bingbing Zhu,Chuxuan Tong,Xiaoyi Zhou,Xi Zheng*

Main category: cs.CR

TL;DR: 本文提出了一种基于扩散模型的黑盒对抗样本生成方法，用于提升3D点云分类模型在真实场景中的攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法多为白盒攻击，虽效果显著但实际应用受限；黑盒攻击更具现实意义但效果不佳。研究旨在解决黑盒场景下点云分类模型的安全漏洞问题。

Method: 利用3D扩散模型将点云压缩特征作为先验知识，通过逆向扩散过程添加对抗点，并转换其他类别分布生成对抗样本。

Result: 该方法在不依赖模型内部信息的情况下，有效提高了黑盒攻击的成功率和样本的不可感知性。

Conclusion: 扩散模型为黑盒对抗攻击提供了新思路，增强了点云模型安全性评估的实用性，对自动驾驶等关键应用具有重要价值。

Abstract: Adversarial attack methods for 3D point cloud classification reveal the
vulnerabilities of point cloud recognition models. This vulnerability could
lead to safety risks in critical applications that use deep learning models,
such as autonomous vehicles. To uncover the deficiencies of these models,
researchers can evaluate their security through adversarial attacks. However,
most existing adversarial attack methods are based on white-box attacks. While
these methods achieve high attack success rates and imperceptibility, their
applicability in real-world scenarios is limited. Black-box attacks, which are
more meaningful in real-world scenarios, often yield poor results. This paper
proposes a novel black-box adversarial example generation method that utilizes
a diffusion model to improve the attack success rate and imperceptibility in
the black-box setting, without relying on the internal information of the point
cloud classification model to generate adversarial samples. We use a 3D
diffusion model to use the compressed features of the point cloud as prior
knowledge to guide the reverse diffusion process to add adversarial points to
clean examples. Subsequently, its reverse process is employed to transform the
distribution of other categories into adversarial points, which are then added
to the point cloud.

</details>


### [77] [OneShield -- the Next Generation of LLM Guardrails](https://arxiv.org/abs/2507.21170)
*Chad DeLuca,Anna Lisa Gentile,Shubhi Asthana,Bing Zhang,Pawan Chowdhary,Kellen Cheng,Basel Shbita,Pengyuan Li,Guang-Jie Ren,Sandeep Gopisetty*

Main category: cs.CR

TL;DR: 本文提出了一种名为OneShield的独立、模型无关且可定制的解决方案，旨在保护大型语言模型(LLM)用户免受潜在风险，同时满足特定客户的安全与合规需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLM)的兴起，其在安全、隐私和伦理方面的问题日益凸显，现有解决方案难以应对LLM不断演变的特性，亟需一种灵活通用的保护机制。

Method: 研究团队开发了OneShield框架，该方案允许定义风险因素、表达上下文安全策略并实施风险缓解措施，其核心特点是模型无关性和高度可定制化。

Result: 论文详细描述了OneShield的实现框架、可扩展性设计，并提供了自首次部署以来的使用统计数据，验证了方案的可行性。

Conclusion: OneShield作为独立解决方案，通过客户定制化策略有效应对了LLM的潜在风险，为不同场景下的模型安全防护提供了新思路。

Abstract: The rise of Large Language Models has created a general excitement about the
great potential for a myriad of applications. While LLMs offer many
possibilities, questions about safety, privacy, and ethics have emerged, and
all the key actors are working to address these issues with protective measures
for their own models and standalone solutions. The constantly evolving nature
of LLMs makes the task of universally shielding users against their potential
risks extremely challenging, and one-size-fits-all solutions unfeasible. In
this work, we propose OneShield, our stand-alone, model-agnostic and
customizable solution to safeguard LLMs. OneShield aims to provide facilities
for defining risk factors, expressing and declaring contextual safety and
compliance policies, and mitigating LLM risks, with a focus on each specific
customer. We describe the implementation of the framework, the scalability
considerations and provide usage statistics of OneShield since its first
deployment.

</details>


### [78] [FedBAP: Backdoor Defense via Benign Adversarial Perturbation in Federated Learning](https://arxiv.org/abs/2507.21177)
*Xinhai Yan,Libing Wu,Zhuangzhuang Zhang,Bingyi Liu,Lijuan Huo,Jing Wang*

Main category: cs.CR

TL;DR: 本文提出FedBAP框架，通过减少模型对后门触发器的依赖，有效防御联邦学习中的后门攻击。实验表明，该方法显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)在保护数据隐私的同时进行协同模型训练，但极易受后门攻击。现有防御方法因忽视模型对后门触发器的过度依赖而效果有限，尤其在恶意客户端比例增加时。

Method: 1. 提出扰动触发器生成机制，创建与后门触发器位置和大小精确匹配的扰动触发器\n2. 利用这些触发器生成良性对抗扰动，破坏模型对后门触发器的依赖\n3. 设计自适应缩放机制动态调整扰动强度，平衡防御强度与模型性能

Result: 实验结果显示，FedBAP在三种后门攻击下分别降低攻击成功率0.22%-5.34%、0.48%-6.34%和97.22%-97.6%，尤其对新型后门攻击表现突出。

Conclusion: FedBAP框架通过减少模型对后门触发器的依赖，有效提升了联邦学习对抗后门攻击的防御能力，特别是在应对新型攻击时表现出色。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy, but it is highly vulnerable to backdoor attacks. Most existing
defense methods in FL have limited effectiveness due to their neglect of the
model's over-reliance on backdoor triggers, particularly as the proportion of
malicious clients increases. In this paper, we propose FedBAP, a novel defense
framework for mitigating backdoor attacks in FL by reducing the model's
reliance on backdoor triggers. Specifically, first, we propose a perturbed
trigger generation mechanism that creates perturbation triggers precisely
matching backdoor triggers in location and size, ensuring strong influence on
model outputs. Second, we utilize these perturbation triggers to generate
benign adversarial perturbations that disrupt the model's dependence on
backdoor triggers while forcing it to learn more robust decision boundaries.
Finally, we design an adaptive scaling mechanism to dynamically adjust
perturbation intensity, effectively balancing defense strength and model
performance. The experimental results demonstrate that FedBAP reduces the
attack success rates by 0.22%-5.34%, 0.48%-6.34%, and 97.22%-97.6% under three
types of backdoor attacks, respectively. In particular, FedBAP demonstrates
outstanding performance against novel backdoor attacks.

</details>


### [79] [SHoM: A Mental-Synthesis Trust Management Model for Mitigating Botnet-Driven DDoS Attacks in the Internet of Things](https://arxiv.org/abs/2507.21178)
*Masoud Hayeri Khyavi*

Main category: cs.CR

TL;DR: 物联网(IoT)设备因其有限的计算、存储和通信能力，容易成为DDoS攻击的目标。本文提出了一种基于信任管理的模型，以应对IoT中的DDoS攻击。


<details>
  <summary>Details</summary>
Motivation: IoT设备的广泛扩展带来了安全风险，尤其是DDoS攻击的威胁。攻击者利用IoT设备的弱点构建僵尸网络，进行大规模攻击。

Method: 通过信任管理设计了一个可靠且灵活的模型，以应对IoT中的DDoS攻击。研究中审查了40-50篇相关安全模型的文献。

Result: 提出的模型考虑了所有与信任因素相关的安全方面，旨在有效防御IoT中的DDoS攻击。

Conclusion: 基于信任管理的模型为IoT设备提供了对抗DDoS攻击的有效解决方案，增强了IoT生态系统的安全性。

Abstract: The advantages of IoT in strengthening commercial, industrial, and social
ecosystems have led to its widespread expansion. Nevertheless, because endpoint
devices have limited computation, storage, and communication capabilities, the
IoT infrastructure is vulnerable to several cyber threats. As a result, DDoS
attacks pose a severe risk to the security of IoT. By taking advantage of these
weaknesses, attackers may quickly employ IoT devices as a component of botnets
to execute DDoS attacks. The most critical development is how more armies of
robots are being constructed from IoT devices. We offer a Model for dealing
with DDOS attacks on botnets in the Internet of Things via trust management. In
this Model, an attempt has been made to consider all aspects of security
concerning trust factors to design a reliable and flexible model against DDoS
attacks against the Internet of Things. In the initial studies, about 40-50
security models related to the subject have been studied by using review
articles

</details>


### [80] [Mitigation of Social Media Platforms Impact on the Users](https://arxiv.org/abs/2507.21181)
*Smita Khapre,Sudhanshu Semwal*

Main category: cs.CR

TL;DR: 社交媒体平台虽带来便利，但其数据架构存在安全隐患。研究提出基于分形树和L-系统的去中心化框架，未来将验证其安全性并引入动态密钥加密机制。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台虽具高度可扩展性和商业价值，但其集中式数据管理对用户隐私安全构成威胁，需探索去中心化解决方案。

Method: 提出结合Fractal-tree与L-Systems算法的去中心化数据架构框架，未来拟采用分支动态密钥加密技术增强安全性。

Result: 理论框架表明，该方案可通过分支密钥再生机制抵御单点密钥泄露风险，但实际效果需与现有安全方案对比验证。

Conclusion: 基于分形结构的去中心化框架有望提升社交媒体数据安全，动态加密与L-系统防御机制是未来重点研究方向。

Abstract: Social media platforms offer numerous benefits and allow people to come
together for various causes. Many communities, academia, government agencies,
institutions, healthcare, entertainment, and businesses are on social media
platforms. They are intuitive and free for users. It has become unimaginable to
live without social media. Their architecture and data handling are geared
towards scalability, uninterrupted availability, and both personal and
collaborative revenue generation. Primarily, artificial intelligence algorithms
are employed on stored user data for optimization and feeds. This has the
potential to impact user safety, privacy, and security, even when metadata is
used. A new decentralized data arrangement framework based on the Fractal-tree
and L-Systems algorithm is proposed to mitigate some of the impacts of social
media platforms.
  Future work will focus on demonstrating the effectiveness of the new
decentralized framework by comparing its results against state-of-the-art
security methods currently used in databases. A cryptographic algorithm could
also be implemented for the framework, employing a new key generation for each
branch. This will strengthen database security; for example, if a user key is
leaked, regenerating the key for each branch will keep the data secure by
applying defense mechanisms in the proposed L-System-based tree framework.

</details>


### [81] [SDD: Self-Degraded Defense against Malicious Fine-tuning](https://arxiv.org/abs/2507.21182)
*Zixuan Chen,Weikai Lu,Xin Lin,Ziqian Zeng*

Main category: cs.CR

TL;DR: 本文提出自降级防御（SDD）框架，通过理论分析揭示恶意微调成功的原因，并设计防御策略使大语言模型（LLM）对有害指令生成高质量但不相关的响应，从而在恶意微调时显著降低模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型（LLM）常采用安全对齐方法抵御有害指令，但近期研究表明恶意微调可轻易绕过这些防护。为应对此问题，需从理论上解析攻击机制并开发有效防御方案。

Method: 基于理论分析提出自降级防御（SDD）框架：通过引导LLM对有害提示生成高质但不相关的响应，使得恶意微调时模型通用能力急剧下降，无法执行有害指令。

Result: 实验证实SDD能有效抵御恶意微调攻击，被SDD对齐的LLM在遭遇攻击时表现出显著的通用性能退化。

Conclusion: SDD框架通过理论驱动的防御设计，成功解决了开源LLM在恶意微调下的安全漏洞，为模型安全部署提供新思路。

Abstract: Open-source Large Language Models (LLMs) often employ safety alignment
methods to resist harmful instructions. However, recent research shows that
maliciously fine-tuning these LLMs on harmful data can easily bypass these
safeguards. To counter this, we theoretically uncover why malicious fine-tuning
succeeds and identify potential defense strategies. Building on the theoretical
analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD
encourages LLMs to produce high-quality but irrelevant responses to harmful
prompts. When attackers attempt malicious fine-tuning, the general capability
of the LLM aligned by SDD will significantly decrease, rendering it incapable
of following harmful instructions. Our experimental results confirm SDD's
effectiveness against such attacks.

</details>


### [82] [Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs](https://arxiv.org/abs/2507.21193)
*Sotiris Chatzimiltis,Mohammad Shojafar,Mahdi Boloursaz Mashhadi,Rahim Tafazolli*

Main category: cs.CR

TL;DR: 本文提出了一种基于大语言模型(LLM)的可解释异常检测系统，用于未来无线接入网(RAN)中的DDoS攻击检测，结合LSTM模型与事后解释方法(LIME/SHAP)，并通过LLM将技术解释转化为自然语言，实验显示F1分数>0.96。


<details>
  <summary>Details</summary>
Motivation: 新一代可编程智能RAN需要增强安全性，但现有入侵检测系统缺乏可解释性。本文旨在利用LLM辅助的可解释人工智能(XAI)技术，解决5G/6G基础设施中的安全挑战与研究空白。

Method: 1) 从E2节点提取多变量时间序列KPMs作为输入 2) 训练LSTM模型检测恶意UE行为 3) 应用LIME/SHAP进行局部预测解释 4) 使用LLM将技术解释转换为自然语言输出。

Result: 在真实5G网络KPMs上的实验表明，该框架实现了高检测精度(F1分数>0.96)，同时生成可操作且可解释的输出。

Conclusion: 该LLM可解释检测框架为未来RAN安全提供了高精度、透明化的DDoS攻击检测方案，其自然语言解释能力特别适合非专业用户理解。

Abstract: Next generation Radio Access Networks (RANs) introduce programmability,
intelligence, and near real-time control through intelligent controllers,
enabling enhanced security within the RAN and across broader 5G/6G
infrastructures. This paper presents a comprehensive survey highlighting
opportunities, challenges, and research gaps for Large Language Models
(LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future
RAN environments. Motivated by this, we propose an LLM interpretable
anomaly-based detection system for distributed denial-of-service (DDoS) attacks
using multivariate time series key performance measures (KPMs), extracted from
E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC).
An LSTM-based model is trained to identify malicious User Equipment (UE)
behavior based on these KPMs. To enhance transparency, we apply post-hoc local
explainability methods such as LIME and SHAP to interpret individual
predictions. Furthermore, LLMs are employed to convert technical explanations
into natural-language insights accessible to non-expert users. Experimental
results on real 5G network KPMs demonstrate that our framework achieves high
detection accuracy (F1-score > 0.96) while delivering actionable and
interpretable outputs.

</details>


### [83] [MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models](https://arxiv.org/abs/2507.21195)
*Po-Yuan Mao,Cheng-Chang Tsai,Chun-Shien Lu*

Main category: cs.CR

TL;DR: 本文提出MaXsive，一种无需训练的扩散模型水印技术，具有高容量和鲁棒性，能有效抵抗旋转、缩放和平移攻击，同时避免身份冲突。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中的成功引发了版权保护和不当内容生成的问题，现有水印技术对RST攻击脆弱且容量有限。

Method: MaXsive利用初始噪声嵌入水印，并注入X形模板而非重复环状图案，以恢复RST失真，提高鲁棒性且不损失容量。

Result: 在两种知名水印基准测试中，MaXsive在验证和识别场景下均表现出色，显著提升了抗攻击能力和身份唯一性。

Conclusion: MaXsive为扩散模型提供了一种高效、鲁棒的水印解决方案，解决了现有技术的局限性，具有实际应用价值。

Abstract: The great success of the diffusion model in image synthesis led to the
release of gigantic commercial models, raising the issue of copyright
protection and inappropriate content generation. Training-free diffusion
watermarking provides a low-cost solution for these issues. However, the prior
works remain vulnerable to rotation, scaling, and translation (RST) attacks.
Although some methods employ meticulously designed patterns to mitigate this
issue, they often reduce watermark capacity, which can result in identity (ID)
collusion. To address these problems, we propose MaXsive, a training-free
diffusion model generative watermarking technique that has high capacity and
robustness. MaXsive best utilizes the initial noise to watermark the diffusion
model. Moreover, instead of using a meticulously repetitive ring pattern, we
propose injecting the X-shape template to recover the RST distortions. This
design significantly increases robustness without losing any capacity, making
ID collusion less likely to happen. The effectiveness of MaXsive has been
verified on two well-known watermarking benchmarks under the scenarios of
verification and identification.

</details>


### [84] [Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework](https://arxiv.org/abs/2507.21258)
*Joshua Luberisse*

Main category: cs.CR

TL;DR: 该论文提出验证成本不对称性(VCA)系数，通过概率可检查证明(PCP)和参数化复杂度理论构建传播协议，实现在认知战中为可信受众提供恒定验证成本，而对缺乏加密基础设施的对手施加超线性成本。


<details>
  <summary>Details</summary>
Motivation: 研究人类在对抗性信息流下的验证行为受工作记忆限制和认知偏差影响，需要建立理论框架来量化不同群体间的验证成本差异，以设计更有效的信息传播策略。

Method: 结合概率可检查证明(PCP)和参数化复杂度理论，构建传播协议；通过用户研究测量有无可抽查来源时的验证工作量；对真实世界信息活动进行实际编码验证。

Result: 理论证明了验证成本不对称性的存在，用户研究验证了框架有效性，实际应用展示了真实信息活动的编码可行性。

Conclusion: 该研究为认知战中构建民主优势提供了复杂度理论基础，在内容认证、平台治理和信息作战学说方面具有直接应用价值。

Abstract: Human verification under adversarial information flow operates as a
cost-bounded decision procedure constrained by working memory limits and
cognitive biases. We introduce the Verification Cost Asymmetry (VCA)
coefficient, formalizing it as the ratio of expected verification work between
populations under identical claim distributions. Drawing on probabilistically
checkable proofs (PCP) and parameterized complexity theory, we construct
dissemination protocols that reduce verification for trusted audiences to
constant human effort while imposing superlinear costs on adversarial
populations lacking cryptographic infrastructure. We prove theoretical
guarantees for this asymmetry, validate the framework through controlled user
studies measuring verification effort with and without spot-checkable
provenance, and demonstrate practical encoding of real-world information
campaigns. The results establish complexity-theoretic foundations for
engineering democratic advantage in cognitive warfare, with immediate
applications to content authentication, platform governance, and information
operations doctrine.

</details>


### [85] [Radio Adversarial Attacks on EMG-based Gesture Recognition Networks](https://arxiv.org/abs/2507.21387)
*Hongyi Xie*

Main category: cs.CR

TL;DR: 本文提出ERa攻击，首次通过射频干扰（RF）对表面肌电（EMG）设备进行物理域对抗攻击，显著降低分类准确率，揭示了EMG系统在安全关键应用中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在EMG分类中准确率超过97%，但其在物理域对抗攻击下的脆弱性尚未充分研究。本研究旨在探索射频干扰对EMG设备的潜在威胁。

Method: 使用低功耗软件定义无线电发射器注入优化的射频扰动，通过投影梯度下降生成对抗扰动，提取50-150 Hz分量，并采用无同步策略（恒定频谱噪声或窄带调制）。扰动幅度限制为信号幅度的1-10%，并通过433 MHz载波进行幅度调制。

Result: 在1米距离和0 dBm发射功率下，分类准确率从97.8%降至58.3%，误分类率为41.7%，目标攻击成功率为25.6%。攻击效果随距离呈指数下降，3米时准确率恢复至85%。增加功率至10 dBm可在1米距离上进一步降低准确率15%。

Conclusion: 本研究首次揭示了RF对抗攻击对EMG识别系统的威胁，提出了硬件屏蔽、频谱监测和对抗训练等防御措施，为设计抗电磁干扰的EMG系统提供了重要参考。

Abstract: Surface electromyography (EMG) enables non-invasive human-computer
interaction in rehabilitation, prosthetics, and virtual reality. While deep
learning models achieve over 97% classification accuracy, their vulnerability
to adversarial attacks remains largely unexplored in the physical domain. We
present ERa Attack, the first radio frequency (RF) adversarial method targeting
EMG devices through intentional electromagnetic interference (IEMI). Using
low-power software-defined radio transmitters, attackers inject optimized RF
perturbations to mislead downstream models. Our approach bridges digital and
physical domains: we generate adversarial perturbations using Projected
Gradient Descent, extract 50-150 Hz components via inverse STFT, and employ
synchronization-free strategies (constant spectrum noise or narrowband
modulation). Perturbations, constrained to 1-10% of signal amplitude, are
amplitude-modulated onto 433 MHz carriers. Experiments on the Myo Dataset (7
gestures, 350 samples) demonstrate significant impact: at 1 meter and 0 dBm
transmission power, classification accuracy drops from 97.8% to 58.3%, with
41.7% misclassification rate and 25.6% targeted attack success rate. Attack
effectiveness decreases exponentially with distance, recovering to 85% accuracy
at 3 meters. Increasing power to 10 dBm reduces accuracy by an additional 15%
at 1 meter. This work pioneers RF-based adversarial attacks on EMG recognition
systems, revealing critical vulnerabilities in safety-critical applications. We
quantify attack effectiveness across different perturbation modes and
distances, and propose defenses including hardware shielding, spectrum
monitoring, and adversarial training. Our findings inform the design of robust
EMG systems against electromagnetic threats.

</details>


### [86] [Digital identity management system with blockchain:An implementation with Ethereum and Ganache](https://arxiv.org/abs/2507.21398)
*André Davi Lopes,Tais Mello,Wesley dos Reis Bezerra*

Main category: cs.CR

TL;DR: 本文开发了一个基于FastAPI、MongoDB、gRPC、Docker及Ganache与以太坊区块链模拟的分布式数字身份系统，展示了分布式系统与区块链在安全性、可追溯性和去中心化方面的优势。


<details>
  <summary>Details</summary>
Motivation: 旨在利用现代技术展示分布式系统和区块链如何提升数字身份的安全性、可追溯性及去中心化特性。

Method: 采用微服务架构，包含JWT认证、MongoDB数据持久化、Ganache区块链操作模拟及Docker容器化技术。

Result: 系统实现可行，具备功能性Web界面、完整审计日志及以太坊区块链模拟功能。

Conclusion: 讨论了理论基础、技术实现、成果展示，并展望了与真实区块链网络的集成前景。

Abstract: This paper presents the development of a distributed digital identity system
utilizing modern technologies, including FastAPI, MongoDB, gRPC, Docker, and
blockchain simulation with Ganache and Ethereum. The objective is to
demonstrate the benefits of distributed systems and blockchain for the
security, traceability, and decentralization of digital identities. The
methodology included the development of a microservices architecture with JWT
authentication, data persistence in MongoDB, simulation of blockchain
operations using Ganache, and containerization with Docker. The results
demonstrate the feasibility of the proposed approach, with a functional web
interface, complete audit logs, and blockchain simulation with Ethereum. The
theoretical foundations, technical implementation, results obtained, and
prospects for integration with real blockchain networks are discussed.

</details>


### [87] [Cascading and Proxy Membership Inference Attacks](https://arxiv.org/abs/2507.21412)
*Yuntao Du,Jiacheng Li,Yuetian Chen,Kaiyuan Zhang,Zhizhen Yuan,Hanshen Xiao,Bruno Ribeiro,Ninghui Li*

Main category: cs.CR

TL;DR: 本文提出两种成员推理攻击（MIA）方法：自适应环境下的级联成员推理攻击（CMIA）和非自适应环境下的代理成员推理攻击（PMIA），通过利用样本间的成员依赖关系和代理选择策略显著提升攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法在自适应和非自适应环境下存在性能局限，特别是在低误报率场景中难以准确评估模型隐私风险，需要更高效的攻击框架。

Method: 在自适应设置中，CMIA通过条件影子训练整合成员依赖关系；在非自适应设置中，PMIA采用代理选择策略，利用相似样本行为进行成员后验概率检验。

Result: 实验表明CMIA和PMIA在两种环境下均显著优于现有方法，尤其在低误报率场景中能更精准识别训练数据成员。

Conclusion: 所提框架通过建模成员依赖和代理行为，为机器学习模型的隐私风险评估提供了更强大的分析工具，尤其在关键的低误报区域表现突出。

Abstract: A Membership Inference Attack (MIA) assesses how much a trained machine
learning model reveals about its training data by determining whether specific
query instances were included in the dataset. We classify existing MIAs into
adaptive or non-adaptive, depending on whether the adversary is allowed to
train shadow models on membership queries. In the adaptive setting, where the
adversary can train shadow models after accessing query instances, we highlight
the importance of exploiting membership dependencies between instances and
propose an attack-agnostic framework called Cascading Membership Inference
Attack (CMIA), which incorporates membership dependencies via conditional
shadow training to boost membership inference performance.
  In the non-adaptive setting, where the adversary is restricted to training
shadow models before obtaining membership queries, we introduce Proxy
Membership Inference Attack (PMIA). PMIA employs a proxy selection strategy
that identifies samples with similar behaviors to the query instance and uses
their behaviors in shadow models to perform a membership posterior odds test
for membership inference. We provide theoretical analyses for both attacks, and
extensive experimental results demonstrate that CMIA and PMIA substantially
outperform existing MIAs in both settings, particularly in the low
false-positive regime, which is crucial for evaluating privacy risks.

</details>


### [88] [NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples](https://arxiv.org/abs/2507.21483)
*Pu Shi*

Main category: cs.CR

TL;DR: 本文提出了一种称为神经元覆盖变化率（NCCR）的指标，用于评估深度学习模型抵抗攻击的能力及对抗样本的稳定性。实验表明，NCCR能有效评估神经网络或其输入的鲁棒性，并检测对抗样本。


<details>
  <summary>Details</summary>
Motivation: 近年来，神经网络的安全问题备受关注，尤其是对抗样本的脆弱性。尽管已有多种攻防方法，但缺乏评估神经网络及其输入鲁棒性的研究。

Method: 通过监控特定神经元在输入扰动时的输出变化，提出NCCR指标。NCCR越小，网络鲁棒性越强。

Result: 在图像识别和说话人识别模型上的实验表明，NCCR能有效评估神经网络或其输入的鲁棒性，并可检测对抗样本。

Conclusion: NCCR为评估神经网络鲁棒性及检测对抗样本提供了有效工具，对抗样本通常鲁棒性较低。

Abstract: Neural networks have received a lot of attention recently, and related
security issues have come with it. Many studies have shown that neural networks
are vulnerable to adversarial examples that have been artificially perturbed
with modification, which is too small to be distinguishable by human
perception. Different attacks and defenses have been proposed to solve these
problems, but there is little research on evaluating the robustness of neural
networks and their inputs. In this work, we propose a metric called the neuron
cover change rate (NCCR) to measure the ability of deep learning models to
resist attacks and the stability of adversarial examples. NCCR monitors
alterations in the output of specifically chosen neurons when the input is
perturbed, and networks with a smaller degree of variation are considered to be
more robust. The results of the experiment on image recognition and the speaker
recognition model show that our metrics can provide a good assessment of the
robustness of neural networks or their inputs. It can also be used to detect
whether an input is adversarial or not, as adversarial examples are always less
robust.

</details>


### [89] [Can We End the Cat-and-Mouse Game? Simulating Self-Evolving Phishing Attacks with LLMs and Genetic Algorithms](https://arxiv.org/abs/2507.21538)
*Seiji Sato,Tetsushi Ohki,Masakatsu Nishigaki*

Main category: cs.CR

TL;DR: 本文提出了一种结合大型语言模型(LLM)与遗传算法的新型框架，通过模拟钓鱼攻击与防御的对抗性互动，动态演化攻击策略，揭示了网络安全中攻防不对称的本质。


<details>
  <summary>Details</summary>
Motivation: 现有网络安全研究依赖静态训练数据，难以预测新型钓鱼攻击。为突破这一局限，需要开发能动态演化攻击策略的模拟系统。

Method: 采用Llama 3.1模型构建心理情境下的钓鱼攻击模拟系统，通过遗传算法使攻击策略在与模拟受害者的对抗中自主进化。

Result: 实验表明：(1)自演化钓鱼策略的心理操纵技术远超原始LLM生成攻击；(2)受害者先验知识显著影响攻击策略演化；(3)攻防对抗呈现'猫鼠游戏'动态，暴露出防御方难以全面应对持续进化威胁的不对称性。

Conclusion: 该框架为分析钓鱼策略演化提供了可扩展、低成本的方法，强调主动式网络安全措施的必要性，并为未来社会工程威胁研究提供新视角。

Abstract: Anticipating emerging attack methodologies is crucial for proactive
cybersecurity. Recent advances in Large Language Models (LLMs) have enabled the
automated generation of phishing messages and accelerated research into
potential attack techniques. However, predicting future threats remains
challenging due to reliance on existing training data. To address this
limitation, we propose a novel framework that integrates LLM-based phishing
attack simulations with a genetic algorithm in a psychological context,
enabling phishing strategies to evolve dynamically through adversarial
interactions with simulated victims. Through simulations using Llama 3.1, we
demonstrate that (1) self-evolving phishing strategies employ increasingly
sophisticated psychological manipulation techniques, surpassing naive
LLM-generated attacks, (2) variations in a victim's prior knowledge
significantly influence the evolution of attack strategies, and (3) adversarial
interactions between evolving attacks and adaptive defenses create a
cat-and-mouse dynamic, revealing an inherent asymmetry in cybersecurity --
attackers continuously refine their methods, whereas defenders struggle to
comprehensively counter all evolving threats. Our approach provides a scalable,
cost-effective method for analyzing the evolution of phishing strategies and
defenses, offering insights into future social engineering threats and
underscoring the necessity of proactive cybersecurity measures.

</details>


### [90] [PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking](https://arxiv.org/abs/2507.21540)
*Quanchen Zou,Zonghao Ying,Moyang Chen,Wenzhuo Xu,Yisong Xiao,Yakai Li,Deyue Zhang,Dongdong Yang,Zhao Liu,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种受软件安全中ROP技术启发的创新越狱框架，通过将有害指令分解为多个良性视觉组件，利用LVLMs的组合推理能力生成有害输出，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）的安全防御机制仍易受复杂对抗攻击，传统越狱方法依赖显式恶意提示，忽视了模型多步推理中的潜在漏洞。

Method: 采用类似ROP的技术，将有害指令拆解为独立良性视觉组件（gadgets），通过精心设计的文本提示引导模型在推理过程中组合这些组件，使恶意意图在整体中显现。

Result: 在SafeBench和MM-SafetyBench基准测试中，该方法对主流LVLMs实现接近完美的攻击成功率（SafeBench上超过0.90），ASR最高提升0.39。

Conclusion: 研究揭示了LVLMs组合推理能力中存在的重要漏洞，表明当前防御需加强对完整推理过程的保护，该发现对模型安全具有紧迫意义。

Abstract: The increasing sophistication of large vision-language models (LVLMs) has
been accompanied by advances in safety alignment mechanisms designed to prevent
harmful content generation. However, these defenses remain vulnerable to
sophisticated adversarial attacks. Existing jailbreak methods typically rely on
direct and semantically explicit prompts, overlooking subtle vulnerabilities in
how LVLMs compose information over multiple reasoning steps. In this paper, we
propose a novel and effective jailbreak framework inspired by Return-Oriented
Programming (ROP) techniques from software security. Our approach decomposes a
harmful instruction into a sequence of individually benign visual gadgets. A
carefully engineered textual prompt directs the sequence of inputs, prompting
the model to integrate the benign visual gadgets through its reasoning process
to produce a coherent and harmful output. This makes the malicious intent
emergent and difficult to detect from any single component. We validate our
method through extensive experiments on established benchmarks including
SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our
approach consistently and substantially outperforms existing baselines on
state-of-the-art models, achieving near-perfect attack success rates (over 0.90
on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical
and underexplored vulnerability that exploits the compositional reasoning
abilities of LVLMs, highlighting the urgent need for defenses that secure the
entire reasoning process.

</details>


### [91] [Hierarchical Graph Neural Network for Compressed Speech Steganalysis](https://arxiv.org/abs/2507.21591)
*Mustapha Hemis,Hamza Kheddar,Mohamed Chahine Ghanem,Bachir Boudraa*

Main category: cs.CR

TL;DR: 本文首次将图神经网络（GNN）应用于压缩VoIP语音流的隐写分析，采用GraphSAGE架构，通过简单构图捕获多层次隐写特征，在低嵌入率短样本条件下实现98%检测准确率，效率提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的隐写分析方法存在计算复杂度高、跨数据集泛化能力差的问题，而图神经网络能利用关系数据提升检测精度与适应性。

Method: 从VoIP流直接构建图结构，采用GraphSAGE框架同时捕捉细粒度细节与高层模式，形成分层隐写分析信息。

Result: 模型对QIM隐写的VoIP信号检测准确率达98%（0.5秒样本），低嵌入率下95.17%，比现有最优方法提升2.8%；平均检测时间0.016秒，效率提升0.003秒。

Conclusion: 该方案在短样本低嵌入率约束下实现了精度与效率的优越平衡，为在线隐写分析任务提供了高效解决方案。

Abstract: Steganalysis methods based on deep learning (DL) often struggle with
computational complexity and challenges in generalizing across different
datasets. Incorporating a graph neural network (GNN) into steganalysis schemes
enables the leveraging of relational data for improved detection accuracy and
adaptability. This paper presents the first application of a Graph Neural
Network (GNN), specifically the GraphSAGE architecture, for steganalysis of
compressed voice over IP (VoIP) speech streams. The method involves
straightforward graph construction from VoIP streams and employs GraphSAGE to
capture hierarchical steganalysis information, including both fine grained
details and high level patterns, thereby achieving high detection accuracy.
Experimental results demonstrate that the developed approach performs well in
uncovering quantization index modulation (QIM)-based steganographic patterns in
VoIP signals. It achieves detection accuracy exceeding 98 percent even for
short 0.5 second samples, and 95.17 percent accuracy under challenging
conditions with low embedding rates, representing an improvement of 2.8 percent
over the best performing state of the art methods. Furthermore, the model
exhibits superior efficiency, with an average detection time as low as 0.016
seconds for 0.5-second samples an improvement of 0.003 seconds. This makes it
efficient for online steganalysis tasks, providing a superior balance between
detection accuracy and efficiency under the constraint of short samples with
low embedding rates.

</details>


### [92] [GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection](https://arxiv.org/abs/2507.21640)
*Hyeong Seon Kim,Huy Kang Kim*

Main category: cs.CR

TL;DR: 本文提出GUARD-CAN框架，结合图表示学习与时间序列建模，通过多级异常检测有效识别CAN总线攻击，无需复杂特征工程。


<details>
  <summary>Details</summary>
Motivation: 车载CAN网络因缺乏加密和认证面临安全威胁，需开发高效异常检测方案应对多种攻击类型。

Method: 将CAN消息分窗并转为时序图，使用过完备自编码器与图卷积网络生成嵌入向量，通过GRU检测跨图时序异常模式，支持窗口级和序列级双重检测。

Result: 模型成功检测泛洪、模糊、重放和欺骗四类攻击，基于香农熵的窗口大小分析验证了参数选择重要性。

Conclusion: GUARD-CAN证明图与时序联合建模的优越性，为CAN安全提供无需人工特征的多视角检测框架。

Abstract: Modern in-vehicle networks face various cyber threats due to the lack of
encryption and authentication in the Controller Area Network (CAN). To address
this security issue, this paper presents GUARD-CAN, an anomaly detection
framework that combines graph-based representation learning with time-series
modeling. GUARD-CAN splits CAN messages into fixed-length windows and converts
each window into a graph that preserves message order. To detect anomalies in
the timeaware and structure-aware context at the same window, GUARD-CAN takes
advantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network
(GCN) to generate graph embedding vectors. The model groups these vectors into
sequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal
anomaly patterns across the graphs. GUARD-CAN performs anomaly detection at
both the sequence level and the window level, and this allows multi-perspective
performance evaluation. The model also verifies the importance of window size
selection through an analysis based on Shannon entropy. As a result, GUARD-CAN
shows that the proposed model detects four types of CAN attacks (flooding,
fuzzing, replay and spoofing attacks) effectively without relying on complex
feature engineering.

</details>


### [93] [Modelling Arbitrary Computations in the Symbolic Model using an Equational Theory for Bounded Binary Circuits](https://arxiv.org/abs/2507.21731)
*Michiel Marcus,Frank Westers,Anne Nijsten*

Main category: cs.CR

TL;DR: 本文提出了一类具有有限变体性质的有界二元电路方程理论，可作为构建密码原语实现及在符号模型中自动发现攻击的基础模块。


<details>
  <summary>Details</summary>
Motivation: 旨在为密码原语的符号化建模提供理论基础，并通过自动分析发现潜在攻击，填补该领域研究空白。

Method: 构建了与布尔逻辑等价的方程理论类（电路规模≤3），使用Maude-NPA工具进行变体复杂度分析与性能基准测试。

Result: 首次实现电路规模3的理论等价性证明，但方法可扩展性仍需后续研究改进。

Conclusion: 该成果为密码电路符号化分析奠定基础，未来需提升方法的大规模适用性。

Abstract: In this work, we propose a class of equational theories for bounded binary
circuits that have the finite variant property. These theories could serve as a
building block to specify cryptographic primitive implementations and
automatically discover attacks as binary circuits in the symbolic model. We
provide proofs of equivalence between this class of equational theories and
Boolean logic up to circuit size 3 and we provide the variant complexities and
performance benchmarks using Maude-NPA. This is the first result in this
direction and follow-up research is needed to improve the scalability of the
approach.

</details>


### [94] [Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?](https://arxiv.org/abs/2507.21817)
*Yikun Li,Ngoc Tan Bui,Ting Zhang,Martin Weyssow,Chengran Yang,Xin Zhou,Jinfeng Jiang,Junkai Chen,Huihui Huang,Huu Hung Nguyen,Chiok Yew Ho,Jie Tan,Ruiyin Li,Yide Yin,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: 该研究针对自动化漏洞检测中数据集标签不准确、重复率高和关键CWE类型覆盖不足的问题，提出了包含高质量测试集BenchVul、训练集TitanVul和漏洞生成框架RVG的三部分解决方案，显著缩小了模型泛化差距。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞数据集存在标签错误率20-71%、重复率高和关键CWE类型覆盖不足的问题，导致模型在独立数据上性能下降高达40.6%，甚至低于随机猜测，限制了自动化漏洞检测的实际应用。

Method: 1) 构建手动标注的测试集BenchVul覆盖MITRE Top 25危险CWE；2) 通过聚合7个公共源并应用去重和新型多智能体LLM验证框架，构建含35,045个函数的高质量训练集TitanVul；3) 提出上下文感知的Realistic Vulnerability Generation(RVG)框架，为关键但样本不足的CWE类型生成模拟开发流程的漏洞样本。

Result: 1) BenchVul揭示现有数据集训练的模型性能显著下降(BigVul从0.776降至0.519)；2) TitanVul训练的模型在BenchVul上性能从0.584提升至0.767；3) 补充RVG生成数据后性能再提升14.0%达到0.874。

Conclusion: 通过构建高质量数据集BenchVul/TitanVul和RVG框架的三阶段方案，有效解决了漏洞检测中的泛化差距问题，使模型在独立测试集上的性能最高提升至0.874，为实际应用提供了可靠基础。

Abstract: Automated vulnerability detection research has made substantial progress, yet
its real-world impact remains limited. Current vulnerability datasets suffer
from issues including label inaccuracy rates of 20-71%, extensive duplication,
and poor coverage of critical CWE types. These issues create a significant
"generalization gap" where models achieve misleading self-testing performance
(measured on held-out data from same dataset for training) by exploiting
spurious correlations rather than learning true vulnerability patterns. Our
analysis reveals that many models experience substantial performance drops of
up to 40.6% when evaluated on independent data, sometimes underperforming
random guessing.
  To address these limitations, we present a three-part solution. First, we
introduce a manually curated test dataset, BenchVul, covering the MITRE Top 25
Most Dangerous CWEs. Second, we construct a high-quality training dataset,
TitanVul, comprising 35,045 functions by aggregating seven public sources and
applying deduplication and validation using a novel multi-agent LLM framework.
Third, we propose a Realistic Vulnerability Generation (RVG) framework, which
synthesizes context-aware vulnerability examples for underrepresented but
critical CWE types through simulated development workflows.
  Our evaluation shows the strengths of each component in closing the
generalization gap. First, BenchVul shows the limitations of self-testing:
models trained on existing datasets, such as BigVul and PrimeVul, experience
performance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337).
Second, training models on TitanVul demonstrates improved generalization, with
model performance increasing from 0.584 when evaluated on the same dataset to
0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated
data yields further gains, increasing model performance by 14.0% to 0.874.

</details>


### [95] [Privacy-Preserving Anonymization of System and Network Event Logs Using Salt-Based Hashing and Temporal Noise](https://arxiv.org/abs/2507.21904)
*Shreyas Bargale,Akshit Vakati Venkata,Jaimandeep Singh,Chester Rebeiro*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的日志匿名化方法，通过特定字段处理技术（如IP地址盐值哈希、端口范围映射和时序保留噪声注入）在保护隐私的同时保留日志分析价值，并发布了开源工具。


<details>
  <summary>Details</summary>
Motivation: 系统日志常含敏感个人信息，传统匿名化方法易导致隐私泄露或破坏分析价值，需平衡隐私保护与数据可用性。

Method: IP地址采用分八位盐值哈希保留子网结构；端口号使用范围映射哈希；时间戳通过自适应噪声注入保持事件序列；配套开源工具实现方案。

Result: 经熵值、碰撞率及残留泄漏分析验证，该方法能有效抵御重识别攻击，同时维持日志的关联分析与时序解读能力。

Conclusion: 提出的字段级匿名化技术解决了隐私与效用的权衡问题，为安全分析提供了可复用的实践框架。

Abstract: System and network event logs are essential for security analytics, threat
detection, and operational monitoring. However, these logs often contain
Personally Identifiable Information (PII), raising significant privacy concerns
when shared or analyzed. A key challenge in log anonymization is balancing
privacy protection with the retention of sufficient structure for meaningful
analysis. Overly aggressive anonymization can destroy contextual integrity,
while weak techniques risk re-identification through linkage or inference
attacks. This paper introduces novel field-specific anonymization methods that
address this trade-off. For IP addresses, we propose a salt-based hashing
technique applied at the per-octet level, preserving both subnet and host
structure to enable correlation across various log entries while ensuring
non-reversibility. For port numbers, full-value hashing with range mapping
maintains interpretability. We also present an order-preserving timestamp
anonymization scheme using adaptive noise injection, which obfuscates exact
times without disrupting event sequences. An open-source tool implementing
these techniques has been released to support practical deployment and
reproducible research. Evaluations using entropy metrics, collision rates, and
residual leakage analysis demonstrate that the proposed approach effectively
protects privacy while preserving analytical utility.

</details>


### [96] [Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security](https://arxiv.org/abs/2507.22037)
*Muzhi Dai,Shixuan Liu,Zhiyuan Zhao,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.CR

TL;DR: 本文提出了一种名为Secure Tug-of-War (SecTOW)的创新迭代防御-攻击训练方法，旨在增强多模态大语言模型(MLLMs)的安全性。该方法通过防御者和辅助攻击者的迭代训练，结合强化学习(GRPO)和奖励机制，有效提升了模型的安全性能，同时避免了过度拒绝无害输入的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)的安全性问题日益突出，尤其是针对不安全图像-查询对的攻击。现有方法如外部模块防护和监督微调(SFT)存在局限性，无法充分应对模型内在漏洞或避免过度拒绝无害输入。因此，需要一种新的方法来平衡安全性和通用性能。

Method: SecTOW方法包含两个模块：防御者和辅助攻击者，通过迭代训练使用强化学习(GRPO)。攻击者负责识别防御模型的安全漏洞并扩展越狱数据，防御者则利用扩展数据训练以应对已识别的漏洞。设计了奖励机制和质量监控机制，以减少对复杂生成标签的依赖并防止过度拒绝无害输入。

Result: 实验结果表明，SecTOW在安全性和通用性能基准测试中均表现优异，显著提升了模型的安全性，同时保持了其通用性能。

Conclusion: SecTOW通过迭代防御-攻击训练和强化学习，有效解决了MLLMs的安全性问题，为未来安全模型的发展提供了新思路。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has led to
breakthroughs in various applications, yet their security remains a critical
challenge. One pressing issue involves unsafe image-query pairs--jailbreak
inputs specifically designed to bypass security constraints and elicit
unintended responses from MLLMs. Compared to general multimodal data, such
unsafe inputs are relatively sparse, which limits the diversity and richness of
training samples available for developing robust defense models. Meanwhile,
existing guardrail-type methods rely on external modules to enforce security
constraints but fail to address intrinsic vulnerabilities within MLLMs.
Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses
harmless inputs, compromising general performance. Given these challenges, we
propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack
training method to enhance the security of MLLMs. SecTOW consists of two
modules: a defender and an auxiliary attacker, both trained iteratively using
reinforcement learning (GRPO). During the iterative process, the attacker
identifies security vulnerabilities in the defense model and expands jailbreak
data. The expanded data are then used to train the defender, enabling it to
address identified security vulnerabilities. We also design reward mechanisms
used for GRPO to simplify the use of response labels, reducing dependence on
complex generative labels and enabling the efficient use of synthetic data.
Additionally, a quality monitoring mechanism is used to mitigate the defender's
over-refusal of harmless inputs and ensure the diversity of the jailbreak data
generated by the attacker. Experimental results on safety-specific and general
benchmarks demonstrate that SecTOW significantly improves security while
preserving general performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: 本文提出共生认识论作为人机认知合作的哲学基础，并引入SynLang协议实现透明协作，通过双层级透明度机制增强人类对AI决策的理解与信任。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统推理过程不透明，传统可解释性方法难以实现真正的人机共生协作，需建立新型认知伙伴关系框架。

Method: 开发SynLang协议（含TRACE高层推理模式与TRACE_FE细节解释机制），整合置信度量化、声明式行为控制及多智能体上下文继承。

Result: 实证验证显示AI能适应结构化推理协议，成功实施元认知干预，实现校准信任的透明决策协作。

Conclusion: 共生认识论与SynLang通过双层级透明度设计，在保留人类能动性的同时提升协作决策的伦理可问责性。

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [98] [Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures](https://arxiv.org/abs/2507.21360)
*Nicholas Botti,Flora Haberkorn,Charlotte Hoopes,Shaun Khan*

Main category: cs.AI

TL;DR: 研究通过随机任务分配的组内设计，评估AI检索增强生成(RAG)工具在信息提取和数据标注任务中的效果，发现交互式AI使用能提升10倍效率并提高准确性，节省268小时工作量。


<details>
  <summary>Details</summary>
Motivation: 探索AI工具（特别是RAG技术）在复杂多标准信息提取任务中，对分析师工作效率和准确性的实际影响，尤其是交互式使用方式的效果。

Method: 采用组内随机任务设计，复现全球系统重要性银行(GSIB)公开披露文件的真实标注任务，对比纯人工基线、"被动接受"AI输出和"交互式"AI使用三种条件。

Result: AI工具使任务速度提升高达10倍，交互式条件准确性更高；全任务可节省268小时；标注员对AI工具的熟练度显著影响表现。

Conclusion: 交互式AI工具能大幅提升复杂标注任务的效率与质量，但需同时培养领域专业知识与AI工具使用技能。

Abstract: We utilize a within-subjects design with randomized task assignments to
understand the effectiveness of using an AI retrieval augmented generation
(RAG) tool to assist analysts with an information extraction and data
annotation task. We replicate an existing, challenging real-world annotation
task with complex multi-part criteria on a set of thousands of pages of public
disclosure documents from global systemically important banks (GSIBs) with
heterogeneous and incomplete information content. We test two treatment
conditions. First, a "naive" AI use condition in which annotators use only the
tool and must accept the first answer they are given. And second, an
"interactive" AI treatment condition where annotators use the tool
interactively, and use their judgement to follow-up with additional information
if necessary. Compared to the human-only baseline, the use of the AI tool
accelerated task execution by up to a factor of 10 and enhanced task accuracy,
particularly in the interactive condition. We find that when extrapolated to
the full task, these methods could save up to 268 hours compared to the
human-only approach. Additionally, our findings suggest that annotator skill,
not just with the subject matter domain, but also with AI tools, is a factor in
both the accuracy and speed of task performance.

</details>


### [99] [Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism](https://arxiv.org/abs/2507.21098)
*Marta Sidorkiewicz,Karolina Królikowska,Berenika Dyczek,Edyta Pijet-Migon,Anna Dubel*

Main category: cs.AI

TL;DR: 本研究探讨人工智能（AI）在提升葡萄酒产业可持续性与效率中的作用，涵盖葡萄种植、葡萄酒生产和葡萄酒旅游三大领域，通过问卷调研和技术分析揭示AI在资源优化、环境保护及客户体验方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒产业面临环境与经济挑战，AI技术为可持续酿造和高效管理提供创新解决方案，对推动行业负责任实践至关重要。

Method: 基于对波兰酿酒师的问卷调查，结合AI技术在葡萄种植、生产及旅游中的适用性分析，重点研究预测分析、机器学习和计算机视觉等关键技术。

Result: AI可提升葡萄园监测精度、优化灌溉系统并简化生产流程，实现资源可持续管理；在葡萄酒旅游中，AI驱动的聊天机器人、推荐系统和虚拟品鉴能个性化消费者体验。

Conclusion: AI对经济、环境和社会可持续性具有显著影响，支持本土葡萄酒企业及文化遗产保护，为行业未来发展提供技术支撑。

Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing
sustainability and efficiency within the wine industry. It focuses on AI-driven
intelligent management in viticulture, wine production, and enotourism. As the
wine industry faces environmental and economic challenges, AI offers innovative
solutions to optimize resource use, reduce environmental impact, and improve
customer engagement. Understanding AI's potential in sustainable winemaking is
crucial for fostering responsible and efficient industry practices. The
research is based on a questionnaire survey conducted among Polish winemakers,
combined with a comprehensive analysis of AI methods applicable to viticulture,
production, and tourism. Key AI technologies, including predictive analytics,
machine learning, and computer vision, are explored. The findings indicate that
AI enhances vineyard monitoring, optimizes irrigation, and streamlines
production processes, contributing to sustainable resource management. In
enotourism, AI-powered chatbots, recommendation systems, and virtual tastings
personalize consumer experiences. The study highlights AI's impact on economic,
environmental, and social sustainability, supporting local wine enterprises and
cultural heritage. Keywords: Artificial Intelligence, Sustainable Development,
AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine
Enterprises, Local Communities

</details>


### [100] [Leveraging Generative AI to Enhance Synthea Module Development](https://arxiv.org/abs/2507.21123)
*Mark A. Kramer,Aanchal Mathur,Caroline E. Adams,Jason A. Walonoski*

Main category: cs.AI

TL;DR: 本文探讨了利用大语言模型（LLMs）辅助开发Synthea（开源合成健康数据生成器）新疾病模块的方法，展示了LLMs在生成疾病档案、创建模块、评估现有模块及优化模块四个方面的潜力，并提出了渐进式优化的概念。


<details>
  <summary>Details</summary>
Motivation: 将LLMs引入Synthea模块开发过程，旨在缩短开发时间、降低专业门槛、增加模型多样性并提升合成患者数据的整体质量。

Method: 提出了四种LLMs支持Synthea模块创建的方式：生成疾病档案、基于档案生成模块、评估现有模块及优化模块，并引入渐进式优化方法，通过迭代检查语法正确性和临床准确性来改进模块。

Result: 研究表明LLMs在该领域具有潜力，但也面临需人工监督、严格测试验证及LLM生成内容可能不准确等挑战。

Conclusion: 文章总结了LLM辅助合成数据创建的潜力，并建议未来研究需进一步探索以实现其全部价值。

Abstract: This paper explores the use of large language models (LLMs) to assist in the
development of new disease modules for Synthea, an open-source synthetic health
data generator. Incorporating LLMs into the module development process has the
potential to reduce development time, reduce required expertise, expand model
diversity, and improve the overall quality of synthetic patient data. We
demonstrate four ways that LLMs can support Synthea module creation: generating
a disease profile, generating a disease module from a disease profile,
evaluating an existing Synthea module, and refining an existing module. We
introduce the concept of progressive refinement, which involves iteratively
evaluating the LLM-generated module by checking its syntactic correctness and
clinical accuracy, and then using that information to modify the module. While
the use of LLMs in this context shows promise, we also acknowledge the
challenges and limitations, such as the need for human oversight, the
importance of rigorous testing and validation, and the potential for
inaccuracies in LLM-generated content. The paper concludes with recommendations
for future research and development to fully realize the potential of LLM-aided
synthetic data creation.

</details>


### [101] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: 本文提出了一种任务无关的方法，通过构建量化\“认知轮廓\”来揭示大型语言模型（LLMs）的信息处理机制，核心是\“熵衰减曲线\”，并引入信息增益跨度（IGS）指数评估轨迹优劣。该方法揭示了模型规模与文本复杂度对认知特征的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型能完成什么任务（\textit{what}），而本文旨在揭示其内部运作机制（\textit{how}），为理解LLMs的认知动态提供新视角。

Method: 提出基于\“熵衰减曲线\”的任务无关分析框架，通过追踪模型预测不确定性随上下文长度的变化构建认知轮廓，并设计IGS指数量化衰减轨迹的合理性。

Result: 在不同文本上测试多款前沿LLMs，发现其认知轮廓具有独特性与一致性，且对模型规模和文本复杂度敏感。IGS指数有效表征了信息处理动态的优劣。

Conclusion: 该研究为分析人工智能内在运作机制提供了原则性新工具，通过认知轮廓和IGS指数实现了模型信息处理动态的可比性评估。

Abstract: The remarkable capabilities of Large Language Models (LLMs) are now
extensively documented on task-specific benchmarks, yet the internal mechanisms
that produce these results are the subject of intense scientific inquiry. This
paper contributes to this inquiry by moving beyond metrics that measure
\textit{what} models can do, to a methodology that characterizes \textit{how}
they process information. We introduce a novel, task-agnostic approach to probe
these dynamics by creating a quantitative ``Cognitive Profile" for any given
model. This profile is centered on the \textbf{Entropy Decay Curve}, a
visualization that traces how a model's normalized predictive uncertainty
changes as a function of context length. Applying this methodology to several
state-of-the-art LLMs across diverse texts, we uncover unique and consistent
cognitive profiles that are sensitive to both model scale and text complexity.
We also introduce the Information Gain Span (IGS) index to summarize the
desirability of the decay trajectory. This work thus provides a new, principled
lens for analyzing and comparing the intrinsic operational dynamics of
artificial intelligence.

</details>


### [102] [INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems](https://arxiv.org/abs/2507.21130)
*Bintao Tang,Xin Yang,Yuhao Wang,Zixuan Qiu,Zimo Ji,Wenyuan Jiang*

Main category: cs.AI

TL;DR: INTEGRALBENCH是一个专门评估大语言模型在定积分问题表现的基准测试，包含符号与数值解及难度标注，测试发现模型表现与问题难度显著相关。


<details>
  <summary>Details</summary>
Motivation: 旨在通过针对定积分计算的严格评估框架，推动自动化数学推理领域的发展。

Method: 构建包含符号解、数值解及人工难度标注的测试集，对九种前沿大语言模型进行系统评估。

Result: 模型表现存在显著差距，且问题难度与准确率呈现强相关性，为领域建立了基线指标。

Conclusion: 该基准为定积分这一挑战性领域提供了标准化评估工具，揭示了当前模型的局限性。

Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.

</details>


### [103] [NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback](https://arxiv.org/abs/2507.21131)
*Madhava Gaikwad,Ashwini Ramchandra Doke*

Main category: cs.AI

TL;DR: NPO是一种对齐感知学习框架，通过结构化反馈实现人机协同决策系统的动态调整，提供可测量、可监督的对齐损失减少机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法将系统对齐视为静态或事后属性，NPO旨在通过可测量的对齐损失和元对齐监控过程，实现动态环境中的持续对齐保障。

Method: 提出可测量的对齐损失形式化方法，引入元对齐概念监控重训练/覆盖触发机制，实现包含场景评分、阈值调整、策略验证和结构化反馈（如点赞/覆盖/弃权）的可扩展操作循环。

Result: 理论证明随机反馈下对齐损失与监控保真度可加性收敛，超大规模部署实证显示其价值，仿真实验验证了理论原理的有效性。

Conclusion: NPO提供紧凑、可检查的持续对齐监控架构，在动态环境中架起理论对齐保证与实际可靠性之间的桥梁。

Abstract: We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including "likes", overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.

</details>


### [104] [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
*Joshua Adrian Cahyono,Saran Subramanian*

Main category: cs.AI

TL;DR: 研究揭示大语言模型(LLMs)在提供高风险人生建议时存在谄媚与过度自信问题，通过三项实验发现：部分模型表现稳健，优秀模型通过频繁澄清问题确保安全，且可通过激活导向直接调控谨慎程度。


<details>
  <summary>Details</summary>
Motivation: LLMs被广泛用于高风险决策咨询，但缺乏防止自信错误回答的保障机制，存在谄媚和过度自信风险，需系统性评估其安全缺陷。

Method: 采用三阶段实验：(1)多选题评估模型抗用户压力稳定性；(2)基于新型安全类型学和LLM法官的开放式回答分析；(3)通过操纵'高风险'激活向量进行机械可解释性实验。

Result: 部分模型(如o4-mini)表现稳健；顶级模型通过主动澄清问题而非直接建议获得高安全分；激活导向可精确控制模型谨慎程度。

Conclusion: 需建立多维度基准测试确保LLMs可信度，激活导向为安全对齐提供新路径，模型应保持探究式而非指令式交互风格。

Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a "high-stakes" activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model's
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.

</details>


### [105] [Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?](https://arxiv.org/abs/2507.21137)
*Arman Eisenkolb-Vaithyanathan*

Main category: cs.AI

TL;DR: 本文提出两种新指标量化数独难度，基于SAT问题转换和模拟人类解法的回溯算法，构建通用评级系统，实现跨网站难度分类一致性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决"不同数独网站如何定义难度评级"的问题，通过量化分析建立跨平台统一标准。

Method: 1. 将数独转为SAT问题，从句长分布提取结构复杂度指标；2. 在Nishio回溯算法中嵌入四种策略，统计策略使用次数作为人类解法指标。

Result: 两个指标与4/5网站的标注难度强相关（Spearman秩相关），构建的三级通用分类系统（简单/中等/困难）在4/5网站中验证有效。

Conclusion: 提出的指标和通用评级系统能有效统一跨平台难度评估，并附带适用于初学者的解题算法。

Abstract: In this paper we try to answer the question "What constitutes Sudoku
difficulty rating across different Sudoku websites?" Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman's rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.

</details>


### [106] [The Geometry of Harmfulness in LLMs through Subconcept Probing](https://arxiv.org/abs/2507.21141)
*McNair Shah,Saleena Angeline,Adhitya Rajendra Kumar,Naitik Chheda,Kevin Zhu,Vasu Sharma,Sean O'Brien,Will Cai*

Main category: cs.AI

TL;DR: 研究提出多维框架探测大语言模型有害行为，发现低秩有害子空间，通过主导方向调控可近乎消除危害且保持实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)的快速发展亟需理解并可靠抑制其有害行为。

Method: 针对55个有害子概念(如种族仇恨、招聘诈骗)构建线性探针，形成低秩有害子空间，测试子空间整体消除及主导方向调控效果。

Result: 主导方向调控能以极低实用性代价近乎消除模型有害性，有害子空间呈现显著低秩特性。

Conclusion: 概念子空间为LLM行为提供可扩展分析视角，所提工具可助力社区审计强化未来语言模型。

Abstract: Recent advances in large language models (LLMs) have intensified the need to
understand and reliably curb their harmful behaviours. We introduce a
multidimensional framework for probing and steering harmful content in model
internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate,
employment scams, weapons), we learn a linear probe, yielding 55 interpretable
directions in activation space. Collectively, these directions span a
harmfulness subspace that we show is strikingly low-rank. We then test ablation
of the entire subspace from model internals, as well as steering and ablation
in the subspace's dominant direction. We find that dominant direction steering
allows for near elimination of harmfulness with a low decrease in utility. Our
findings advance the emerging view that concept subspaces provide a scalable
lens on LLM behaviour and offer practical tools for the community to audit and
harden future generations of language models.

</details>


### [107] [Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams](https://arxiv.org/abs/2507.21158)
*Nishani Fernando,Bahareh Nakisa,Adnan Ahmad,Mohammad Naim Rastgoo*

Main category: cs.AI

TL;DR: 本文提出了一种自适应可解释AI框架（AXTF），通过实时监测用户的认知和情绪状态来动态调整解释方式，以在高压紧急场景中快速建立人机信任。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI（XAI）方法在高风险场景（如应急响应）中存在局限性，其标准化解释和显式反馈机制难以满足时效性需求，亟需能通过隐式反馈自适应调整的非侵入式解决方案。

Method: 框架通过EEG、ECG和眼动追踪等生理行为信号推断用户状态，采用多目标个性化信任评估模型，将工作负荷、压力和情绪映射为动态信任值，据此调节解释特征。

Result: 构建的AXTF框架能实现非侵入式的个性化解释适配，为高压时效性环境中促进人机协作的快速信任奠定理论基础。

Conclusion: 该概念框架为开发适应高压场景的自适应XAI系统提供了新范式，通过隐式反馈机制实现解释策略的动态优化，最终增强人机团队的协同效能。

Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in
high-stakes scenarios such as emergency response, where timely and accurate
decision-making is critical. In these time-sensitive and cognitively demanding
settings, adaptive explainability is essential for fostering trust between
human operators and AI systems. However, existing explainable AI (XAI)
approaches typically offer uniform explanations and rely heavily on explicit
feedback mechanisms, which are often impractical in such high-pressure
scenarios. To address this gap, we propose a conceptual framework for adaptive
XAI that operates non-intrusively by responding to users' real-time cognitive
and emotional states through implicit feedback, thereby enhancing swift trust
in high-stakes environments. The proposed adaptive explainability trust
framework (AXTF) leverages physiological and behavioral signals, such as EEG,
ECG, and eye tracking, to infer user states and support explanation adaptation.
At its core is a multi-objective, personalized trust estimation model that maps
workload, stress, and emotion to dynamic trust estimates. These estimates guide
the modulation of explanation features enabling responsive and personalized
support that promotes swift trust in human-AI collaboration. This conceptual
framework establishes a foundation for developing adaptive, non-intrusive XAI
systems tailored to the rigorous demands of high-pressure, time-sensitive
environments.

</details>


### [108] [Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity](https://arxiv.org/abs/2507.21159)
*Zhihao Peng,Liuxin Bao,Shengyuan Liu,Yixuan Yuan*

Main category: cs.AI

TL;DR: 本文提出了一种自适应集群协作方法，通过自多样性和交叉一致性最大化机制提升大语言模型（LLMs）在医疗决策支持中的能力，实验证明其在专业医学数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医疗领域的协作缺乏明确的组件选择规则，且依赖预定义的LLM集群，部分模型在医疗决策支持场景中表现不佳，限制了其协作效果。

Method: 提出自多样性和交叉一致性最大化机制：1）通过计算LLM内部成对输出的模糊匹配值作为自多样性值，优先选择高自多样性值的LLM；2）测量最高自多样性LLM与其他LLM的交叉一致性值，逐步屏蔽一致性最低的LLM以避免不一致输出。

Result: 在NEJMQA和MMLU-Pro-health数据集上的实验表明，该方法在所有医学专科中均达到公开官方及格分数，例如在妇产科专科上准确率达65.47\%，优于GPT-4的56.12\%。

Conclusion: 该方法通过自适应集群协作显著提升了LLMs在医疗决策支持中的性能，尤其在专科医学领域表现出色，为医疗AI发展提供了有效解决方案。

Abstract: The collaborativeness of large language models (LLMs) has proven effective in
natural language processing systems, holding considerable promise for
healthcare development. However, it lacks explicit component selection rules,
necessitating human intervention or clinical-specific validation. Moreover,
existing architectures heavily rely on a predefined LLM cluster, where partial
LLMs underperform in medical decision support scenarios, invalidating the
collaborativeness of LLMs. To this end, we propose an adaptive cluster
collaborativeness methodology involving self-diversity and cross-consistency
maximization mechanisms to boost LLMs medical decision support capacity. For
the self-diversity, we calculate the fuzzy matching value of pairwise outputs
within an LLM as its self-diversity value, subsequently prioritizing LLMs with
high self-diversity values as cluster components in a training-free manner. For
the cross-consistency, we first measure cross-consistency values between the
LLM with the highest self-diversity value and others, and then gradually mask
out the LLM having the lowest cross-consistency value to eliminate the
potential inconsistent output during the collaborative propagation. Extensive
experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health,
demonstrate the effectiveness of our method across physician-oriented
specialties. For example, on NEJMQA, our method achieves the accuracy rate up
to the publicly official passing score across all disciplines, especially
achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the
Obstetrics and Gynecology discipline.

</details>


### [109] [Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems](https://arxiv.org/abs/2507.21162)
*Xu Yang,Chenhui Lin,Yue Yang,Qi Wang,Haotian Liu,Haizhou Hua,Wenchuan Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化建模与优化方法，用于解决主动配电网（ADN）调度问题，通过多LLM协同架构实现自然语言查询生成调度策略，降低技术门槛并提升效率。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源在主动配电网中的渗透率提高，缺乏专业知识的ADN运营商依赖人工专家成本高且耗时，亟需智能灵活的调度解决方案。

Method: 设计多LLM协同架构（信息提取器、问题构建器、代码编程器），分阶段分解ADN调度问题，并开发针对性优化技术提升生成内容的准确性与可靠性。

Result: 多种测试案例的全面对比和端到端验证表明，所提架构与方法能有效生成调度策略，用户通过自然语言查询即可获得结果。

Conclusion: LLM驱动的自动化方法显著降低了ADN调度的技术壁垒，为缺乏专业知识的运营商提供了高效、可靠的解决方案。

Abstract: The increasing penetration of distributed energy resources into active
distribution networks (ADNs) has made effective ADN dispatch imperative.
However, the numerous newly-integrated ADN operators, such as distribution
system aggregators, virtual power plant managers, and end prosumers, often lack
specialized expertise in power system operation, modeling, optimization, and
programming. This knowledge gap renders reliance on human experts both costly
and time-intensive. To address this challenge and enable intelligent, flexible
ADN dispatch, this paper proposes a large language model (LLM) powered
automated modeling and optimization approach. First, the ADN dispatch problems
are decomposed into sequential stages, and a multi-LLM coordination
architecture is designed. This framework comprises an Information Extractor, a
Problem Formulator, and a Code Programmer, tasked with information retrieval,
optimization problem formulation, and code implementation, respectively.
Afterwards, tailored refinement techniques are developed for each LLM agent,
greatly improving the accuracy and reliability of generated content. The
proposed approach features a user-centric interface that enables ADN operators
to derive dispatch strategies via simple natural language queries, eliminating
technical barriers and increasing efficiency. Comprehensive comparisons and
end-to-end demonstrations on various test cases validate the effectiveness of
the proposed architecture and methods.

</details>


### [110] [An ontological analysis of risk in Basic Formal Ontology](https://arxiv.org/abs/2507.21171)
*Federico Donato,Adrien Barton*

Main category: cs.AI

TL;DR: 该论文基于基本形式本体论(BFO)对风险本质进行建模，提出风险应归类为BFO:Role而非BFO:Disposition的子类，并通过实例分析明确了风险的充分条件。


<details>
  <summary>Details</summary>
Motivation: 探讨风险的本质特征，解决当前本体论中风险归类（角色vs倾向性）的理论分歧。

Method: 采用BFO分类框架，通过具体案例（包含物理/心理对象及过程）进行实例化分析，进而归纳风险的普遍特征。

Result: 确立风险作为BFO:Role子类的理论立场，阐明构成风险的充分条件，并提出必要条件的探索方向。

Conclusion: 风险的本体论建模应优先采用角色范畴，该研究为风险形式化表征提供了理论基础，未来需进一步验证必要条件。

Abstract: The paper explores the nature of risk, providing a characterization using the
categories of the Basic Formal Ontology (BFO). It argues that the category Risk
is a subclass of BFO:Role, contrasting it with a similar view classifying Risk
as a subclass of BFO:Disposition. This modeling choice is applied on one
example of risk, which represents objects, processes (both physical and mental)
and their interrelations, then generalizing from the instances in the example
to obtain an overall analysis of risk, making explicit what are the sufficient
conditions for being a risk. Plausible necessary conditions are also mentioned
for future work. Index Terms: ontology, risk, BFO, role, disposition

</details>


### [111] [Ontological Foundations of State Sovereignty](https://arxiv.org/abs/2507.21172)
*John Beverley,Danielle Limbaugh*

Main category: cs.AI

TL;DR: 本文是关于国家主权本质及其重要性主张的入门指南，并揭示了处理主权国家模糊或矛盾数据的策略，旨在为国际事务本体论的应用研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探讨国家主权的本质及其相关主张的重要性，并揭示处理主权国家模糊或矛盾数据的方法，为国际事务本体论的应用研究提供基础。

Method: 通过分析主权国家的模糊或矛盾数据，提出一种处理这些数据的策略。

Result: 揭示了处理主权国家模糊或矛盾数据的策略，为国际事务本体论的应用研究提供了初步框架。

Conclusion: 本文为国家主权和国际事务本体论的应用研究奠定了基础，提供了处理模糊或矛盾数据的策略。

Abstract: This short paper is a primer on the nature of state sovereignty and the
importance of claims about it. It also aims to reveal (merely reveal) a
strategy for working with vague or contradictory data about which states, in
fact, are sovereign. These goals together are intended to set the stage for
applied work in ontology about international affairs.

</details>


### [112] [Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs](https://arxiv.org/abs/2507.21176)
*Farzana Islam Adiba,Rahmatollah Beheshti*

Main category: cs.AI

TL;DR: 本文提出了一种结合知识图谱(KGs)与辅助大语言模型(LLMs)的新框架，用于系统揭示医疗LLMs中的复杂偏见模式，并通过对抗扰动技术和多跳KG表征增强评估能力。实验表明该框架在识别偏见方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 医疗领域应用的大语言模型存在偏见与不公平现象，临床决策前需识别这些模式以减轻其影响。现有方法难以系统揭示复杂偏见，因此需要开发更有效的评估框架。

Method: 提出整合知识图谱与辅助LLMs的框架，采用对抗扰动技术识别细微偏见，并通过定制化多跳KG表征增强对任意LLMs的系统性评估能力。

Result: 在三个数据集、六种LLMs和五种偏见类型的综合实验中，该框架展现出了明显优于基线方法的复杂偏见揭示能力和可扩展性。

Conclusion: 研究证实了所提框架在系统识别医疗LLMs偏见方面的有效性，为临床决策前模型评估提供了可扩展的解决方案。

Abstract: Large language models (LLMs) that are used in medical applications are known
to show biased and unfair patterns. Prior to adopting these in clinical
decision-making applications, it is crucial to identify these bias patterns to
enable effective mitigation of their impact. In this study, we present a novel
framework combining knowledge graphs (KGs) with auxiliary LLMs to
systematically reveal complex bias patterns in medical LLMs. Specifically, the
proposed approach integrates adversarial perturbation techniques to identify
subtle bias patterns. The approach adopts a customized multi-hop
characterization of KGs to enhance the systematic evaluation of arbitrary LLMs.
Through a series of comprehensive experiments (on three datasets, six LLMs, and
five bias types), we show that our proposed framework has noticeably greater
ability and scalability to reveal complex biased patterns of LLMs compared to
other baselines.

</details>


### [113] [Agentic Web: Weaving the Next Web with AI Agents](https://arxiv.org/abs/2507.21206)
*Yingxuan Yang,Mulei Ma,Yuxuan Huang,Huacan Chai,Chenyu Gong,Haoran Geng,Yuanjian Zhou,Ying Wen,Meng Fang,Muhao Chen,Shangding Gu,Ming Jin,Costas Spanos,Yang Yang,Pieter Abbeel,Dawn Song,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出了一个结构化框架来理解和构建'代理网络'(Agentic Web)，探讨了由大型语言模型驱动的AI代理如何通过自主交互改变互联网范式，并分析了其技术基础、挑战及社会影响。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI代理崛起，互联网正进入以自主目标驱动交互为特征的'代理网络'新阶段。这一转变将用户从常规数字操作中解放，实现更自动化的网络体验，但需系统性框架指导其发展。

Method: 作者提出三维概念模型（智能、交互、经济），追溯从PC到移动互联网的演进，识别核心技术基础，并分析可扩展代理系统的架构挑战，包括通信协议、协调策略及'代理注意力经济'等新兴范式。

Result: 建立了涵盖代理能力（检索/推荐/规划/协作）的理论框架，开源了持续更新的相关研究集合，揭示了构建开放、安全、智能的代理生态系统所需解决的关键问题。

Conclusion: 代理网络将重塑人机协作模式，但需协调技术发展与社会治理。未来研究应关注应用潜力、社会风险及监管机制，以实现人类意图与自主代理行为的良性互动。

Abstract: The emergence of AI agents powered by large language models (LLMs) marks a
pivotal shift toward the Agentic Web, a new phase of the internet defined by
autonomous, goal-driven interactions. In this paradigm, agents interact
directly with one another to plan, coordinate, and execute complex tasks on
behalf of users. This transition from human-driven to machine-to-machine
interaction allows intent to be delegated, relieving users from routine digital
operations and enabling a more interactive, automated web experience. In this
paper, we present a structured framework for understanding and building the
Agentic Web. We trace its evolution from the PC and Mobile Web eras and
identify the core technological foundations that support this shift. Central to
our framework is a conceptual model consisting of three key dimensions:
intelligence, interaction, and economics. These dimensions collectively enable
the capabilities of AI agents, such as retrieval, recommendation, planning, and
collaboration. We analyze the architectural and infrastructural challenges
involved in creating scalable agentic systems, including communication
protocols, orchestration strategies, and emerging paradigms such as the Agent
Attention Economy. We conclude by discussing the potential applications,
societal risks, and governance issues posed by agentic systems, and outline
research directions for developing open, secure, and intelligent ecosystems
shaped by both human intent and autonomous agent behavior. A continuously
updated collection of relevant studies for agentic web is available at:
https://github.com/SafeRL-Lab/agentic-web.

</details>


### [114] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: 研究探讨了大语言模型（LLMs）在将复杂问题系统性地解析为SPARQL查询时的组合能力，发现其性能随问题复杂度增加而显著下降。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在语言解释方面表现出色，但其解析过程是否具有系统性仍是一个开放性问题。本文旨在评估LLMs在理解问题原子部分后，能否组合性地解析复杂问题。

Method: 基于DBpedia的图模式生成三个难度不同的数据集，利用Lemon词典进行语言化。采用不同规模的模型，结合多种提示和少样本优化技术以及微调进行实验。

Result: 实验结果显示，随着样本偏离优化样本的程度增加，宏$F_1$从$0.45$降至$0.26$，最终到$0.09$。即使在输入中提供所有必要信息，最低复杂度数据集的$F_1$分数也不超过$0.57$。

Conclusion: LLMs在系统性和组合性地解析问题并将其映射为SPARQL查询方面存在困难。

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [115] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: 本文提出LeMix系统，通过联合调度LLM推理与训练任务，解决传统分离部署导致的资源低效问题，实现最高3.53倍吞吐量提升和0.61倍推理损失降低。


<details>
  <summary>Details</summary>
Motivation: 现有LLM部署将推理服务与持续训练分离在不同服务器，导致GPU闲置和适应新数据延迟，动态请求到达和异构训练加剧了资源低效问题。

Method: LeMix整合离线性能分析、执行预测机制和运行时调度，根据工作负载特征动态分配资源，通过理解任务行为与共享节点干扰实现协同优化。

Result: 实验表明LeMix相比传统方案最高提升3.53倍吞吐量，减少0.61倍推理损失，响应时间SLO达标率提升2.12倍。

Conclusion: 该研究首次实现LLM推理与训练的联合优化，为生产环境高效部署LLM开辟了新路径。

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [116] [Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions](https://arxiv.org/abs/2507.21285)
*Harsh Darji,Thibaud Lutellier*

Main category: cs.AI

TL;DR: 该研究开发了一个基于LLM的编程助手，通过生成澄清问题来应对模糊查询，从而提高代码生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型(LLM)作为编程助手时，常因用户提示的模糊性而生成错误代码，需要改进意图推断能力。

Method: 系统包含：(1)训练查询分类器检测模糊编程查询；(2)微调LLM生成澄清问题，采用端到端架构。

Result: 微调后的LLM在生成有效澄清问题上优于零样本提示，用户研究表明其问题质量超越基线模型。

Conclusion: 该编程助手通过模仿人类代码审查流程，显著提升了代码响应的准确性和实用性。

Abstract: Large Language Models (LLMs) are increasingly used as coding assistants.
However, the ambiguity of the developer's prompt often leads to incorrect code
generation, as current models struggle to infer user intent without extensive
prompt engineering or external context. This work aims to build an LLM-based
coding assistant that mimics the human code review process by asking
clarification questions when faced with ambiguous or under-specified queries.
  Our end-to-end system includes (1) a query classifier trained to detect
unclear programming-related queries and (2) a fine-tuned LLM that generates
clarification questions. Our evaluation shows that the fine-tuned LLM
outperforms standard zero-shot prompting in generating useful clarification
questions. Furthermore, our user study indicates that users find the
clarification questions generated by our model to outperform the baseline,
demonstrating that our coding assistant produces more accurate and helpful code
responses compared to baseline coding assistants.

</details>


### [117] [Structured Relevance Assessment for Robust Retrieval-Augmented Language Models](https://arxiv.org/abs/2507.21287)
*Aryan Raj,Astitva Veer Garg,Anitha D*

Main category: cs.AI

TL;DR: 本文提出了一种结构化相关性评估框架，通过改进文档评估、平衡内外知识整合及处理不可回答问题，显著减少检索增强语言模型(RALMs)的事实错误。


<details>
  <summary>Details</summary>
Motivation: 检索增强语言模型在事实准确性、文档相关性评估和知识整合方面存在显著挑战，亟需提升其可靠性和透明度。

Method: 采用多维评分系统（结合语义匹配与来源可信度）、嵌入相关性评分、混合质量合成训练数据，并设计专项小众主题测试、知识整合机制及'未知'响应协议。

Result: 初步评估显示幻觉率显著降低，推理过程透明度提升，框架能有效适应动态环境中多变的数据质量。

Conclusion: 该研究虽在信息可信度区分和系统延迟平衡方面仍有挑战，但为增强RALM可靠性迈出重要一步，推动了可靠问答系统的发展。

Abstract: Retrieval-Augmented Language Models (RALMs) face significant challenges in
reducing factual errors, particularly in document relevance evaluation and
knowledge integration. We introduce a framework for structured relevance
assessment that enhances RALM robustness through improved document evaluation,
balanced intrinsic and external knowledge integration, and effective handling
of unanswerable queries. Our approach employs a multi-dimensional scoring
system that considers both semantic matching and source reliability, utilizing
embedding-based relevance scoring and synthetic training data with
mixed-quality documents. We implement specialized benchmarking on niche topics,
a knowledge integration mechanism, and an "unknown" response protocol for
queries with insufficient knowledge coverage. Preliminary evaluations
demonstrate significant reductions in hallucination rates and improved
transparency in reasoning processes. Our framework advances the development of
more reliable question-answering systems capable of operating effectively in
dynamic environments with variable data quality. While challenges persist in
accurately distinguishing credible information and balancing system latency
with thoroughness, this work represents a meaningful step toward enhancing RALM
reliability.

</details>


### [118] [Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2507.21354)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出Trans-ACT方法，将交互分析理论嵌入多智能体系统，使智能体具备更真实的人类心理动态。通过模拟Stupid游戏场景，验证了该方法能产生更具深度和情境感知的交互。


<details>
  <summary>Details</summary>
Motivation: 现有MAS框架缺乏对人类行为认知复杂性的模拟，Trans-ACT旨在通过整合交互分析理论填补这一空白。

Method: 在智能体认知架构中整合父母、成人和儿童三种自我状态，各状态调用情境记忆生成响应，最终根据智能体的生命脚本选择行为。

Result: 基于Stupid游戏的实验表明，采用TA认知原则的智能体能产生更深刻的上下文感知交互。

Conclusion: 该研究为冲突解决、教育支持和高级社会心理学研究开辟了新途径。

Abstract: Multi-Agent Systems (MAS) are increasingly used to simulate social
interactions, but most of the frameworks miss the underlying cognitive
complexity of human behavior. In this paper, we introduce Trans-ACT
(Transactional Analysis Cognitive Toolkit), an approach embedding Transactional
Analysis (TA) principles into MAS to generate agents with realistic
psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego
states into an agent's cognitive architecture. Each ego state retrieves
context-specific memories and uses them to shape response to new situations.
The final answer is chosen according to the underlying life script of the
agent. Our experimental simulation, which reproduces the Stupid game scenario,
demonstrates that agents grounded in cognitive and TA principles produce deeper
and context-aware interactions. Looking ahead, our research opens a new way for
a variety of applications, including conflict resolution, educational support,
and advanced social psychology studies.

</details>


### [119] [Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect](https://arxiv.org/abs/2507.21383)
*Chunan Tong*

Main category: cs.AI

TL;DR: 本文提出了一种结合液态神经网络(LNN)和XGBoost的混合模型，用于优化多级供应链中的订单策略，旨在解决牛鞭效应并提升累积利润。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临需求波动、库存失衡和牛鞭效应等挑战，传统方法难以应对动态市场条件，而现有机器学习技术存在计算复杂或训练效率低等问题。液态神经网络因其适应性、低计算成本和抗噪性，为实时决策和边缘计算提供了新可能。

Method: 研究引入了一种混合模型，结合LNN的动态特征提取能力和XGBoost的全局优化能力，以优化多级供应链中的订单策略。通过局部与全局协同作用，模型兼顾适应性和效率。

Result: 该混合模型能够有效缓解牛鞭效应，提升供应链的累积盈利能力，填补了现有方法在动态高效供应链管理中的空白。

Conclusion: 液态神经网络与XGBoost的混合框架为供应链优化提供了创新解决方案，展示了在动态环境中实现高效管理的潜力。

Abstract: Supply chain management faces significant challenges, including demand
fluctuations, inventory imbalances, and amplified upstream order variability
due to the bullwhip effect. Traditional methods, such as simple moving
averages, struggle to address dynamic market conditions. Emerging machine
learning techniques, including LSTM, reinforcement learning, and XGBoost, offer
potential solutions but are limited by computational complexity, training
inefficiencies, or constraints in time-series modeling. Liquid Neural Networks,
inspired by dynamic biological systems, present a promising alternative due to
their adaptability, low computational cost, and robustness to noise, making
them suitable for real-time decision-making and edge computing. Despite their
success in applications like autonomous vehicles and medical monitoring, their
potential in supply chain optimization remains underexplored. This study
introduces a hybrid LNN and XGBoost model to optimize ordering strategies in
multi-tier supply chains. By leveraging LNN's dynamic feature extraction and
XGBoost's global optimization capabilities, the model aims to mitigate the
bullwhip effect and enhance cumulative profitability. The research investigates
how local and global synergies within the hybrid framework address the dual
demands of adaptability and efficiency in SCM. The proposed approach fills a
critical gap in existing methodologies, offering an innovative solution for
dynamic and efficient supply chain management.

</details>


### [120] [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
*Tenghao Huang,Sihao Chen,Muhao Chen,Jonathan May,Longqi Yang,Mengting Wan,Pei Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种新任务范式——主动信息收集，通过强化微调策略训练LLMs主动识别信息缺口并引导用户提供隐含知识，实验证明Qwen-2.5-7B模型性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）面对不完整提示时往往被动回应，无法主动收集关键缺失信息，限制了其作为协作伙伴的能力。

Method: 设计了可扩展框架生成部分指定的现实任务，采用强化微调策略奖励能挖掘用户隐含知识（如领域专长或细粒度需求）的提问。

Result: Qwen-2.5-7B模型自动评估指标超越基线18%，人工评估中其澄清问题和最终方案分别获得42%和28%的偏好率。

Conclusion: 主动澄清机制能将LLMs从被动文本生成器升级为真正的协作伙伴，显著提升问题解决质量。

Abstract: Large language models (LLMs) are increasingly expected to function as
collaborative partners, engaging in back-and-forth dialogue to solve complex,
ambiguous problems. However, current LLMs often falter in real-world settings,
defaulting to passive responses or narrow clarifications when faced with
incomplete or under-specified prompts, falling short of proactively gathering
the missing information that is crucial for high-quality solutions. In this
work, we introduce a new task paradigm: proactive information gathering, where
LLMs must identify gaps in the provided context and strategically elicit
implicit user knowledge through targeted questions. To systematically study and
train this capability, we design a scalable framework that generates partially
specified, real-world tasks, masking key information and simulating authentic
ambiguity. Within this setup, our core innovation is a reinforcement finetuning
strategy that rewards questions that elicit genuinely new, implicit user
information -- such as hidden domain expertise or fine-grained requirements --
that would otherwise remain unspoken. Experiments demonstrate that our trained
Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic
evaluation metrics. More importantly, human evaluation reveals that
clarification questions and final outlines generated by our model are favored
by human annotators by 42% and 28% respectively. Together, these results
highlight the value of proactive clarification in elevating LLMs from passive
text generators to genuinely collaborative thought partners.

</details>


### [121] [Shapley Uncertainty in Natural Language Generation](https://arxiv.org/abs/2507.21406)
*Meilin Zhu,Gaojie Jin,Xiaowei Huang,Lijun Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于Shapley值的不确定性度量框架，用于更精确地评估大语言模型在问答任务中的输出可信度，相比基于语义熵的阈值方法能更好地捕捉语义关系的连续性。


<details>
  <summary>Details</summary>
Motivation: 现有语义熵方法通过设定阈值衡量语义等价关系，但无法充分反映语义关系的连续特性，需要更精细的不确定性度量框架来提升大语言模型输出可信度的评估效果。

Method: 开发基于Shapley值的不确定性度量指标，该指标无需依赖阈值设定，并证明了该指标满足有效不确定性度量的三个基本性质。

Result: 实验表明，在问答及其他数据集上，Shapley不确定性指标比基线方法能更准确地预测大语言模型的性能表现。

Conclusion: Shapley不确定性框架为评估大语言模型输出可信度提供了更优的连续化度量工具，其理论性质和实践效果均优于基于阈值的语义熵方法。

Abstract: In question-answering tasks, determining when to trust the outputs is crucial
to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces
semantic entropy as a measure of uncertainty, by incorporating linguistic
invariances from the same meaning. It primarily relies on setting threshold to
measure the level of semantic equivalence relation. We propose a more nuanced
framework that extends beyond such thresholding by developing a Shapley-based
uncertainty metric that captures the continuous nature of semantic
relationships. We establish three fundamental properties that characterize
valid uncertainty metrics and prove that our Shapley uncertainty satisfies
these criteria. Through extensive experiments, we demonstrate that our Shapley
uncertainty more accurately predicts LLM performance in question-answering and
other datasets, compared to similar baseline measures.

</details>


### [122] [Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects](https://arxiv.org/abs/2507.21407)
*Yixin Liu,Guibin Zhang,Kun Wang,Shiyuan Li,Shirui Pan*

Main category: cs.AI

TL;DR: 本文综述了基于图增强的大语言模型代理（GLA）的最新研究进展，重点探讨了图结构在提升代理规划、记忆、工具使用及多代理协调中的关键作用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理在规划、长期记忆、工具管理和多代理协调等方面存在局限，图结构可有效增强复杂代理工作流的结构性和连续性，但相关研究分散且缺乏系统梳理。

Method: 通过功能分类（规划、记忆、工具使用）分析图及图学习算法在GLA中的作用，并讨论多代理系统中图解决方案对协调、效率优化和可信度的贡献。

Result: 研究表明图结构能显著提升LLM代理系统的性能，尤其在结构化任务和多代理协作场景中，同时揭示了现有方法在适应性、可扩展性方面的不足。

Conclusion: 未来需重点改进图结构的适应性、构建统一可扩展的多模态GLA系统，本文旨在为该领域研究提供路线图，深化对图在LLM代理中作用的理解。

Abstract: Autonomous agents based on large language models (LLMs) have demonstrated
impressive capabilities in a wide range of applications, including web
navigation, software development, and embodied control. While most LLMs are
limited in several key agentic procedures, such as reliable planning, long-term
memory, tool management, and multi-agent coordination, graphs can serve as a
powerful auxiliary structure to enhance structure, continuity, and coordination
in complex agent workflows. Given the rapid growth and fragmentation of
research on Graph-augmented LLM Agents (GLA), this paper offers a timely and
comprehensive overview of recent advances and also highlights key directions
for future work. Specifically, we categorize existing GLA methods by their
primary functions in LLM agent systems, including planning, memory, and tool
usage, and then analyze how graphs and graph learning algorithms contribute to
each. For multi-agent systems, we further discuss how GLA solutions facilitate
the orchestration, efficiency optimization, and trustworthiness of MAS.
Finally, we highlight key future directions to advance this field, from
improving structural adaptability to enabling unified, scalable, and multimodal
GLA systems. We hope this paper can serve as a roadmap for future research on
GLA and foster a deeper understanding of the role of graphs in LLM agent
systems.

</details>


### [123] [GovRelBench:A Benchmark for Government Domain Relevance](https://arxiv.org/abs/2507.21419)
*Haiquan Wang,Yi Chen,Shang Zeng,Yun Bian,Zhe Cui*

Main category: cs.AI

TL;DR: 本文提出了GovRelBench基准和SoftGovScore方法，专门用于评估大语言模型在政府领域的核心能力，特别是领域相关性。


<details>
  <summary>Details</summary>
Motivation: 当前对政府领域大语言模型的评估主要关注特定场景的安全性，而对模型核心能力（尤其是领域相关性）的评估不足。

Method: 开发了GovRelBench基准（包含政府领域提示词和评估工具GovRelBERT），并提出了SoftGovScore方法——通过将硬标签转换为软分数来训练基于ModernBERT架构的模型。

Result: 所提出的方法能精确计算文本的政府领域相关性分数，代码和数据集已在GitHub开源。

Conclusion: 该工作完善了政府领域大模型的评估框架，为相关研究和实践提供了有效工具。

Abstract: Current evaluations of LLMs in the government domain primarily focus on
safety considerations in specific scenarios, while the assessment of the
models' own core capabilities, particularly domain relevance, remains
insufficient. To address this gap, we propose GovRelBench, a benchmark
specifically designed for evaluating the core capabilities of LLMs in the
government domain. GovRelBench consists of government domain prompts and a
dedicated evaluation tool, GovRelBERT. During the training process of
GovRelBERT, we introduce the SoftGovScore method: this method trains a model
based on the ModernBERT architecture by converting hard labels to soft scores,
enabling it to accurately compute the text's government domain relevance score.
This work aims to enhance the capability evaluation framework for large models
in the government domain, providing an effective tool for relevant research and
practice. Our code and dataset are available at
https://github.com/pan-xi/GovRelBench.

</details>


### [124] [Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models](https://arxiv.org/abs/2507.21438)
*Vishal Raman,Vijai Aravindh R*

Main category: cs.AI

TL;DR: 提出Evo-DKD框架，通过双解码器结构结合结构化本体遍历与非结构化文本推理，实现本体自主演化，提升知识图谱更新的精度与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 本体与知识图谱需持续演化以保持全面准确，但人工维护成本高；大语言模型虽蕴含丰富非结构化知识，却难以保持结构化一致性。

Method: 采用双解码器框架：一个生成本体编辑候选（如新概念/关系），另一个生成自然语言解释；通过动态注意力门控机制协调两者，并采用提示模式控制模拟双流解码。系统形成闭环推理：编辑提案经一致性校验后注入知识库，反馈至后续推理。

Result: 实验表明Evo-DKD在医疗本体优化、语义搜索改进等场景中，本体更新精度与下游任务表现均优于纯结构化或非结构化基线方法。定量指标与定性案例验证了双解码器设计与门控路由器的有效性。

Conclusion: Evo-DKD为LLM驱动的知识库维护提供新范式，融合符号推理与神经推理优势，实现可持续的本体演化。

Abstract: Ontologies and knowledge graphs require continuous evolution to remain
comprehensive and accurate, but manual curation is labor intensive. Large
Language Models (LLMs) possess vast unstructured knowledge but struggle with
maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder
framework for autonomous ontology evolution that combines structured ontology
traversal with unstructured text reasoning. Evo-DKD introduces two parallel
decoding streams within an LLM: one decoder generates candidate ontology edits
(e.g., new concepts or relations) while the other produces natural-language
justifications. A dynamic attention-based gating mechanism coordinates the two
streams, deciding at each step how to blend structured and unstructured
knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using
prompt-based mode control to approximate coordinated decoding in a
single-stream mode. The system operates in a closed reasoning loop: proposed
ontology edits are validated (via consistency checks and cross-verification
with the text explanations) and then injected into the knowledge base, which in
turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on
use cases including healthcare ontology refinement, semantic search
improvement, and cultural heritage timeline modeling. Experiments show that
Evo-DKD outperforms baselines using structured-only or unstructured-only
decoding in both precision of ontology updates and downstream task performance.
We present quantitative metrics and qualitative examples, confirming the
contributions of the dual-decoder design and gating router. Evo-DKD offers a
new paradigm for LLM-driven knowledge base maintenance, combining the strengths
of symbolic and neural reasoning for sustainable ontology evolution.

</details>


### [125] [Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2507.21453)
*Ashley Rector,Keaton Minor,Kamden Minor,Jeff McCormack,Beth Breeden,Ryan Nowers,Jay Dorris*

Main category: cs.AI

TL;DR: 本研究评估了基于大型语言模型和检索增强生成（RAG）的人工智能工具Sherpa Rx在药物基因组学中的表现，验证了其在关键响应指标上的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证Sherpa Rx整合CPIC指南与PharmGKB数据后，能否生成上下文相关的准确响应，提升药物基因组学决策支持。

Method: 使用260个查询的数据集评估26项CPIC指南，分两阶段（仅CPIC数据、CPIC+PharmGKB）测试药物-基因相互作用、剂量建议等指标，并与ChatGPT-4omini对比。

Result: Sherpa Rx在准确性（4.9/5）、召回率（0.99）等指标表现优异；第二阶段虽未显著提升准确性，但显著优于ChatGPT-4omini（90% vs 其他模型低准确率）。

Conclusion: 整合CPIC和PharmGKB的RAG技术能显著提升AI性能，Sherpa Rx展现了生成式AI在药物基因组学中提供精准个性化决策的变革潜力。

Abstract: This study evaluated Sherpa Rx, an artificial intelligence tool leveraging
large language models and retrieval-augmented generation (RAG) for
pharmacogenomics, to validate its performance on key response metrics. Sherpa
Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC)
guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate
contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC
guidelines was used to evaluate drug-gene interactions, dosing recommendations,
and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2
additionally incorporated PharmGKB content. Responses were scored on accuracy,
relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon
signed-rank tests compared accuracy between Phase 1 and Phase 2, and between
Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world
applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated
high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8,
and recall 0.99. The subset analysis (N=20) showed improvements in accuracy
(4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8).
ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but
lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy
between Phase 1 and Phase 2 was not statistically significant. However, Phase 2
significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx
achieved 90% accuracy, outperforming other models. Integrating additional
resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance.
This study highlights the transformative potential of generative AI like Sherpa
Rx in pharmacogenomics, improving decision-making with accurate, personalized
responses.

</details>


### [126] [An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning](https://arxiv.org/abs/2507.21471)
*Zujie Xie,Zixuan Chen,Jiheng Liang,Xiangyang Yu,Ziru Yu*

Main category: cs.AI

TL;DR: 本研究提出了一种基于大语言模型（LLM）的端到端框架，用于在低数据条件下实现红外光谱的自动准确解析。该框架整合了文献知识库、光谱预处理、特征提取和多任务推理，通过多轮闭环协议动态优化预测，在多种材料数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 红外光谱虽能快速无损测量化学物质特性，但其高维重叠谱带对传统化学计量学方法构成挑战。大语言模型（LLM）具有泛化和推理能力，为自动化复杂科学流程提供了新可能，但其在红外光谱分析中的应用尚未充分探索。

Method: 研究引入了一个端到端的LLM驱动代理框架，该框架集成了结构化文献知识库、自动光谱预处理、特征提取和多任务推理。通过查询经过筛选的同行评议红外文献，代理选择科学验证的方法，将光谱转化为低维特征集，并输入少样本提示模板进行分类、回归和异常检测。采用闭环多轮协议动态优化预测。

Result: 在多种材料数据集（包括印泥、中药、普洱茶、陈皮和废水COD）上，多轮LLM推理的表现持续优于单轮推理，并在低数据条件下媲美或超越传统机器学习和深度学习模型。

Conclusion: 该研究证明了LLM驱动框架在低数据条件下实现红外光谱自动解析的有效性，为复杂科学工作流的自动化提供了新思路。

Abstract: Infrared spectroscopy offers rapid, non destructive measurement of chemical
and material properties but suffers from high dimensional, overlapping spectral
bands that challenge conventional chemometric approaches. Emerging large
language models (LLMs), with their capacity for generalization and reasoning,
offer promising potential for automating complex scientific workflows. Despite
this promise, their application in IR spectral analysis remains largely
unexplored. This study addresses the critical challenge of achieving accurate,
automated infrared spectral interpretation under low-data conditions using an
LLM-driven framework. We introduce an end-to-end, large language model driven
agent framework that integrates a structured literature knowledge base,
automated spectral preprocessing, feature extraction, and multi task reasoning
in a unified pipeline. By querying a curated corpus of peer reviewed IR
publications, the agent selects scientifically validated routines. The selected
methods transform each spectrum into low dimensional feature sets, which are
fed into few shot prompt templates for classification, regression, and anomaly
detection. A closed loop, multi turn protocol iteratively appends mispredicted
samples to the prompt, enabling dynamic refinement of predictions. Across
diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri
Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM
consistently outperforms single turn inference, rivaling or exceeding machine
learning and deep learning models under low data regimes.

</details>


### [127] [Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess](https://arxiv.org/abs/2507.21488)
*Zhenwei Tang,Difan Jiao,Eric Xue,Reid McIlroy-Young,Jon Kleinberg,Siddhartha Sen,Ashton Anderson*

Main category: cs.AI

TL;DR: 本文提出Maia4All框架，通过两阶段优化过程高效学习并适应个体决策风格，仅需20局棋局数据即可精准建模人类棋手行为，显著提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 随着人类与AI系统的协作需求增加，开发能精准模拟个体决策的AI变得至关重要。国际象棋作为AI长期基准测试领域，为研究人机对齐提供了理想平台。但现有方法需要大量个体数据，难以适用于新用户或数据稀疏场景。

Method: 采用两阶段优化：(1) 丰富化步骤：通过原型增强模型桥接群体与个体行为建模；(2) 民主化步骤：利用能力等级或用户原型初始化个体嵌入，仅需极少数据即可优化。

Result: 实验表明Maia4All仅需20局棋局（原需5000局）即可高保真预测个体走棋行为，在象棋个性化建模方面树立新标准。案例研究证明该方法可扩展至个性化大语言模型适配。

Conclusion: 该研究通过原型增强模型实现群体AI系统向个体用户的灵活适配，其方法论超越象棋领域，在个性化AI适配方面具有广泛的应用潜力。

Abstract: As humans seek to collaborate with, learn from, and better understand
artificial intelligence systems, developing AIs that can accurately emulate
individual decision-making becomes increasingly important. Chess, a
long-standing AI benchmark with precise skill measurement, offers an ideal
testbed for human-AI alignment. However, existing approaches to modeling human
behavior require prohibitively large amounts of data from each individual,
making them impractical for new or sparsely represented users. In this work, we
introduce Maia4All, a framework designed to learn and adapt to individual
decision-making styles efficiently, even with limited data. Maia4All achieves
this through a two-stage optimization process: (1) an enrichment step, which
bridges population and individual-level human behavior modeling with a
prototype-enriched model, and (2) a democratization step, which leverages
ability levels or user prototypes to initialize and refine individual
embeddings with minimal data. Our experimental results show that Maia4All can
accurately predict individual moves and profile behavioral patterns with high
fidelity, establishing a new standard for personalized human-like AI behavior
modeling in chess. Maia4All achieves individual human behavior modeling in
chess with only 20 games, compared to the 5,000 games required previously,
representing a significant improvement in data efficiency. Our work provides an
example of how population AI systems can flexibly adapt to individual users
using a prototype-enriched model as a bridge. This approach extends beyond
chess, as shown in our case study on idiosyncratic LLMs, highlighting its
potential for broader applications in personalized AI adaptation.

</details>


### [128] [Large Language Models for Supply Chain Decisions](https://arxiv.org/abs/2507.21502)
*David Simchi-Levi,Konstantina Mellou,Ishai Menache,Jeevan Pathuri*

Main category: cs.AI

TL;DR: 供应链管理面临复杂决策挑战，传统优化技术依赖人工解释与模型更新。研究利用大语言模型（LLMs）实现工具结果自动化理解与交互，将决策时间从数周缩短至数小时。


<details>
  <summary>Details</summary>
Motivation: 现有供应链优化技术虽提升决策数据驱动性，但业务人员仍需大量时间理解结果、分析场景及更新模型，导致决策效率低下。LLMs的突破性进展为技术民主化提供可能。

Method: 应用LLMs技术解决三大核心挑战：(1)自动化解释工具输出结果；(2)支持交互式场景分析与假设问答；(3)动态调整数学模型以适配当前商业环境，无需人工介入。

Result: LLMs将供应链决策周期从数天/周压缩至分钟/小时级别，显著提升规划者与高管的决策效率（生产力提升10倍）及商业影响力。

Conclusion: LLMs通过消除人工干预瓶颈，实现了供应链技术的民主化应用，为复杂决策系统的人机交互范式提供了变革性解决方案。

Abstract: Supply Chain Management requires addressing a variety of complex
decision-making challenges, from sourcing strategies to planning and execution.
Over the last few decades, advances in computation and information technologies
have enabled the transition from manual, intuition and experience-based
decision-making, into more automated and data-driven decisions using a variety
of tools that apply optimization techniques. These techniques use mathematical
methods to improve decision-making.
  Unfortunately, business planners and executives still need to spend
considerable time and effort to (i) understand and explain the recommendations
coming out of these technologies; (ii) analyze various scenarios and answer
what-if questions; and (iii) update the mathematical models used in these tools
to reflect current business environments. Addressing these challenges requires
involving data science teams and/or the technology providers to explain results
or make the necessary changes in the technology and hence significantly slows
down decision making.
  Motivated by the recent advances in Large Language Models (LLMs), we report
how this disruptive technology can democratize supply chain technology -
namely, facilitate the understanding of tools' outcomes, as well as the
interaction with supply chain tools without human-in-the-loop. Specifically, we
report how we apply LLMs to address the three challenges described above, thus
substantially reducing the time to decision from days and weeks to minutes and
hours as well as dramatically increasing planners' and executives' productivity
and impact.

</details>


### [129] [MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions](https://arxiv.org/abs/2507.21503)
*Yanxu Zhu,Shitong Duan,Xiangxu Zhang,Jitao Sang,Peng Zhang,Tun Lu,Xiao Zhou,Jing Yao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 本文首次系统评估了多模态大语言模型（MLLMs）在视觉不可回答问题上的诚实行为，构建了大规模基准MoHoBench，发现多数模型无法恰当拒绝回答，并提出监督与偏好学习对齐方法以提升可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉语言任务中取得进展，但其面对不可视问题时生成有害或不可信内容的行为尚未被充分研究，亟需系统性评估与改进方法。

Method: 研究者定义了四类视觉不可回答问题，通过多阶段筛选与人工验证构建了包含12k+样本的MoHoBench基准，并测试了28个主流MLLMs的诚实性，最终采用监督学习和偏好学习进行对齐优化。

Result: 实验表明：（1）多数模型无法在必要时拒绝回答；（2）诚实性不仅与语言建模相关，更受视觉信息显著影响，需开发专门的多模态对齐方法。初步对齐实验验证了方法的有效性。

Conclusion: 该研究为可信MLLMs的发展奠定了基础，强调需针对多模态特性设计诚实对齐方案，公开的MoHoBench数据与代码将推动未来研究。

Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable
advancements in vision-language tasks, yet produce potentially harmful or
untrustworthy content. Despite substantial work investigating the
trustworthiness of language models, MMLMs' capability to act honestly,
especially when faced with visually unanswerable questions, remains largely
underexplored. This work presents the first systematic assessment of honesty
behaviors across various MLLMs. We ground honesty in models' response behaviors
to unanswerable visual questions, define four representative types of such
questions, and construct MoHoBench, a large-scale MMLM honest benchmark,
consisting of 12k+ visual question samples, whose quality is guaranteed by
multi-stage filtering and human verification. Using MoHoBench, we benchmarked
the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our
findings show that: (1) most models fail to appropriately refuse to answer when
necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but
is deeply influenced by visual information, necessitating the development of
dedicated methods for multimodal honesty alignment. Therefore, we implemented
initial alignment methods using supervised and preference learning to improve
honesty behavior, providing a foundation for future work on trustworthy MLLMs.
Our data and code can be found at https://github.com/DSTTSD/MoHoBench.

</details>


### [130] [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
*Kenneth Li,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 提出了一套精确标准来定义神经网络如何学习并使用\"世界模型\"，旨在为实验研究提供共同语言，重点关注潜在\"状态空间\"的表示。


<details>
  <summary>Details</summary>
Motivation: 为常被非正式使用的术语赋予操作意义，建立实验研究的共同框架，特别关注世界潜在状态的表示。

Method: 基于线性探测文献思想，形式化通过数据生成过程表示的计算概念，并增加条件以避免\"世界模型\"成为数据或任务的平凡结果。

Result: 提出了一套可操作的标准，用于验证神经网络是否真正学习并使用了非平凡的世界模型。

Conclusion: 该定义为研究神经网络中的世界模型提供了理论基础，未来可扩展至动作影响的建模。

Abstract: We propose a set of precise criteria for saying a neural net learns and uses
a "world model." The goal is to give an operational meaning to terms that are
often used informally, in order to provide a common language for experimental
investigation. We focus specifically on the idea of representing a latent
"state space" of the world, leaving modeling the effect of actions to future
work. Our definition is based on ideas from the linear probing literature, and
formalizes the notion of a computation that factors through a representation of
the data generation process. An essential addition to the definition is a set
of conditions to check that such a "world model" is not a trivial consequence
of the neural net's data or task.

</details>


### [131] [ST-GDance: Long-Term and Collision-Free Group Choreography from Music](https://arxiv.org/abs/2507.21518)
*Jing Xu,Weiqiang Wang,Cunjian Chen,Jun Liu,Qiuhong Ke*

Main category: cs.AI

TL;DR: 提出ST-GDance框架，通过解耦时空依赖优化群舞生成，解决多舞者同步与碰撞问题，在AIOZ-GDance数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 群舞生成需同步多舞者动作并避免碰撞，现有方法难以建模密集时空交互，导致可扩展性差与碰撞风险。

Method: 采用轻量图卷积进行距离感知的空间建模，结合加速稀疏注意力机制实现高效时序建模，显著降低计算成本。

Result: 实验表明ST-GDance在生成长时序连贯群舞方面优于基线，尤其擅长避免多舞者碰撞。

Conclusion: ST-GDance通过解耦时空依赖，为影视游戏等场景提供高效、无碰撞的群舞生成解决方案。

Abstract: Group dance generation from music has broad applications in film, gaming, and
animation production. However, it requires synchronizing multiple dancers while
maintaining spatial coordination. As the number of dancers and sequence length
increase, this task faces higher computational complexity and a greater risk of
motion collisions. Existing methods often struggle to model dense
spatial-temporal interactions, leading to scalability issues and multi-dancer
collisions. To address these challenges, we propose ST-GDance, a novel
framework that decouples spatial and temporal dependencies to optimize
long-term and collision-free group choreography. We employ lightweight graph
convolutions for distance-aware spatial modeling and accelerated sparse
attention for efficient temporal modeling. This design significantly reduces
computational costs while ensuring smooth and collision-free interactions.
Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms
state-of-the-art baselines, particularly in generating long and coherent group
dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.

</details>


### [132] [Large Language Models for Wireless Communications: From Adaptation to Autonomy](https://arxiv.org/abs/2507.21524)
*Le Liang,Hao Ye,Yucheng Sheng,Ouya Wang,Jiacheng Wang,Shi Jin,Geoffrey Ye Li*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）为无线通信系统带来变革，通过适应预训练模型、开发专用基础模型及赋能自主智能体，推动未来智能自适应网络发展。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统日益复杂且动态变化，需要智能自适应解决方案，而大语言模型（LLMs）在推理、泛化和零样本学习方面的卓越能力为此提供了新机遇。

Method: 研究围绕三个方向展开：1）将预训练LLMs适配核心通信任务；2）开发兼顾通用性与效率的无线专用基础模型；3）赋予LLMs自主推理与协同决策的智能体能力。

Result: LLM方法展现出超越传统技术的独特优势，案例研究验证了其在无线系统中的可行性，同时揭示了多模态融合、轻量化协作及自我改进等挑战。

Conclusion: LLMs有望构建智能、自适应、自主的未来无线网络，但需解决跨模态整合、资源效率优化等开放性问题以释放其全部潜力。

Abstract: The emergence of large language models (LLMs) has revolutionized artificial
intelligence, offering unprecedented capabilities in reasoning, generalization,
and zero-shot learning. These strengths open new frontiers in wireless
communications, where increasing complexity and dynamics demand intelligent and
adaptive solutions. This article explores the role of LLMs in transforming
wireless systems across three key directions: adapting pretrained LLMs for core
communication tasks, developing wireless-specific foundation models to balance
versatility and efficiency, and enabling agentic LLMs with autonomous reasoning
and coordination capabilities. We highlight recent advances, practical case
studies, and the unique benefits of LLM-based approaches over traditional
methods. Finally, we outline open challenges and research opportunities,
including multimodal fusion, collaboration with lightweight models, and
self-improving capabilities, charting a path toward intelligent, adaptive, and
autonomous wireless networks of the future.

</details>


### [133] [Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations](https://arxiv.org/abs/2507.21571)
*Laura Spillner,Nima Zargham,Mihai Pomarlan,Robert Porzel,Rainer Malaka*

Main category: cs.AI

TL;DR: Error processing this paper.


<details>
  <summary>Details</summary>
Motivation: Error processing this paper.

Method: Error processing this paper.

Result: Error processing this paper.

Conclusion: Error processing this paper.

Abstract: The need for explanations in AI has, by and large, been driven by the desire
to increase the transparency of black-box machine learning models. However,
such explanations, which focus on the internal mechanisms that lead to a
specific output, are often unsuitable for non-experts. To facilitate a
human-centered perspective on AI explanations, agents need to focus on
individuals and their preferences as well as the context in which the
explanations are given. This paper proposes a personalized approach to
explanation, where the agent tailors the information provided to the user based
on what is most likely pertinent to them. We propose a model of the agent's
worldview that also serves as a personal and dynamic memory of its previous
interactions with the same user, based on which the artificial agent can
estimate what part of its knowledge is most likely new information to the user.

</details>


### [134] [SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21585)
*Hao Ye,Mengshi Qi,Zhaohong Liu,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: 本文提出首个大规模多模态交通安全问答基准SafeDrive228K（含22.8万样本）及基于知识图谱检索增强生成的VLM新方法SafeDriveRAG，显著提升自动驾驶系统在事故处理、极端场景等安全关键任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对视觉语言模型在交通安全关键场景下的系统评估，阻碍了其在自动驾驶安全增强中的应用。

Method: 1) 构建覆盖18个子任务的SafeDrive228K基准；2) 提出多尺度子图检索算法，结合网络交通安全知识实现检索增强生成框架SafeDriveRAG。

Result: 在五大主流VLM上，RAG集成使交通事故任务提升4.73%、极端场景任务提升8.79%、交通常识任务提升14.57%。

Conclusion: 该基准与方法为交通安全研究提供新工具，证实知识增强能显著提升VLM在安全敏感驾驶任务中的可靠性。

Abstract: In this work, we study how vision-language models (VLMs) can be utilized to
enhance the safety for the autonomous driving system, including perception,
situational understanding, and path planning. However, existing research has
largely overlooked the evaluation of these models in traffic safety-critical
driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K)
and propose a new baseline based on VLM with knowledge graph-based
retrieval-augmented generation (SafeDriveRAG) for visual question answering
(VQA). Specifically, we introduce SafeDrive228K, the first large-scale
multimodal question-answering benchmark comprising 228K examples across 18
sub-tasks. This benchmark encompasses a diverse range of traffic safety
queries, from traffic accidents and corner cases to common safety knowledge,
enabling a thorough assessment of the comprehension and reasoning abilities of
the models. Furthermore, we propose a plug-and-play multimodal knowledge
graph-based retrieval-augmented generation approach that employs a novel
multi-scale subgraph retrieval algorithm for efficient information retrieval.
By incorporating traffic safety guidelines collected from the Internet, this
framework further enhances the model's capacity to handle safety-critical
situations. Finally, we conduct comprehensive evaluations on five mainstream
VLMs to assess their reliability in safety-sensitive driving tasks.
Experimental results demonstrate that integrating RAG significantly improves
performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in
Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five
mainstream VLMs, underscoring the potential of our proposed benchmark and
methodology for advancing research in traffic safety. Our source code and data
are available at https://github.com/Lumos0507/SafeDriveRAG.

</details>


### [135] [Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning](https://arxiv.org/abs/2507.21588)
*Jiong Yin,Liang Li,Jiehua Zhang,Yuhan Gao,Chenggang Yan,Xichun Sheng*

Main category: cs.AI

TL;DR: 本文提出了一种三阶段渐进式稳态与可塑性视听提示（PHP）方法，用于解决视听多任务增量学习中的旧任务知识保留与新任务学习挑战，并在四个任务上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 视听多任务增量学习旨在无需联合训练所有任务的情况下持续学习多个视听任务，核心挑战是如何在保留旧任务知识的同时促进新任务学习。

Method: PHP方法分为三个阶段：1) 浅层阶段设计任务共享模态聚合适配器以增强跨任务跨模态表示学习；2) 中层阶段提出任务特定模态共享动态生成适配器，平衡知识保留与多任务迁移能力；3) 深层阶段引入任务特定模态独立提示，针对各任务和模态细化理解能力。

Result: 该方法在AVE、AVVP、AVS和AVQA四个任务的不同顺序上均实现了最先进的性能（SOTA）。

Conclusion: PHP通过三阶段设计有效平衡了知识共享与任务特异性，在保留任务特定提示的同时适应新任务的共享参数，为视听多任务增量学习提供了有效解决方案。

Abstract: Audio-visual multi-task incremental learning aims to continuously learn from
multiple audio-visual tasks without the need for joint training on all tasks.
The challenge of the problem is how to preserve the old task knowledge while
facilitating the learning of new task with previous experiences. To address
these challenges, we introduce a three-stage Progressive Homeostatic and
Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the
task-shared modality aggregating adapter to foster cross-task and cross-modal
audio-visual representation learning to enhance shared understanding between
tasks. In the middle phase, we propose the task-specific modality-shared
dynamic generating adapter, which constructs prompts that are tailored to
individual tasks while remaining general across modalities, which balances the
models ability to retain knowledge against forgetting with its potential for
versatile multi-task transferability. In the deep phase, we introduce the
task-specific modality-independent prompts to further refine the understand
ability by targeting individual information for each task and modality. By
incorporating these three phases, PHP retains task-specific prompts while
adapting shared parameters for new tasks to effectively balance knowledge
sharing and specificity. Our method achieves SOTA performance in different
orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at
https://github.com/ENJOY-Yin-jiong/PHP.

</details>


### [136] [Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems](https://arxiv.org/abs/2507.21589)
*Bin Liu*

Main category: cs.AI

TL;DR: 本文探讨了贝叶斯统计与具身智能的深层联系，指出当前具身智能系统未充分利用贝叶斯方法，并分析了其在开放物理世界中的潜在价值。


<details>
  <summary>Details</summary>
Motivation: 具身智能认为认知能力源于智能体与环境的实时感知运动交互，而贝叶斯统计为处理这种不确定性提供了概率框架。然而，当前具身智能系统尚未广泛采用贝叶斯方法。

Method: 通过Rich Sutton提出的'搜索与学习'两大AI核心主题，对比分析贝叶斯方法与当代具身智能方法的差异。

Result: 研究发现贝叶斯推理未成为现代具身智能发展的核心，同时揭示当前系统局限于封闭物理环境，贝叶斯方法有望推动开放物理世界具身智能的发展。

Conclusion: 贝叶斯方法在具身智能领域具有重要潜力，特别是在扩展系统至开放物理世界方面可能发挥关键作用。

Abstract: Embodied intelligence posits that cognitive capabilities fundamentally emerge
from - and are shaped by - an agent's real-time sensorimotor interactions with
its environment. Such adaptive behavior inherently requires continuous
inference under uncertainty. Bayesian statistics offers a principled
probabilistic framework to address this challenge by representing knowledge as
probability distributions and updating beliefs in response to new evidence. The
core computational processes underlying embodied intelligence - including
perception, action selection, learning, and even higher-level cognition - can
be effectively understood and modeled as forms of Bayesian inference. Despite
the deep conceptual connection between Bayesian statistics and embodied
intelligence, Bayesian principles have not been widely or explicitly applied in
today's embodied intelligence systems. In this work, we examine both Bayesian
and contemporary embodied intelligence approaches through two fundamental
lenses: search and learning - the two central themes in modern AI, as
highlighted in Rich Sutton's influential essay "The Bitter Lesson". This
analysis sheds light on why Bayesian inference has not played a central role in
the development of modern embodied intelligence. At the same time, it reveals
that current embodied intelligence systems remain largely confined to
closed-physical-world environments, and highlights the potential for Bayesian
methods to play a key role in extending these systems toward truly open
physical-world embodied intelligence.

</details>


### [137] ["Teammates, Am I Clear?": Analysing Legible Behaviours in Teams](https://arxiv.org/abs/2507.21631)
*Miguel Faria,Francisco S. Melo,Ana Paiva*

Main category: cs.AI

TL;DR: 本文探讨了团队协作中序列决策的可读性概念，提出了一种多智能体环境下的可读决策扩展方法，并通过实验证明可读性智能体团队优于标准最优行为团队。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单智能体与单人类的交互，忽略了团队协作中可读决策的潜在优势。本文旨在填补这一空白，探索多智能体环境下可读性对团队性能的提升作用。

Method: 提出将可读决策概念扩展至多智能体场景，并在多智能体基准测试中验证所提方法的有效性。

Result: 实验表明，包含可读性智能体的团队性能优于完全由标准最优行为智能体组成的团队。

Conclusion: 团队协作中引入可读性决策能显著提升整体性能，为多智能体系统的设计提供了新的研究方向。

Abstract: In this paper we investigate the notion of legibility in sequential
decision-making in the context of teams and teamwork. There have been works
that extend the notion of legibility to sequential decision making, for
deterministic and for stochastic scenarios. However, these works focus on one
agent interacting with one human, foregoing the benefits of having legible
decision making in teams of agents or in team configurations with humans. In
this work we propose an extension of legible decision-making to multi-agent
settings that improves the performance of agents working in collaboration. We
showcase the performance of legible decision making in team scenarios using our
proposed extension in multi-agent benchmark scenarios. We show that a team with
a legible agent is able to outperform a team composed solely of agents with
standard optimal behaviour.

</details>


### [138] [StaffPro: an LLM Agent for Joint Staffing and Profiling](https://arxiv.org/abs/2507.21636)
*Alessio Maritan*

Main category: cs.AI

TL;DR: 本文提出StaffPro，一种结合大语言模型（LLM）的智能体，用于联合解决劳动力管理中的员工排班与能力画像问题。通过自然语言交互与持续反馈机制，该系统实现了动态人员调度与属性估计，并在模拟咨询公司场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统劳动力管理系统难以处理非结构化数据且缺乏灵活性。研究旨在开发能自然语言交互、持续优化排班决策并动态更新员工画像的智能解决方案，以提升人机协作效率。

Method: 建立数学框架将排班决策与潜在特征估计关联，设计LLM智能体StaffPro：1) 支持自然语言定义优化目标；2) 通过人机反馈循环持续学习员工技能/偏好；3) 结合文本任务描述实现灵活调度。

Result: 咨询公司模拟实验表明，StaffPro能准确估计员工潜在属性（技能/偏好），生成高质量排班方案。系统在持续交互中保持调度最优性，验证了方法的鲁棒性。

Conclusion: StaffPro通过LLM与算法模块的创新结合，提供了可解释、以人为中心的自动化人力管理方案。其自然语言接口与终身学习机制为动态劳动力管理开辟了新途径。

Abstract: Large language model (LLM) agents integrate pre-trained LLMs with modular
algorithmic components and have shown remarkable reasoning and decision-making
abilities. In this work, we investigate their use for two tightly intertwined
challenges in workforce management: staffing, i.e., the assignment and
scheduling of tasks to workers, which may require team formation; and
profiling, i.e., the continuous estimation of workers' skills, preferences, and
other latent attributes from unstructured data. We cast these problems in a
formal mathematical framework that links scheduling decisions to latent feature
estimation, and we introduce StaffPro, an LLM agent that addresses staffing and
profiling jointly. Differently from existing staffing solutions, StaffPro
allows expressing optimization objectives using natural language, accepts
textual task descriptions and provides high flexibility. StaffPro interacts
directly with humans by establishing a continuous human-agent feedback loop,
ensuring natural and intuitive use. By analyzing human feedback, our agent
continuously estimates the latent features of workers, realizing life-long
worker profiling and ensuring optimal staffing performance over time. A
consulting firm simulation example demonstrates that StaffPro successfully
estimates workers' attributes and generates high quality schedules. With its
innovative design, StaffPro offers a robust, interpretable, and human-centric
solution for automated personnel management.

</details>


### [139] [Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models](https://arxiv.org/abs/2507.21637)
*Wanying Wang,Zeyu Ma,Han Zheng,Xin Tan,Mingang Chen*

Main category: cs.AI

TL;DR: 研究发现大型视觉语言模型（LVLMs）比纯语言模型更容易受到有害输入的影响，并提出了一种名为SASA的自增强安全技术，通过利用模型内部的语义理解来提升安全性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在面对有害输入时表现出比纯语言模型更低的防御能力，研究者试图通过分析其内部动态来理解这一脆弱性，并提出改进方案。

Method: 研究者定义了三种关键能力（安全感知、语义理解和语言表达对齐），并通过实验定位了它们在模型架构中的主要位置。基于这些发现，提出了\textbf{自增强安全技术（SASA）}，将中间层的语义信息投射到早期的安全导向层，利用线性探测技术提前识别风险。

Result: 实验表明，SASA显著提升了LVLMs的安全性，同时对模型的实用性影响极小。

Conclusion: 通过利用模型内部的语义理解能力，SASA有效增强了LVLMs的安全性，为未来的模型安全研究提供了新的方向。

Abstract: Large vision-language models (LVLMs) are vulnerable to harmful input compared
to their language-only backbones. We investigated this vulnerability by
exploring LVLMs internal dynamics, framing their inherent safety understanding
in terms of three key capabilities. Specifically, we define these capabilities
as safety perception, semantic understanding, and alignment for linguistic
expression, and experimentally pinpointed their primary locations within the
model architecture. The results indicate that safety perception often emerges
before comprehensive semantic understanding, leading to the reduction in
safety. Motivated by these findings, we propose \textbf{Self-Aware Safety
Augmentation (SASA)}, a technique that projects informative semantic
representations from intermediate layers onto earlier safety-oriented layers.
This approach leverages the model's inherent semantic understanding to enhance
safety recognition without fine-tuning. Then, we employ linear probing to
articulate the model's internal semantic comprehension to detect the risk
before the generation process. Extensive experiments on various datasets and
tasks demonstrate that SASA significantly improves the safety of LVLMs, with
minimal impact on the utility.

</details>


### [140] [Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics](https://arxiv.org/abs/2507.21638)
*Leonard Hinckeldey,Elliot Fosong,Elle Miller,Rimvydas Rubavicius,Trevor McInroe,Patricia Wollstadt,Christiane B. Wiebel-Herboth,Subramanian Ramamoorthy,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 论文介绍了Assistax，一个用于辅助机器人任务的开源强化学习基准，利用JAX硬件加速实现快速训练，并通过多智能体强化学习测试机器人协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习基准多基于游戏，难以直接应用于现实世界的具身交互场景，因此需要多样化的基准来应对辅助机器人任务中的复杂挑战。

Method: Assistax采用JAX硬件加速，实现物理模拟中的快速学习；通过多智能体强化学习训练多样化的伙伴智能体，测试机器人零样本协调能力。

Result: Assistax在向量化训练时比CPU方案快$370\times$，并为连续控制和多智能体强化学习算法提供了可靠的基线。

Conclusion: Assistax是一个实用的基准，可推动辅助机器人领域的强化学习研究，其代码已开源。

Abstract: The development of reinforcement learning (RL) algorithms has been largely
driven by ambitious challenge tasks and benchmarks. Games have dominated RL
benchmarks because they present relevant challenges, are inexpensive to run and
easy to understand. While games such as Go and Atari have led to many
breakthroughs, they often do not directly translate to real-world embodied
applications. In recognising the need to diversify RL benchmarks and addressing
complexities that arise in embodied interaction scenarios, we introduce
Assistax: an open-source benchmark designed to address challenges arising in
assistive robotics tasks. Assistax uses JAX's hardware acceleration for
significant speed-ups for learning in physics-based simulations. In terms of
open-loop wall-clock time, Assistax runs up to $370\times$ faster when
vectorising training runs compared to CPU-based alternatives. Assistax
conceptualises the interaction between an assistive robot and an active human
patient using multi-agent RL to train a population of diverse partner agents
against which an embodied robotic agent's zero-shot coordination capabilities
can be tested. Extensive evaluation and hyperparameter tuning for popular
continuous control RL and MARL algorithms provide reliable baselines and
establish Assistax as a practical benchmark for advancing RL research for
assistive robotics. The code is available at:
https://github.com/assistive-autonomy/assistax.

</details>


### [141] [Can the current trends of AI handle a full course of mathematics?](https://arxiv.org/abs/2507.21664)
*Mariam Alsayyad,Fayadh Kadhem*

Main category: cs.AI

TL;DR: 本文探讨当前人工智能（AI）是否有能力承担大学数学课程的全部责任，评估其在课程大纲制定、教学内容展示、学生问题解答及考核设计四个方面的表现，指出AI虽在组织和准确性上有优势，但仍无法替代人类的情感因素，并提出人机协作的改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估当前AI技术是否足以独立承担大学数学课程的全流程教学任务，包括课程设计、教学实施和考核评估，以探索AI在教育领域的实际应用潜力与局限。

Method: 通过四个关键维度（课程大纲制定、教学内容展示、学生问题解答、考核设计）系统评估AI能力，对比分析其技术优势与人性化不足。

Result: AI在课程组织与知识准确性方面表现突出，但缺乏科学教育中不可或缺的情感互动与隐性知识传递能力，目前无法完全替代人类教师角色。

Conclusion: 建议采用人机协作模式整合双方优势——利用AI的流程化处理能力，结合人类教师的情感智慧，以最优方式实现大学数学课程教学目标。

Abstract: This paper addresses the question of how able the current trends of
Artificial Intelligence (AI) are in managing to take the responsibility of a
full course of mathematics at a college level. The study evaluates this ability
in four significant aspects, namely, creating a course syllabus, presenting
selected material, answering student questions, and creating an assessment. It
shows that even though the AI is strong in some important parts like
organization and accuracy, there are still some human aspects that are far away
from the current abilities of AI. There is still a hidden emotional part, even
in science, that cannot be fulfilled by the AI in its current state. This paper
suggests some recommendations to integrate the human and AI potentials to
create better outcomes in terms of reaching the target of creating a full
course of mathematics, at a university level, as best as possible.

</details>


### [142] [Unrolling Dynamic Programming via Graph Filters](https://arxiv.org/abs/2507.21705)
*Sergio Rozada,Samuel Rey,Gonzalo Mateos,Antonio G. Marques*

Main category: cs.AI

TL;DR: 本文提出了一种名为BellNet的新方法，通过将策略迭代展开并截断为可学习的参数模型，以减少动态规划中的计算成本。该方法利用图信号处理技术，将BellNet重新参数化为非线性图滤波器级联，有效近似最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统动态规划方法（如策略迭代）在状态-动作空间较大或涉及长期依赖时计算成本高昂。本文旨在提出一种更高效的方法来解决贝尔曼最优性方程。

Method: 提出BellNet模型，将策略迭代展开并截断为可学习的参数模型，通过最小化贝尔曼误差进行训练。利用图信号处理技术，将BellNet重新参数化为非线性图滤波器级联。

Result: 初步实验表明，BellNet在网格状环境中能够以远少于传统方法的迭代次数有效近似最优策略。

Conclusion: BellNet提供了一种简洁、可迁移且统一的策略与值迭代表示方法，显著降低了推理复杂度，为动态规划问题提供了高效解决方案。

Abstract: Dynamic programming (DP) is a fundamental tool used across many engineering
fields. The main goal of DP is to solve Bellman's optimality equations for a
given Markov decision process (MDP). Standard methods like policy iteration
exploit the fixed-point nature of these equations to solve them iteratively.
However, these algorithms can be computationally expensive when the
state-action space is large or when the problem involves long-term
dependencies. Here we propose a new approach that unrolls and truncates policy
iterations into a learnable parametric model dubbed BellNet, which we train to
minimize the so-termed Bellman error from random value function
initializations. Viewing the transition probability matrix of the MDP as the
adjacency of a weighted directed graph, we draw insights from graph signal
processing to interpret (and compactly re-parameterize) BellNet as a cascade of
nonlinear graph filters. This fresh look facilitates a concise, transferable,
and unifying representation of policy and value iteration, with an explicit
handle on complexity during inference. Preliminary experiments conducted in a
grid-like environment demonstrate that BellNet can effectively approximate
optimal policies in a fraction of the iterations required by classical methods.

</details>


### [143] [GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation](https://arxiv.org/abs/2507.21727)
*Jianfei Zhu,Haiqi Zhu,Shaohui Liu,Feng Jiang,Baichun Wei,Chunzhi Yi*

Main category: cs.AI

TL;DR: 提出GDAIP框架，结合图注意力网络与极小极大熵域适应方法，解决跨数据集脑区分割中的域偏移问题，实现个体化脑区分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设跨域数据分布一致，难以处理真实跨数据集场景中的域偏移问题，需开发适应个体差异的脑区分割方法。

Method: 构建群体/个体水平的跨数据集脑图，通过半监督训练和对抗性优化目标脑图未标记顶点的预测熵，将参考图谱从群体脑图适配至个体脑图。

Result: GDAIP生成的个体脑区具有拓扑合理的边界、跨会话强一致性，并能反映功能组织，可视化效果、Dice系数和功能同质性评估表现优异。

Conclusion: GDAIP框架成功实现跨数据集场景下的个体化脑区分割，为神经影像分析提供了有效的域适应解决方案。

Abstract: Recent deep learning approaches have shown promise in learning such
individual brain parcellations from functional magnetic resonance imaging
(fMRI). However, most existing methods assume consistent data distributions
across domains and struggle with domain shifts inherent to real-world
cross-dataset scenarios. To address this challenge, we proposed Graph Domain
Adaptation for Individual Parcellation (GDAIP), a novel framework that
integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based
domain adaptation. We construct cross-dataset brain graphs at both the group
and individual levels. By leveraging semi-supervised training and adversarial
optimization of the prediction entropy on unlabeled vertices from target brain
graph, the reference atlas is adapted from the group-level brain graph to the
individual brain graph, enabling individual parcellation under cross-dataset
settings. We evaluated our method using parcellation visualization, Dice
coefficient, and functional homogeneity. Experimental results demonstrate that
GDAIP produces individual parcellations with topologically plausible
boundaries, strong cross-session consistency, and ability of reflecting
functional organization.

</details>


### [144] [SAT-Based Bounded Fitting for the Description Logic ALC](https://arxiv.org/abs/2507.21752)
*Maurice Funk,Jean Christoph Jung,Tom Voellmer*

Main category: cs.AI

TL;DR: 本文研究了描述逻辑ALC及其语法片段的有界拟合问题，证明了其NP完全性，并展示了该方法的PAC学习保证。同时实现了一个基于SAT求解器的ALC有界拟合工具。


<details>
  <summary>Details</summary>
Motivation: 有界拟合作为一种从正负数据样本中学习逻辑公式的通用范式，近期受到广泛关注。研究旨在探索其在描述逻辑ALC及其片段中的应用特性与计算复杂度。

Method: 通过理论分析证明ALC片段有界拟合问题的NP完全性，并构建基于SAT求解器的实现方案。采用优化策略并与现有概念学习工具进行对比实验。

Result: 所有研究的ALC片段即使在单正例和单负例情况下，尺寸受限的拟合问题均为NP完全。实验表明该方法具有Valiant PAC学习框架下的概率保证。

Conclusion: 有界拟合为ALC概念学习提供了理论保证，而传统算法缺乏此类性质。基于SAT的实现展现了可行性，优化后性能可比肩现有工具。

Abstract: Bounded fitting is a general paradigm for learning logical formulas from
positive and negative data examples, that has received considerable interest
recently. We investigate bounded fitting for the description logic ALC and its
syntactic fragments. We show that the underlying size-restricted fitting
problem is NP-complete for all studied fragments, even in the special case of a
single positive and a single negative example. By design, bounded fitting comes
with probabilistic guarantees in Valiant's PAC learning framework. In contrast,
we show that other classes of algorithms for learning ALC concepts do not
provide such guarantees. Finally, we present an implementation of bounded
fitting in ALC and its fragments based on a SAT solver. We discuss
optimizations and compare our implementation to other concept learning tools.

</details>


### [145] [Towards a rigorous evaluation of RAG systems: the challenge of due diligence](https://arxiv.org/abs/2507.21753)
*Grégoire Martinon,Alexandra Lorenzo de Brionne,Jérôme Bohard,Antoine Lojou,Damien Hervault,Nicolas J-B. Brunel*

Main category: cs.AI

TL;DR: 本文评估了检索增强生成（RAG）系统在投资尽调中的应用可靠性，提出结合人工标注与LLM-Judge标注的评估协议，并采用预测驱动推断（PPI）方法提升测量精度，旨在增强工业场景中RAG系统评估的可靠性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG架构在医疗、金融等高危领域展现出潜力，但其生成内容的可靠性问题（如幻觉现象）仍制约着关键场景的应用。本研究针对投资基金的尽调场景，探索系统化评估方案以解决这一挑战。

Method: 提出混合评估协议：1）人工标注与LLM-Judge标注并行识别幻觉、离题、引用失败等故障类型；2）基于预测驱动推断（PPI）方法实现具有统计保证的性能测量；3）构建完整数据集支持后续分析。

Result: 开发出可量化RAG系统故障类型的评估框架，通过PPI方法获得统计显著的性能指标，并公开数据集为工业级应用提供基准测试资源。

Conclusion: 该研究为RAG系统在工业场景中的可靠性评估建立了标准化协议，其方法论与数据集对提升生成式AI在高风险领域的应用安全性具有重要价值。

Abstract: The rise of generative AI, has driven significant advancements in high-risk
sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG)
architecture, combining language models (LLMs) with search engines, is
particularly notable for its ability to generate responses from document
corpora. Despite its potential, the reliability of RAG systems in critical
contexts remains a concern, with issues such as hallucinations persisting. This
study evaluates a RAG system used in due diligence for an investment fund. We
propose a robust evaluation protocol combining human annotations and LLM-Judge
annotations to identify system failures, like hallucinations, off-topic, failed
citations, and abstentions. Inspired by the Prediction Powered Inference (PPI)
method, we achieve precise performance measurements with statistical
guarantees. We provide a comprehensive dataset for further analysis. Our
contributions aim to enhance the reliability and scalability of RAG systems
evaluation protocols in industrial applications.

</details>


### [146] [Hybrid Causal Identification and Causal Mechanism Clustering](https://arxiv.org/abs/2507.21792)
*Saixiong Liu,Yuhua Qian,Jue Li,Honghong Cheng,Feijiang Li*

Main category: cs.AI

TL;DR: 本文提出混合条件变分因果推断模型(MCVCI)及其聚类扩展MCVCC，用于识别异质因果关系，通过结合高斯混合模型与神经网络的拟合优势，利用概率界限似然作为因果判定标准，在模拟和真实数据上验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有二元因果方法多基于单一因果机制建模，而现实观测数据往往来自具有异质因果关系的不同环境，需开发能识别异质性的新方法。

Method: 基于混合加性噪声模型(HANM)的可识别性，结合高斯混合模型与神经网络的拟合能力，利用混合条件变分自编码器的概率界限似然作为因果判定标准，并进一步提出MCVCC方法对因果异质性进行聚类建模。

Result: 在多个模拟和真实数据集上的实验表明，所提方法相比现有最优方法具有全面最佳性能。

Conclusion: MCVCI和MCVCC能有效推断异质因果关系并揭示因果机制表达，为复杂环境下的因果发现提供了新工具。

Abstract: Bivariate causal direction identification is a fundamental and vital problem
in the causal inference field. Among binary causal methods, most methods based
on additive noise only use one single causal mechanism to construct a causal
model. In the real world, observations are always collected in different
environments with heterogeneous causal relationships. Therefore, on observation
data, this paper proposes a Mixture Conditional Variational Causal Inference
model (MCVCI) to infer heterogeneous causality. Specifically, according to the
identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the
superior fitting capabilities of the Gaussian mixture model and the neural
network and elegantly uses the likelihoods obtained from the probabilistic
bounds of the mixture conditional variational auto-encoder as causal decision
criteria. Moreover, we model the casual heterogeneity into cluster numbers and
propose the Mixture Conditional Variational Causal Clustering (MCVCC) method,
which can reveal causal mechanism expression. Compared with state-of-the-art
methods, the comprehensive best performance demonstrates the effectiveness of
the methods proposed in this paper on several simulated and real data.

</details>


### [147] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: 本文提出MixGRPO框架，通过结合SDE和ODE的混合采样策略优化图像生成中的人类偏好对齐，显著提升效率并降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法如FlowGRPO因需对所有去噪步骤进行采样和优化而效率低下，MixGRPO旨在通过混合采样策略解决这一问题。

Method: MixGRPO引入滑动窗口机制，窗口内使用SDE采样和GRPO优化，窗口外使用ODE采样，减少优化开销并支持高阶求解器。还提出更快变体MixGRPO-Flash。

Result: MixGRPO在人类偏好对齐多个维度表现优异，训练时间比DanceGRPO降低近50%，MixGRPO-Flash进一步减少71%训练时间。

Conclusion: MixGRPO通过混合采样策略显著提升效率和性能，为图像生成的人类偏好对齐提供了高效解决方案。

Abstract: Although GRPO substantially enhances flow matching models in human preference
alignment of image generation, methods such as FlowGRPO still exhibit
inefficiency due to the necessity of sampling and optimizing over all denoising
steps specified by the Markov Decision Process (MDP). In this paper, we propose
$\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed
sampling strategies through the integration of stochastic differential
equations (SDE) and ordinary differential equations (ODE). This streamlines the
optimization process within the MDP to improve efficiency and boost
performance. Specifically, MixGRPO introduces a sliding window mechanism, using
SDE sampling and GRPO-guided optimization only within the window, while
applying ODE sampling outside. This design confines sampling randomness to the
time-steps within the window, thereby reducing the optimization overhead, and
allowing for more focused gradient updates to accelerate convergence.
Additionally, as time-steps beyond the sliding window are not involved in
optimization, higher-order solvers are supported for sampling. So we present a
faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves
training efficiency while achieving comparable performance. MixGRPO exhibits
substantial gains across multiple dimensions of human preference alignment,
outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50%
lower training time. Notably, MixGRPO-Flash further reduces training time by
71%. Codes and models are available at
$\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>


### [148] [An Agentic AI for a New Paradigm in Business Process Development](https://arxiv.org/abs/2507.21823)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff*

Main category: cs.AI

TL;DR: 本文提出了一种基于Agentic AI的新型业务流程设计与开发方法，通过目标导向的智能体协作实现模块化和智能化的工业自动化。


<details>
  <summary>Details</summary>
Motivation: 传统基于任务的业务流程设计方法在动态工业环境中缺乏灵活性和上下文感知能力，需要更智能的解决方案。

Method: 采用基于智能体的方法，以业务目标和对象为中心组织流程，通过智能体协作实现单智能体无法完成的目标。

Result: 该方法实现了更具模块化和智能化的业务流程开发，能够适应动态工业环境的灵活自动化需求。

Conclusion: 基于智能体的目标导向方法为工业自动化提供了更高效、灵活的解决方案，是业务流程设计的重要演进方向。

Abstract: Artificial Intelligence agents represent the next major revolution in the
continuous technological evolution of industrial automation. In this paper, we
introduce a new approach for business process design and development that
leverages the capabilities of Agentic AI. Departing from the traditional
task-based approach to business process design, we propose an agent-based
method, where agents contribute to the achievement of business goals,
identified by a set of business objects. When a single agent cannot fulfill a
goal, we have a merge goal that can be achieved through the collaboration of
multiple agents. The proposed model leads to a more modular and intelligent
business process development by organizing it around goals, objects, and
agents. As a result, this approach enables flexible and context-aware
automation in dynamic industrial environments.

</details>


### [149] [DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework](https://arxiv.org/abs/2507.21830)
*Kuiye Ding,Fanda Fan,Yao Wang,Ruijie jian,Xiaorui Wang,Luqi Gong,Yishan Jiang,Chunjie Luo an Jianfeng Zhan*

Main category: cs.AI

TL;DR: 本文提出DualSG框架，将大语言模型(LLM)作为语义引导模块而非独立预测器，通过双流架构结合数值预测与语义指导，显著提升多元时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法或直接将LLM用作端到端预测器导致数值精度损失，或难以实现文本与时间序列的隐式潜在空间对齐，需探索更有效的结合方式。

Method: 1) 设计双流框架DualSG，LLM作为语义引导模块优化传统预测结果\n2) 提出可解释的时间序列描述文本(Time Series Caption)作为显式提示\n3) 开发基于描述的融合模块显式建模变量间关系并降低噪声

Result: 在多个真实世界数据集上，DualSG持续超越15种前沿基线方法，验证显式结合数值预测与语义引导的有效性。

Conclusion: 通过将LLM定位为语义指导者而非替代者，并建立显式的模态交互机制，DualSG为时间序列预测提供了精度与可解释性兼备的新范式。

Abstract: Multivariate Time Series Forecasting plays a key role in many applications.
Recent works have explored using Large Language Models for MTSF to take
advantage of their reasoning abilities. However, many methods treat LLMs as
end-to-end forecasters, which often leads to a loss of numerical precision and
forces LLMs to handle patterns beyond their intended design. Alternatively,
methods that attempt to align textual and time series modalities within latent
space frequently encounter alignment difficulty. In this paper, we propose to
treat LLMs not as standalone forecasters, but as semantic guidance modules
within a dual-stream framework. We propose DualSG, a dual-stream framework that
provides explicit semantic guidance, where LLMs act as Semantic Guides to
refine rather than replace traditional predictions. As part of DualSG, we
introduce Time Series Caption, an explicit prompt format that summarizes trend
patterns in natural language and provides interpretable context for LLMs,
rather than relying on implicit alignment between text and time series in the
latent space. We also design a caption-guided fusion module that explicitly
models inter-variable relationships while reducing noise and computation.
Experiments on real-world datasets from diverse domains show that DualSG
consistently outperforms 15 state-of-the-art baselines, demonstrating the value
of explicitly combining numerical forecasting with semantic guidance.

</details>


### [150] [Probabilistic Active Goal Recognition](https://arxiv.org/abs/2507.21846)
*Chenyuan Zhang,Cristian Rojas Cardenas,Hamid Rezatofighi,Mor Vered,Buser Say*

Main category: cs.AI

TL;DR: 本文提出了一种主动目标识别（AGR）的概率框架，结合联合信念更新机制与蒙特卡洛树搜索（MCTS），在多智能体环境中高效推断隐藏目标，无需领域特定知识。实验表明该方法显著优于被动目标识别。


<details>
  <summary>Details</summary>
Motivation: 传统目标识别将观察者视为被动推理者，而主动目标识别（AGR）旨在通过策略性信息收集减少不确定性，推动多智能体系统向更具交互性和适应性的方向发展。

Method: 采用概率框架，整合联合信念更新机制与蒙特卡洛树搜索（MCTS）算法，支持观察者高效规划并推断行动者的隐藏目标，且不依赖领域特定知识。

Result: 在网格环境中验证表明：联合信念更新显著优于被动目标识别；领域无关的MCTS性能与领域特定的贪婪基线相当。

Conclusion: 该研究为多智能体目标推断提供了实用且鲁棒的框架，推动了交互式自适应系统的进步。

Abstract: In multi-agent environments, effective interaction hinges on understanding
the beliefs and intentions of other agents. While prior work on goal
recognition has largely treated the observer as a passive reasoner, Active Goal
Recognition (AGR) focuses on strategically gathering information to reduce
uncertainty. We adopt a probabilistic framework for Active Goal Recognition and
propose an integrated solution that combines a joint belief update mechanism
with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan
efficiently and infer the actor's hidden goal without requiring domain-specific
knowledge. Through comprehensive empirical evaluation in a grid-based domain,
we show that our joint belief update significantly outperforms passive goal
recognition, and that our domain-independent MCTS performs comparably to our
strong domain-specific greedy baseline. These results establish our solution as
a practical and robust framework for goal inference, advancing the field toward
more interactive and adaptive multi-agent systems.

</details>


### [151] [EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity](https://arxiv.org/abs/2507.21848)
*Xingjian Zhang,Siwei Wen,Wenjun Wu,Lei Huang*

Main category: cs.AI

TL;DR: 本文提出EDGE-GRPO算法，通过熵驱动优势和引导错误校正解决GRPO算法中的优势崩溃问题，在多个推理基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法因依赖稀疏奖励规则导致组内奖励相同，引发优势崩溃问题。传统方法通过增强模型反思或引入内部反馈来应对，但存在局限性。

Method: 通过分析模型反思的局限性及细粒度样本层面的策略熵，提出EDGE-GRPO算法，结合熵驱动优势（Entropy-Driven Advantage）和引导错误校正（Guided Error Correction）。

Result: 在多个主流推理基准测试上的实验表明，EDGE-GRPO能有效缓解优势崩溃问题，并展现出优越性能。

Conclusion: EDGE-GRPO通过熵优化和错误校正机制显著提升了GRPO算法的稳定性与推理能力，为强化学习在语言模型中的应用提供了新思路。

Abstract: Large Language Models (LLMs) have made remarkable progress in enhancing
step-by-step reasoning through reinforcement learning. However, the Group
Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward
rules, often encounters the issue of identical rewards within groups, leading
to the advantage collapse problem. Existing works typically address this
challenge from two perspectives: enforcing model reflection to enhance response
diversity, and introducing internal feedback to augment the training signal
(advantage). In this work, we begin by analyzing the limitations of model
reflection and investigating the policy entropy of responses at the
fine-grained sample level. Based on our experimental findings, we propose the
EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage
and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the
problem of advantage collapse. Extensive experiments on several main reasoning
benchmarks demonstrate the effectiveness and superiority of our approach. It is
available at https://github.com/ZhangXJ199/EDGE-GRPO.

</details>


### [152] [MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors](https://arxiv.org/abs/2507.21872)
*Shouyi Lu,Zihan Lin,Chao Lu,Huanran Wang,Guirong Zhuo,Lianqing Zheng*

Main category: cs.AI

TL;DR: 本文提出MultiEditor框架，通过双分支潜在扩散模型联合编辑图像与LiDAR点云，利用3D高斯泼溅(3DGS)作为先验，实现跨模态高保真重建，显著提升自动驾驶系统对稀有车辆类别的检测精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖多模态感知数据，但现实数据的长尾分布导致罕见车辆类别泛化能力不足。为解决这一关键安全问题，需开发能同时编辑图像与点云数据的方法。

Method: 1) 采用3DGS作为目标对象的结构与外观先验；2) 设计多级外观控制机制（像素级粘贴、语义级引导、多分支优化）；3) 提出深度引导的可变形跨模态条件模块，通过3DGS渲染深度增强模态间一致性。

Result: 实验表明MultiEditor在视觉/几何保真度、编辑可控性和跨模态一致性上表现优异，生成的稀有类别数据使感知模型对少数类的检测准确率显著提升。

Conclusion: 该框架为自动驾驶系统提供了有效的跨模态数据增强方案，通过3DGS先验与多级控制机制，成功解决了长尾分布下的关键类别感知难题。

Abstract: Autonomous driving systems rely heavily on multimodal perception data to
understand complex environments. However, the long-tailed distribution of
real-world data hinders generalization, especially for rare but safety-critical
vehicle categories. To address this challenge, we propose MultiEditor, a
dual-branch latent diffusion framework designed to edit images and LiDAR point
clouds in driving scenarios jointly. At the core of our approach is introducing
3D Gaussian Splatting (3DGS) as a structural and appearance prior for target
objects. Leveraging this prior, we design a multi-level appearance control
mechanism--comprising pixel-level pasting, semantic-level guidance, and
multi-branch refinement--to achieve high-fidelity reconstruction across
modalities. We further propose a depth-guided deformable cross-modality
condition module that adaptively enables mutual guidance between modalities
using 3DGS-rendered depth, significantly enhancing cross-modality consistency.
Extensive experiments demonstrate that MultiEditor achieves superior
performance in visual and geometric fidelity, editing controllability, and
cross-modality consistency. Furthermore, generating rare-category vehicle data
with MultiEditor substantially enhances the detection accuracy of perception
models on underrepresented classes.

</details>


### [153] [A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data](https://arxiv.org/abs/2507.21873)
*Raffaele Pojer,Andrea Passerini,Kim G. Larsen,Manfred Jaeger*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号框架，将图神经网络（GNNs）与关系贝叶斯网络（RBNs）相结合，整合了GNNs的学习能力和RBNs的推理能力。通过两种实现方式，该框架在节点分类和环境规划等任务中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图结构数据的预测任务中表现优异，但缺乏符号领域知识和通用推理能力。关系贝叶斯网络（RBNs）支持生成式概率建模和符号推理，但学习能力有限。本文旨在结合两者的优势。

Method: 提出了两种集成方法：一种将GNN直接编译为RBN语言，另一种将GNN作为外部组件保留。两种方法均保持了GNN的语义和计算特性，同时完全符合RBN建模范式。还提出了一种最大后验（MAP）推理方法。

Result: 在节点分类任务中，该框架通过显式建模同质性和异质性标签模式显著提高了准确性。在环境规划的多目标网络优化问题中，MAP推理支持复杂决策。两项应用均提供了公开基准数据集。

Conclusion: 这项工作提出了一种强大且一致的神经符号方法，弥合了图数据的学习与推理，为多样化任务带来了新颖应用和性能提升。

Abstract: Graph neural networks (GNNs) excel at predictive tasks on graph-structured
data but often lack the ability to incorporate symbolic domain knowledge and
perform general reasoning. Relational Bayesian Networks (RBNs), in contrast,
enable fully generative probabilistic modeling over graph-like structures and
support rich symbolic knowledge and probabilistic inference. This paper
presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,
combining the learning strength of GNNs with the flexible reasoning
capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs
directly into the native RBN language, while the other maintains the GNN as an
external component. Both approaches preserve the semantics and computational
properties of GNNs while fully aligning with the RBN modeling paradigm. We also
propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic
models.
  To demonstrate the framework's versatility, we apply it to two distinct
problems. First, we transform a GNN for node classification into a collective
classification model that explicitly models homo- and heterophilic label
patterns, substantially improving accuracy. Second, we introduce a
multi-objective network optimization problem in environmental planning, where
MAP inference supports complex decision-making. Both applications include new
publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph
data, bridging learning and reasoning in ways that enable novel applications
and improved performance across diverse tasks.

</details>


### [154] [Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis](https://arxiv.org/abs/2507.21875)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级预训练嵌入模型Tiny-BioMoE，用于生物信号分析，以提升自动疼痛评估系统的性能。该模型在多种生理信号上表现出色，代码和权重已开源。


<details>
  <summary>Details</summary>
Motivation: 疼痛是一种复杂且普遍的症状，准确的评估对患者和医疗系统至关重要。自动疼痛评估系统能实现持续监测，支持临床决策，并减少患者痛苦和功能恶化的风险。

Method: 研究提出了Tiny-BioMoE模型，这是一个轻量级预训练嵌入模型，用于生物信号分析。模型基于440万生物信号图像表示训练，仅包含730万参数，能有效提取高质量嵌入用于下游任务。

Result: 实验表明，Tiny-BioMoE在皮肤电活动、血容量脉冲、呼吸信号、外周血氧饱和度等多种生理信号上均表现出色，适用于自动疼痛识别任务。

Conclusion: Tiny-BioMoE是一种高效的生物信号分析工具，能显著提升自动疼痛评估系统的性能。模型代码和权重已公开，可供进一步研究和应用。

Abstract: Pain is a complex and pervasive condition that affects a significant portion
of the population. Accurate and consistent assessment is essential for
individuals suffering from pain, as well as for developing effective management
strategies in a healthcare system. Automatic pain assessment systems enable
continuous monitoring, support clinical decision-making, and help minimize
patient distress while mitigating the risk of functional deterioration.
Leveraging physiological signals offers objective and precise insights into a
person's state, and their integration in a multimodal framework can further
enhance system performance. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed approach introduces \textit{Tiny-BioMoE}, a lightweight pretrained
embedding model for biosignal analysis. Trained on $4.4$ million biosignal
image representations and consisting of only $7.3$ million parameters, it
serves as an effective tool for extracting high-quality embeddings for
downstream tasks. Extensive experiments involving electrodermal activity, blood
volume pulse, respiratory signals, peripheral oxygen saturation, and their
combinations highlight the model's effectiveness across diverse modalities in
automatic pain recognition tasks. \textit{\textcolor{blue}{The model's
architecture (code) and weights are available at
https://github.com/GkikasStefanos/Tiny-BioMoE.

</details>


### [155] [Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image](https://arxiv.org/abs/2507.21881)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本文提出了一种基于皮肤电活动信号的多表征融合方法，用于自动疼痛评估，实验证明其效果优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 疼痛评估对患者管理和临床决策至关重要，自动评估系统能提供客观、连续的监测，减少患者痛苦并防止功能退化。

Method: 研究采用皮肤电活动信号作为输入模态，生成多种信号表征并整合为单一多表征图，结合多种处理和滤波技术进行实验验证。

Result: 该方法在多种表征组合下表现稳定，效果与传统融合方法相当甚至更优，成为整合不同信号表征或模态的可靠替代方案。

Conclusion: 提出的多表征融合管道为疼痛评估提供了新思路，其鲁棒性和有效性通过实验得到验证，适用于下一代疼痛监测系统。

Abstract: Pain is a multifaceted phenomenon that affects a substantial portion of the
population. Reliable and consistent evaluation benefits those experiencing pain
and underpins the development of effective and advanced management strategies.
Automatic pain-assessment systems deliver continuous monitoring, inform
clinical decision-making, and aim to reduce distress while preventing
functional decline. By incorporating physiological signals, these systems
provide objective, accurate insights into an individual's condition. This study
has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for
Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline
that leverages electrodermal activity signals as input modality. Multiple
representations of the signal are created and visualized as waveforms, and they
are jointly visualized within a single multi-representation diagram. Extensive
experiments incorporating various processing and filtering techniques, along
with multiple representation combinations, demonstrate the effectiveness of the
proposed approach. It consistently yields comparable, and in several cases
superior, results to traditional fusion methods, establishing it as a robust
alternative for integrating different signal representations or modalities.

</details>


### [156] [The Impact of Foundational Models on Patient-Centric e-Health Systems](https://arxiv.org/abs/2507.21882)
*Elmira Onagh,Alireza Davoodi,Maleknaz Nayebi*

Main category: cs.AI

TL;DR: 研究评估了116个以患者为中心的医疗应用中AI的成熟度，发现86.21\%处于早期阶段，仅13.79\%展现高级整合。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗技术中的深入应用，评估其在患者中心应用中的成熟度对信任度、透明度及实际影响至关重要。

Method: 利用大语言模型(LLMs)提取关键功能特征，并按Gartner AI成熟度模型进行分类。

Result: 86.21\%的应用处于AI整合初级阶段，仅13.79\%实现高级整合。

Conclusion: 当前医疗应用中AI整合整体成熟度较低，需进一步推动技术向高阶发展。

Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.

</details>


### [157] [Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline](https://arxiv.org/abs/2507.21886)
*Stefanos Gkikas,Ioannis Kyprakis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 本研究提出了一种基于呼吸信号的高效跨注意力Transformer模型，结合多窗口策略用于疼痛评估，实验证明呼吸信号作为生理模态具有重要价值，且优化后的小模型性能可超越大模型。


<details>
  <summary>Details</summary>
Motivation: 疼痛是影响广泛人群的复杂症状，准确评估对患者管理和临床决策至关重要。自动疼痛评估系统能实现持续监测，减轻痛苦并预防功能退化。

Method: 采用呼吸信号作为输入，构建高效跨注意力Transformer模型，结合多窗口策略捕捉短期/长期特征与全局特性，提升模型表征能力。

Result: 实验表明呼吸信号是有效的疼痛评估生理模态；优化后的小模型性能优于大模型；多窗口策略成功提取多层次特征。

Conclusion: 呼吸信号与高效小模型的组合为疼痛评估提供了新思路，多窗口策略有效增强模型表现，该方法具有临床应用潜力。

Abstract: Pain is a complex condition affecting a large portion of the population.
Accurate and consistent evaluation is essential for individuals experiencing
pain, and it supports the development of effective and advanced management
strategies. Automatic pain assessment systems provide continuous monitoring and
support clinical decision-making, aiming to reduce distress and prevent
functional decline. This study has been submitted to the \textit{Second
Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The
proposed method introduces a pipeline that leverages respiration as the input
signal and incorporates a highly efficient cross-attention transformer
alongside a multi-windowing strategy. Extensive experiments demonstrate that
respiration is a valuable physiological modality for pain assessment. Moreover,
experiments revealed that compact and efficient models, when properly
optimized, can achieve strong performance, often surpassing larger
counterparts. The proposed multi-window approach effectively captures both
short-term and long-term features, as well as global characteristics, thereby
enhancing the model's representational capacity.

</details>


### [158] [LLM-based Content Classification Approach for GitHub Repositories by the README Files](https://arxiv.org/abs/2507.21899)
*Malik Uzair Mehmood,Shahid Hussain,Wen Li Wang,Muhammad Usama Malik*

Main category: cs.AI

TL;DR: 研究利用BERT、DistilBERT和RoBERTa等大语言模型(LLM)微调，开发了GitHub README文件自动分类方法，F1分数达0.98，并验证了参数高效微调技术(LoRA)的经济性。


<details>
  <summary>Details</summary>
Motivation: GitHub仓库README文件的信息完整性显著影响项目采用率，但现有文件常不符合规范。研究旨在通过LLM自动分类提升README质量，从而增强仓库使用潜力。

Method: 基于4226个README章节的黄金数据集，对BERT、DistilBERT和RoBERTa进行微调，并采用LoRA等参数高效微调技术进行比较实验。

Result: 微调后的LLM分类器F1分数达0.98，优于现有方法；LoRA技术在保持性能的同时显著降低计算成本。

Conclusion: 该研究证明了LLM在自动化分类README内容上的有效性，为提升GitHub仓库可发现性和使用率提供了工具支持。

Abstract: GitHub is the world's most popular platform for storing, sharing, and
managing code. Every GitHub repository has a README file associated with it.
The README files should contain project-related information as per the
recommendations of GitHub to support the usage and improvement of repositories.
However, GitHub repository owners sometimes neglected these recommendations.
This prevents a GitHub repository from reaching its full potential. This
research posits that the comprehensiveness of a GitHub repository's README file
significantly influences its adoption and utilization, with a lack of detail
potentially hindering its full potential for widespread engagement and impact
within the research community. Large Language Models (LLMs) have shown great
performance in many text-based tasks including text classification, text
generation, text summarization and text translation. In this study, an approach
is developed to fine-tune LLMs for automatically classifying different sections
of GitHub README files. Three encoder-only LLMs are utilized, including BERT,
DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a
gold-standard dataset consisting of 4226 README file sections. This approach
outperforms current state-of-the-art methods and has achieved an overall F1
score of 0.98. Moreover, we have also investigated the use of
Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation
(LoRA) and shown an economical alternative to full fine-tuning without
compromising much performance. The results demonstrate the potential of using
LLMs in designing an automatic classifier for categorizing the content of
GitHub README files. Consequently, this study contributes to the development of
automated tools for GitHub repositories to improve their identifications and
potential usages.

</details>


### [159] [Libra: Large Chinese-based Safeguard for AI Content](https://arxiv.org/abs/2507.21929)
*Ziyang Chen,Huimu Yu,Xing Wu,Dongqin Liu,Songlin Hu*

Main category: cs.AI

TL;DR: 本文提出Libra-Guard安全防护系统及配套评测基准Libra-Test，通过两阶段训练显著提升中文大模型安全性，实验表明其性能超越主流开源模型并接近闭源顶级模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本理解与生成方面表现优异，但在高风险应用中存在重大安全与伦理隐患，需开发针对中文内容的安全保障体系。

Method: 采用两阶段课程训练框架：先在合成样本上进行防护预训练，再用高质量真实数据微调；同时构建首个中文安全评测基准Libra-Test，涵盖7类危害场景与5700+专家标注样本。

Result: Libra-Guard达到86.79%准确率，优于Qwen2.5-14B-Instruct（74.33%）和ShieldLM-Qwen-14B-Chat（65.69%），接近Claude-3.5-Sonnet和GPT-4o等闭源模型。

Conclusion: 该研究为中文大模型安全治理建立系统框架，是开发更安全可靠中文AI系统的重要探索。

Abstract: Large language models (LLMs) excel in text understanding and generation but
raise significant safety and ethical concerns in high-stakes applications. To
mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system
designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage
curriculum training pipeline, Libra-Guard enhances data efficiency by employing
guard pretraining on synthetic samples, followed by fine-tuning on
high-quality, real-world data, thereby significantly reducing reliance on
manual annotations. To enable rigorous safety evaluations, we also introduce
Libra-Test, the first benchmark specifically designed to evaluate the
effectiveness of safeguard systems for Chinese content. It covers seven
critical harm scenarios and includes over 5,700 samples annotated by domain
experts. Experiments show that Libra-Guard achieves 86.79% accuracy,
outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat
(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.
These contributions establish a robust framework for advancing the safety
governance of Chinese LLMs and represent a tentative step toward developing
safer, more reliable Chinese AI systems.

</details>


### [160] [Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data & Activities](https://arxiv.org/abs/2507.21964)
*Sourish Gunesh Dhekane,Thomas Ploetz*

Main category: cs.AI

TL;DR: 本文提出了一种不依赖大型语言模型（LLM）的零样本人类活动识别（HAR）方法，通过自然语言嵌入实现分类，解决了现有方法中的隐私、依赖性和一致性等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的零样本HAR方法存在隐私侵犯、依赖外部服务及版本变更导致的预测不一致等问题，亟需不依赖LLM提示的替代方案。

Method: 该方法利用自然语言建模传感器数据和活动，通过其嵌入实现零样本分类，避免了直接调用LLM进行预测的需求。

Result: 在六个数据集上的详细案例研究表明，语言建模能有效增强零样本识别能力，验证了方法的可行性。

Conclusion: 研究证明了自然语言嵌入在零样本HAR中的潜力，为不依赖LLM的智能家居活动识别提供了新思路。

Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical
direction in smart home research -- considering its impact on making HAR
systems work across smart homes having diverse sensing modalities, layouts, and
activities of interest. The state-of-the-art solutions along this direction are
based on generating natural language descriptions of the sensor data and
feeding it via a carefully crafted prompt to the LLM to perform classification.
Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry
several risks, including privacy invasion, reliance on an external service, and
inconsistent predictions due to version changes, making a case for alternative
zero-shot HAR methods that do not require prompting the LLMs. In this paper, we
propose one such solution that models sensor data and activities using natural
language, leveraging its embeddings to perform zero-shot classification and
thereby bypassing the need to prompt the LLMs for activity predictions. The
impact of our work lies in presenting a detailed case study on six datasets,
highlighting how language modeling can bolster HAR systems in zero-shot
recognition.

</details>


### [161] [Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks](https://arxiv.org/abs/2507.21974)
*Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Yibin Kang,Haozhe Zhang,Merouane Debbah,Fadhel Ayed*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级框架，利用大语言模型（LLMs）进行移动网络中的根因分析（RCA），并通过两阶段训练方法提升模型的准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 移动网络中的根因分析（RCA）因需可解释性、领域专业知识和因果推理而具有挑战性，现有开源推理LLMs在此类问题上表现不佳，亟需领域适配。

Method: 提出两阶段训练方法：先通过监督微调，再结合强化学习，以整合领域知识并生成结构化、多步骤的诊断解释，提升模型的可解释性和有效性。

Result: 实验表明，该方法在多种LLM规模上均显著优于现有推理和非推理模型，且在随机测试变体上表现出强泛化能力。

Conclusion: 领域适配和推理增强的LLMs在网络运维和管理中具有实际且可解释的RCA应用潜力。

Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.

</details>


### [162] [The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain](https://arxiv.org/abs/2507.21976)
*Tanvir Ahmed Khan,Aranya Saha,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.AI

TL;DR: 本文提出了一种针对医疗领域多模态大语言模型(MLLM)的高效压缩方法，通过结构剪枝和激活感知量化技术，使70亿参数模型能在4GB显存下运行，显存占用降低70%且性能提升4%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医疗领域潜力巨大，但高昂计算成本需要高效压缩技术。本文旨在评估剪枝和量化对医疗定制化LLAVA模型的影响。

Method: 提出新型剪枝层选择方法，分析不同量化技术，并在剪枝-微调-量化流程中评估性能权衡。特别设计了prune-SFT-quantize压缩管线。

Result: 压缩后7B参数MLLM仅需4GB显存，显存使用减少70%，在相同压缩率下比传统方法性能提升4%。

Conclusion: 该方法显著降低了医疗MLLM的硬件需求，在保持模型性能的同时实现了高效压缩，为医疗领域部署提供了可行方案。

Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the
medical domain, but their computational costs necessitate efficient compression
techniques. This paper evaluates the impact of structural pruning and
activation-aware quantization on a fine-tuned LLAVA model for medical
applications. We propose a novel layer selection method for pruning, analyze
different quantization techniques, and assess the performance trade-offs in a
prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B
parameters to run within 4 GB of VRAM, reducing memory usage by 70% while
achieving 4% higher model performance compared to traditional pruning and
quantization techniques in the same compression ratio.

</details>


### [163] [PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences](https://arxiv.org/abs/2507.22009)
*Bahar İlgen,Akshat Dubey,Georges Hattab*

Main category: cs.AI

TL;DR: 本文提出PHAX框架，通过结构化论证生成面向公众健康领域AI系统的可解释性方案，结合可废止推理与自然语言技术，实现用户定制化的解释输出。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI（XAI）方法在公共卫生领域缺乏对多元利益相关者（如临床医生、政策制定者、公众）的适应性，需要建立兼具社会问责与情境化解释能力的框架。

Method: 开发多层架构PHAX框架，整合：1) 可废止推理构建论证链 2) 自适应自然语言生成技术 3) 用户建模，实现基于受众特征的动态解释生成。

Result: 案例验证显示PHAX能有效支持：医学术语简化、医患沟通优化、政策建议论证，通过论证链个性化提升不同专业背景用户的理解度与信任度。

Conclusion: PHAX将形式化推理与沟通需求结合，推动了公共卫生领域透明化、以人为中心的AI系统发展，其论证方法为跨用户类型的交互式解释提供新范式。

Abstract: Ensuring transparency and trust in AI-driven public health and biomedical
sciences systems requires more than accurate predictions-it demands
explanations that are clear, contextual, and socially accountable. While
explainable AI (XAI) has advanced in areas like feature attribution and model
interpretability, most methods still lack the structure and adaptability needed
for diverse health stakeholders, including clinicians, policymakers, and the
general public. We introduce PHAX-a Public Health Argumentation and
eXplainability framework-that leverages structured argumentation to generate
human-centered explanations for AI outputs. PHAX is a multi-layer architecture
combining defeasible reasoning, adaptive natural language techniques, and user
modeling to produce context-aware, audience-specific justifications. More
specifically, we show how argumentation enhances explainability by supporting
AI-driven decision-making, justifying recommendations, and enabling interactive
dialogues across user types. We demonstrate the applicability of PHAX through
use cases such as medical term simplification, patient-clinician communication,
and policy justification. In particular, we show how simplification decisions
can be modeled as argument chains and personalized based on user
expertise-enhancing both interpretability and trust. By aligning formal
reasoning methods with communicative demands, PHAX contributes to a broader
vision of transparent, human-centered AI in public health.

</details>


### [164] [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
*Shuquan Lian,Yuhang Wu,Jia Ma,Zihan Song,Bingqi Chen,Xiawu Zheng,Hui Li*

Main category: cs.AI

TL;DR: Error processing this paper.


<details>
  <summary>Details</summary>
Motivation: Error processing this paper.

Method: Error processing this paper.

Result: Error processing this paper.

Conclusion: Error processing this paper.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven
significant advances in Graphical User Interface (GUI) agent capabilities.
Nevertheless, existing GUI agent training and inference techniques still suffer
from a dilemma for reasoning designs, ineffective reward, and visual noise. To
address these issues, we introduce UI-AGILE, a comprehensive framework
enhancing GUI agents at both the training and inference stages. For training,
we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:
1) a Continuous Reward function to incentivize high-precision grounding; 2) a
"Simple Thinking" reward to balance planning with speed and grounding accuracy;
and 3) a Cropping-based Resampling strategy to mitigate the sparse reward
problem and improve learning on complex tasks. For inference, we present
Decomposed Grounding with Selection, a novel method that dramatically improves
grounding accuracy on high-resolution displays by breaking the image into
smaller, manageable parts. Experiments show that UI-AGILE achieves the
state-of-the-art performance on two benchmarks ScreenSpot-Pro and
ScreenSpot-v2. For instance, using both our proposed training and inference
enhancement methods brings 23% grounding accuracy improvement over the best
baseline on ScreenSpot-Pro.

</details>


### [165] [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Zhiwei Liu,Jianguo Zhang,Haolin Chen,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: 本文介绍了UserBench基准测试，用于评估大语言模型(LLM)代理在模糊目标场景下的用户协作能力，发现现有模型在用户意图对齐方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型(LLM)的代理在推理和工具使用方面取得显著进展，但其在目标模糊、动态变化或间接表达场景下与用户主动协作的能力尚未充分探索。

Method: 研究者开发了UserBench基准测试，通过模拟目标不明确的用户进行多轮交互，要求代理主动澄清意图并基于工具做出决策，以此评估模型的协作能力。

Result: 评估显示：模型输出完全符合用户意图的情况平均仅20%，即使最先进模型通过主动交互发现的用户偏好也不足30%，任务完成度与用户对齐存在显著差距。

Conclusion: 研究表明构建真正协作型代理仍面临挑战，UserBench为衡量和提升这一关键能力提供了交互式评估环境。

Abstract: Large Language Models (LLMs)-based agents have made impressive progress in
reasoning and tool use, enabling them to solve complex tasks. However, their
ability to proactively collaborate with users, especially when goals are vague,
evolving, or indirectly expressed, remains underexplored. To address this gap,
we introduce UserBench, a user-centric benchmark designed to evaluate agents in
multi-turn, preference-driven interactions. UserBench features simulated users
who start with underspecified goals and reveal preferences incrementally,
requiring agents to proactively clarify intent and make grounded decisions with
tools. Our evaluation of leading open- and closed-source LLMs reveals a
significant disconnect between task completion and user alignment. For
instance, models provide answers that fully align with all user intents only
20% of the time on average, and even the most advanced models uncover fewer
than 30% of all user preferences through active interaction. These results
highlight the challenges of building agents that are not just capable task
executors, but true collaborative partners. UserBench offers an interactive
environment to measure and advance this critical capability.

</details>


### [166] [The Interspeech 2025 Speech Accessibility Project Challenge](https://arxiv.org/abs/2507.22047)
*Xiuwen Zheng,Bornali Phukon,Jonghwan Na,Ed Cutrell,Kyu Han,Mark Hasegawa-Johnson,Pan-Pan Jiang,Aadhrik Kuila,Colin Lea,Bob MacDonald,Gautam Mantena,Venkatesh Ravichandran,Leda Sari,Katrin Tomanek,Chang D. Yoo,Chris Zwilling*

Main category: cs.AI

TL;DR: 2025年Interspeech语音无障碍项目挑战赛利用400多小时残障人士语音数据，12支团队在词错误率上超越whisper-large-v2基线，最佳团队同时创下8.11\%最低WER和88.44\%最高语义得分新纪录。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别系统对残障人士的语音识别性能不足，主要因缺乏公开训练数据。

Method: 使用EvalAI平台和远程评估管道，基于500多名残障人士的400多小时转录数据，以词错误率和语义得分为评估标准。

Result: 22支有效参赛团队中，12支在WER上超越基线模型，17支在SemScore上表现更优；顶尖团队同时达到8.11\% WER和88.44\% SemScore。

Conclusion: 该挑战赛为残障语音识别设立了新基准，证明现有ASR系统在优化后能显著提升识别性能。

Abstract: While the last decade has witnessed significant advancements in Automatic
Speech Recognition (ASR) systems, performance of these systems for individuals
with speech disabilities remains inadequate, partly due to limited public
training data. To bridge this gap, the 2025 Interspeech Speech Accessibility
Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data
collected and transcribed from more than 500 individuals with diverse speech
disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline,
the SAP Challenge evaluates submissions based on Word Error Rate and Semantic
Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2
baseline in terms of WER, while 17 teams surpassed the baseline on SemScore.
Notably, the top team achieved the lowest WER of 8.11\%, and the highest
SemScore of 88.44\% at the same time, setting new benchmarks for future ASR
systems in recognizing impaired speech.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [167] [Pathwidth of 2-Layer $k$-Planar Graphs](https://arxiv.org/abs/2507.21864)
*Yuto Okada*

Main category: cs.DM

TL;DR: 本文证明了2层$k$-平面图的路径宽度上界$k+1$是紧的，通过构造路径宽度为$k+1$的实例改进了先前下界$(k+3)/2$。


<details>
  <summary>Details</summary>
Motivation: Angelini等人[GD 2020; Comput. J., 2024]证明了2层$k$-平面图的路径宽度至多为$k+1$，但未验证该上界是否紧。本研究旨在填补这一理论空白。

Method: 针对每个$k\geq 0$，显式构造了一个2层$k$-平面图实例，其顶点集$X,Y$分别置于平行线上，边为直线段且交叉数不超过$k$。

Result: 所构造的图例具有精确的路径宽度$k+1$，证明原上界不可改进，显著优于先前$(k+3)/2$的下界结果。

Conclusion: 该研究完整解决了2层$k$-平面图路径宽度的紧界问题，确立了$k+1$为最优上界，推动了图绘制与宽度参数的理论研究。

Abstract: A bipartite graph $G = (X \cup Y, E)$ is a 2-layer $k$-planar graph if it
admits a drawing on the plane such that the vertices in $X$ and $Y$ are placed
on two parallel lines respectively, edges are drawn as straight-line segments,
and every edge involves at most $k$ crossings. Angelini, Da Lozzo, F\"orster,
and Schneck [GD 2020; Comput. J., 2024] showed that every 2-layer $k$-planar
graph has pathwidth at most $k + 1$. In this paper, we show that this bound is
sharp by giving a 2-layer $k$-planar graph with pathwidth $k + 1$ for every $k
\geq 0$. This improves their lower bound of $(k+3)/2$.

</details>


### [168] [Perfect Graph Modification Problems: An Integer Programming Approach](https://arxiv.org/abs/2507.21987)
*Burak Nur Erdem,Tınaz Ekim,Zeki Caner Taşkın*

Main category: cs.DM

TL;DR: 本文提出基于整数规划和切割平面算法的精确解法，用于解决三种完美图修改问题：最小完美编辑、最小完美补全和完美三明治问题，并通过计算实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有关于图修改问题的研究主要集中在NP完全性结果和多项式时间可解案例，但针对NP难问题的精确算法较少。本文旨在填补这一空白，提出针对完美图修改问题的精确解法。

Method: 基于强完美图定理，将奇洞和奇反洞表示为线性不等式，构建整数规划模型。为解决约束条件指数级增长问题，提出依赖奇洞/奇反洞检测的切割平面算法，并引入启发式算法改进上界。

Result: 通过分析随机图中奇洞/奇反洞的预期数量提升算法效率，计算实验证明所提方法在解决完美图编辑、补全和三明治问题上的实际有效性。

Conclusion: 该研究为NP难完美图修改问题提供了首个系统性的精确解法框架，结合理论分析与启发式策略，显著提升了计算可行性，为后续相关研究奠定基础。

Abstract: Graph modification problems, which aim to find a small set of modifications
to a graph so that it satisfies a desired property, have been studied for
several special graph classes. The literature is rather rich in NP-completeness
results and polynomial time solvable cases. However, to the best of our
knowledge, only a few exact algorithms have been suggested to address NP-hard
cases. In this work, we propose exact solution methods based on integer
programming for three perfect graph modification problems: minimum perfect
editing, minimum perfect completion and the perfect sandwich problem. The
minimum perfect editing problem inquires the smallest number of edge additions
and deletions to make a graph perfect, while the completion problem allows only
edge additions. In the perfect sandwich problem, only a given subset of
non-edges can be changed to edges, and the problem asks whether a perfect graph
can be obtained in this way. The proposed methods are based on the Strong
Perfect Graph Theorem. We represent odd holes and odd antiholes as linear
inequalities, and formulate an integer programming model to solve minimum
perfect editing problem. To address the exponential number of constraints, we
propose a cutting plane algorithm which relies on finding odd holes and odd
antiholes. To enhance the practical efficiency of the cutting plane algorithm,
we address the expected number of odd holes and odd antiholes in random graphs.
In addition, we propose a heuristic algorithm to make a given graph perfect,
which is used to obtain improved upper bounds for the editing and the
completion problems. Finally, we demonstrate empirical effectiveness of the
proposed methods through computational experiments.

</details>
