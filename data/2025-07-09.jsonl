{"id": "2507.05449", "categories": ["math.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.05449", "abs": "https://arxiv.org/abs/2507.05449", "authors": ["Moshe White"], "title": "Radon Partitions of Random Gaussian Polytopes", "comment": "19 pages, 5 figures. This paper appeared as a chapter in the PhD\n  thesis of the author - submitted in May 2023, and approved in October 2023.\n  References might not be up to date", "summary": "In this paper we study a probabilistic framework for Radon partitions, where\nour points are chosen independently from the $d$-dimensional normal\ndistribution. For every point set we define a corresponding Radon polytope,\nwhich encodes all information about Radon partitions of our set - with Radon\npartitions corresponding to faces of the polytope. This allows us to derive\nexpressions for the probability that a given partition of $N$ randomly chosen\npoints in $\\mathbb{R}^d$ forms a Radon partition. These expressions involve\nconic kinematic formulas and intrinsic volumes, and in general require repeated\nintegration, though we obtain closed formulas in some cases. This framework can\nprovide new perspectives on open problems that can be formulated in terms of\nRadon partitions, such as Reay's relaxed Tverberg conjecture."}
{"id": "2507.05473", "categories": ["math.CO", "05A99, 05C31, 52C07"], "pdf": "https://arxiv.org/pdf/2507.05473", "abs": "https://arxiv.org/abs/2507.05473", "authors": ["Tristram Bogart", "Kevin Woods"], "title": "Counting with two-level polynomials", "comment": "35 pages", "summary": "We examine combinatorial counting functions with two parameters, $n$ and $q$.\nFor fixed $q$, these functions are (quasi-)polynomial in $n$. As $q$ varies,\nthe degree of this polynomial is itself polynomial in $q$, as are the leading\ncoefficients. We carefully define these two-level polynomials, lay out their\nbasic algebraic properties, and provide a schema for showing a function is a\ntwo-level polynomial. Using the schema, we prove that a variety of counting\nfunctions arising in different areas of combinatorics are two-level\npolynomials. These include chromatic polynomials for many infinite families of\ngraphs, partitions of an integer into a given number of parts, placing\nnon-attacking chess pieces on a board, Sidon sets, and Sheffer sequences\n(including binomial type and Appell sequences)."}
{"id": "2507.05548", "categories": ["math.CO", "05C15"], "pdf": "https://arxiv.org/pdf/2507.05548", "abs": "https://arxiv.org/abs/2507.05548", "authors": ["Owen Henderschedt", "Jessica McDonald", "Songling Shan"], "title": "Total coloring graphs with large minimum degree", "comment": "arXiv admin note: text overlap with arXiv:2405.07382", "summary": "We prove that for all $\\varepsilon>0$, there exists a positive integer $n_0$\nsuch that if $G$ is a graph on $n\\geq n_0$ vertices with\n$\\delta(G)\\geq\\tfrac{1}{2}(1 + \\varepsilon)n$, then $G$ satisfies the Total\nColoring Conjecture, that is, $\\chi_T(G)\\leq \\Delta(G)+2$."}
{"id": "2507.05570", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.05570", "abs": "https://arxiv.org/abs/2507.05570", "authors": ["Jin Cai", "Bo Zhou"], "title": "Signless Laplacian index conditions for doubly chorded cycles in graphs with given order", "comment": null, "summary": "In this paper, we show that for a graph of order $n$, where $n\\ge 5$, if the\nsignless Laplacian index is larger than or equal to certain value depending on\n$n$, then the graph contains a doubly chorded cycle, where the chords incident\nto a common vertex, unless it is two specified graphs."}
{"id": "2507.05382", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.05382", "abs": "https://arxiv.org/abs/2507.05382", "authors": ["M. Marques Alves", "J. E. Navarro Caballero", "R. T. Marcavillaca"], "title": "An inexact inertial projective splitting algorithm with strong convergence", "comment": "31 pages", "summary": "We propose and study a strongly convergent inexact inertial projective\nsplitting (PS) algorithm for finding zeros of composite monotone inclusion\nproblems involving the sum of finitely many maximal monotone operators. Strong\nconvergence of the iterates is ensured by projections onto the intersection of\nappropriately defined half-spaces, even in the absence of inertial effects. We\nalso establish iteration-complexity results for the proposed PS method, which\nlikewise hold without requiring inertial terms. The algorithm includes two\ninertial sequences, controlled by parameters satisfying mild conditions, while\npreserving strong convergence and enabling iteration-complexity analysis.\nFurthermore, for more structured monotone inclusion problems, we derive two\nvariants of the main algorithm that employ forward-backward and\nforward-backward-forward steps."}
{"id": "2507.05374", "categories": ["math.NT", "math.AG", "math.RT", "11S31, 11F85, 14G22, 14G45"], "pdf": "https://arxiv.org/pdf/2507.05374", "abs": "https://arxiv.org/abs/2507.05374", "authors": ["Andrew Graham", "Pol van Hoften", "Sean Howe"], "title": "$p$-adic Fourier theory in families", "comment": "80 pages. Comments welcome!", "summary": "We construct Fourier transforms relating functions and distributions on\nfinite height $p$-divisible rigid analytic groups and objects in a dual\ncategory of $\\mathbb{Z}_p$-local systems with analyticity conditions. Our\nFourier transforms are formulated as isomorphisms of solid Hopf algebras over\narbitrary small v-stacks, and generalize earlier constructions of Amice and\nSchneider--Teitelbaum. We also construct compatible integral Fourier transforms\nfor $p$-divisible groups and their dual Tate modules. As an application, we use\nthe Weierstrass $\\wp$-function to construct a global Eisenstein measure over\nthe $p$-adic modular curve, extending previous constructions of Katz over the\nordinary locus and at CM points, and show its generic fiber, the global\nEisenstein distribution, gives rise to new families of quaternionic modular\nforms that overconverge from profinite sets in the rigid analytic supersingular\nlocus."}
{"id": "2507.05919", "categories": ["cs.DM"], "pdf": "https://arxiv.org/pdf/2507.05919", "abs": "https://arxiv.org/abs/2507.05919", "authors": ["Thierry Marchant", "Sandip Sarkar"], "title": "Axiomatic characterizations of dissimilarity orderings and distances between sets", "comment": null, "summary": "We characterize the orderings of pairs of sets induced by several distances:\nHamming, Jaccard, S\\o rensen-Dice and Overlap. We also characterize these\ndistances."}
{"id": "2507.05267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05267", "abs": "https://arxiv.org/abs/2507.05267", "authors": ["Markus Böck"], "title": "Strongly Solving $7 \\times 6$ Connect-Four on Consumer Grade Hardware", "comment": null, "summary": "While the game Connect-Four has been solved mathematically and the best move\ncan be effectively computed with search based methods, a strong solution in the\nform of a look-up table was believed to be infeasible. In this paper, we\nrevisit a symbolic search method based on binary decision diagrams to produce\nstrong solutions. With our efficient implementation we were able to produce a\n89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main\nmemory for the standard $7 \\times 6$ board size. In addition to this\nwin-draw-loss evaluation, we include an alpha-beta search in our open source\nartifact to find the move which achieves the fastest win or slowest loss."}
{"id": "2507.05371", "categories": ["math.LO", "03E25, 22F05 (Primary) 57N50, 57M60 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.05371", "abs": "https://arxiv.org/abs/2507.05371", "authors": ["Justin Young"], "title": "Permutation Models Arising From Topological Ideals", "comment": "13 pages", "summary": "A recent paper by Zapletal arXiv:2404.10612 discusses permutation models of\nset theory which arise from dynamical ideals and highlights properties of the\ndynamical ideal which relate to fragments of choice in the permutation model.\nIn this paper, we provide several examples from topology which illustrate using\nthese connections to argue that the corresponding permutation model satisfies\neither the axiom of countable choice or well-ordered choice."}
{"id": "2507.05628", "categories": ["math.ST", "stat.TH", "60G15, 62F12"], "pdf": "https://arxiv.org/pdf/2507.05628", "abs": "https://arxiv.org/abs/2507.05628", "authors": ["Mitsuki Kobayashi", "Yuto Nishiwaki", "Yasutaka Shimizu", "Nobutoki Takaoka"], "title": "Maximum likelihood estimation of mean functions for Gaussian processes under small noise asymptotics", "comment": null, "summary": "Maximum likelihood estimators for time-dependent mean functions within\nGaussian processes are provided in the context of continuous observations. We\nfind the widest possible class of mean functions for which the likelihood\nfunction can be written explicitly. When it is subjected to a small noise\nasymptotic condition leading to the vanishing of the primary Gaussian noise, we\nattain local asymptotic normality results, accompanied by insights into the\nasymptotic efficiency of these estimators. In addition, we introduce\nM-estimators based on discrete samples, which also leads us to the asymptotic\nefficiency. Furthermore, we provide quasi-information criteria for model\nselection analogous to Akaike Information Criteria in discretely observed\ncases."}
{"id": "2507.05415", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05415", "abs": "https://arxiv.org/abs/2507.05415", "authors": ["Lu Xian", "Van Tran", "Lauren Lee", "Meera Kumar", "Yichen Zhang", "Florian Schaub"], "title": "Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks", "comment": "Accepted for publication in CCS 2025. This is a pre-publication\n  version", "summary": "Privacy policies are often complex. An exception is the two-page standardized\nnotice that U.S. financial institutions must provide under the\nGramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile\napps, and other services that involve complex data sharing practices that\nrequire additional privacy notices and do-not-sell opt-outs. We conducted a\nlarge-scale analysis of how U.S. banks implement privacy policies and controls\nin response to GLBA; other federal privacy policy requirements; and the\nCalifornia Consumer Privacy Act (CCPA), a key example for U.S. state privacy\nlaws. We focused on the disclosure and control of a set of especially\nprivacy-invasive practices: third-party data sharing for marketing-related\npurposes. We collected privacy policies for the 2,067 largest U.S. banks,\n45.3\\% of which provided multiple policies. Across disclosures and controls\nwithin the \\textit{same} bank, we identified frequent, concerning\ninconsistencies -- such as banks indicating in GLBA notices that they do not\nshare with third parties but disclosing sharing elsewhere, or using third-party\nmarketing/advertising cookies without disclosure. This multiplicity of\npolicies, with the inconsistencies it causes, may create consumer confusion and\nundermine the transparency goals of the very laws that require them. Our\nfindings call into question whether current policy requirements, such as the\nGLBA notice, are achieving their intended goals in today's online banking\nlandscape. We discuss potential avenues for reforming and harmonizing privacy\npolicies and control requirements across federal and state laws."}
{"id": "2507.05614", "categories": ["math.CO", "math.AG", "math.AT", "math.RT"], "pdf": "https://arxiv.org/pdf/2507.05614", "abs": "https://arxiv.org/abs/2507.05614", "authors": ["Mathieu Guay-Paquet"], "title": "Divided difference operators for Hessenberg representations", "comment": "23 pages", "summary": "The equivariant cohomology ring of a regular semisimple Hessenberg variety in\ntype A is a free module over the equivariant cohomology ring of a point. When\nequipped with Tymoczko's dot action, it becomes a twisted representation of the\nsymmetric group, and the character of this representation is given by the\nchromatic quasisymmetric function of an indifference graph. In this note, we\nuse divided difference operators to decompose this representation as a direct\nsum of sub-representations in a way that categorifies the modular relation\nbetween chromatic quasisymmetric functions."}
{"id": "2507.05466", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.05466", "abs": "https://arxiv.org/abs/2507.05466", "authors": ["Anne Rubbens", "Sébastien Colla", "Julien M. Hendrickx"], "title": "Computer-aided analyses of stochastic first-order methods, via interpolation conditions for stochastic optimization", "comment": null, "summary": "This work proposes a framework, embedded within the Performance Estimation\nframework (PEP), for obtaining worst-case performance guarantees on stochastic\nfirst-order methods. Given a first-order method, a function class, and a noise\nmodel with prescribed expectation and variance properties, we present a range\nof semidefinite programs (SDPs) of increasingly large size, whose solutions\nyield increasingly strong convergence guarantees on the problem. Eventually, we\npropose SDPs whose size depends on $2^N$, with $N$ the number of iterations\nanalyzed, that yield tight guarantees, attained by specific functions and noise\ndistributions within these classes. On the other side of the spectrum, we\npropose SDPs whose size depends linearly on $N$, and numerically show that, on\nmany problems, they already provide tight guarantees.\n  The framework accommodates a wide range of stochastic settings, with finite\nor infinite support, including the unstructured noise model with bounded\nvariance, finite-sum optimization, and block-coordinate methods, in a unified\nmanner, as guarantees apply to any setting consistent with the noise model,\ni.e., its expectation and variance. It covers both non-variance-reduced and\nvariance-reduced methods. Using the framework, we analyze the stochastic\ngradient method under several noise models, and illustrate how the resulting\nnumerical and analytical convergence rates connect with existing results. In\nparticular, we provide improved convergence rates on the unstructured noise\nmodel with bounded variance and in the block-coordinate setting."}
{"id": "2507.05693", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2507.05693", "abs": "https://arxiv.org/abs/2507.05693", "authors": ["Takeo Uramoto"], "title": "The completeness of the Deligne-Ribet monoids", "comment": "5 pages", "summary": "Following Cornelissen, Li, Marcolli, and Smit, this short paper proves that\nthe isomorphism of the Deligne-Ribet monoids $DR_K, DR_L$ for two number fields\n$K, L$ implies the field isomorphism of $K$ and $L$. Thus the Deligne-Ribet\nmonoid $DR_K$ gives a complete invariant of the number field $K$ as in the case\nof the absolute Galois group $G_K$."}
{"id": "2507.06169", "categories": ["math.CO", "cs.DM", "05C75 (Primary) 05C83 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.06169", "abs": "https://arxiv.org/abs/2507.06169", "authors": ["Maria Chudnovsky", "David Fischer", "Sepehr Hajebi", "Sophie Spirkl", "Bartosz Walczak"], "title": "A simple layered-wheel-like construction", "comment": "16 pages, 2 figures", "summary": "In recent years, there has been significant interest in characterizing the\ninduced subgraph obstructions to bounded treewidth and pathwidth. While this\nhas recently been resolved for pathwidth, the case of treewidth remains open,\nand prior work has reduced the problem to understanding the layered-wheel-like\nobstructions -- graphs that contain large complete minor models with each\nbranching set inducing a path; exclude large walls as induced minors; exclude\nlarge complete bipartite graphs as induced minors; and exclude large complete\nsubgraphs.\n  There are various constructions of such graphs, but they are all rather\ninvolved. In this paper, we present a simple construction of layered-wheel-like\ngraphs with arbitrarily large treewidth. Three notable features of our\nconstruction are: (a) the vertices of degree at least four can be made to be\narbitrarily far apart; (b) the girth can be made to be arbitrarily large; and\n(c) every outerstring induced subgraph of the graphs from our construction has\ntreewidth bounded by an absolute constant. In contrast, among several\npreviously known constructions of layered wheels, none achieves (a); at most\none satisfies either (b) or (c); and none satisfies both (b) and (c)\nsimultaneously.\n  In particular, this is related to a former conjecture of Trotignon, that\nevery graph with large enough treewidth, excluding large walls and large\ncomplete bipartite graphs as induced minors, and large complete subgraphs, must\ncontain an outerstring induced subgraph of large treewidth. Our construction\nprovides the first counterexample to this conjecture that can also be made to\nhave arbitrarily large girth."}
{"id": "2507.05283", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT."}
{"id": "2507.05471", "categories": ["math.LO", "math.KT", "03E35, 03E75, 18G10"], "pdf": "https://arxiv.org/pdf/2507.05471", "abs": "https://arxiv.org/abs/2507.05471", "authors": ["Jeffrey Bergfalk", "Matteo Casarosa"], "title": "Higher limits of wider systems", "comment": "25 pages; comments welcome", "summary": "Write $\\mathbf{A}_\\lambda$ for what might be described as the most elementary\nnontrivial inverse system of abelian groups indexed by the functions from the\ncardinal $\\lambda$ to the set of natural numbers. The question of whether for\nany fixed $n$ the derived limit $\\mathrm{lim}^n\\,\\mathbf{A}_\\lambda$ may vanish\nfor only a nonempty subset of the class of infinite cardinals $\\lambda$ is\nrecorded in both [Be17] and [Ban23], and bears closely on several related\nfurther ones. We answer this question in the affirmative; in fact, we show the\nmaximal possibility, namely that this can simultaneously happen in every degree\n$n>1$."}
{"id": "2507.05634", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.05634", "abs": "https://arxiv.org/abs/2507.05634", "authors": ["Kangda K. Wren"], "title": "A Note on Inferential Decisions, Errors and Path-Dependency", "comment": "11 pages: 7 main text, 1 highlight, 3 appendix and references", "summary": "Consider the standard sequential testing of a binary outcome. The associated\nbelief process and its objectively true conditional-probability counterpart\ngenerally differ, but they converge to the same target in well-defined tests.\nWe show that unless the two processes are 'essentially identical', differing at\nmost by an a priori factor, time-homogeneous continuous sequential decisions\nbased on the former must be path-dependent with respect to state-variables\nbased on the latter or other non-essentially-identical belief processes.\nFurther, total inferential errors decompose into two components with distinct\nand independent characteristics."}
{"id": "2507.05421", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05421", "abs": "https://arxiv.org/abs/2507.05421", "authors": ["Harrison Green", "Claire Le Goues", "Fraser Brown"], "title": "FrameShift: Learning to Resize Fuzzer Inputs Without Breaking Them", "comment": null, "summary": "Coverage-guided fuzzers are powerful automated bug-finding tools. They mutate\nprogram inputs, observe coverage, and save any input that hits an unexplored\npath for future mutation. Unfortunately, without knowledge of input\nformats--for example, the relationship between formats' data fields and\nsizes--fuzzers are prone to generate destructive frameshift mutations. These\ntime-wasting mutations yield malformed inputs that are rejected by the target\nprogram. To avoid such breaking mutations, this paper proposes a novel,\nlightweight technique that preserves the structure of inputs during mutation by\ndetecting and using relation fields.\n  Our technique, FrameShift, is simple, fast, and does not require additional\ninstrumentation beyond standard coverage feedback. We implement our technique\nin two state-of-the-art fuzzers, AFL++ and LibAFL, and perform a 12+ CPU-year\nfuzzer evaluation, finding that FrameShift improves the performance of the\nfuzzer in each configuration, sometimes increasing coverage by more than 50%.\nFurthermore, through a series of case studies, we show that our technique is\nversatile enough to find important structural relationships in a variety of\nformats, even generalizing beyond C/C++ targets to both Rust and Python."}
{"id": "2507.05641", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.05641", "abs": "https://arxiv.org/abs/2507.05641", "authors": ["Xiaoyu He", "Jiaxi Nie", "Yuval Wigderson", "Hung-Hsun Hans Yu"], "title": "Off-Diagonal Ramsey Numbers for Linear Hypergraphs", "comment": "15 pages, 4 figures", "summary": "We study off-diagonal Ramsey numbers $r(H, K_n^{(k)})$ of $k$-uniform\nhypergraphs, where $H$ is a fixed linear $k$-uniform hypergraph and $K_n^{(k)}$\nis complete on $n$ vertices. Recently, Conlon et al.\\ disproved the folklore\nconjecture that $r(H, K_n^{(3)})$ always grows polynomially in $n$. In this\npaper we show that much larger growth rates are possible in higher uniformity.\nIn uniformity $k\\ge 4$, we prove that for any constant $C>0$, there exists a\nlinear $k$-uniform hypergraph $H$ for which $$r(H,K_n^{(k)}) \\geq\n\\twr_{k-2}(2^{(\\log n)^C}).$$"}
{"id": "2507.05501", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.05501", "abs": "https://arxiv.org/abs/2507.05501", "authors": ["Oscar Dowson", "Xavier Gandibleux", "Gökhan Kof"], "title": "MultiObjectiveAlgorithms.jl: a Julia package for solving multi-objective optimization problems", "comment": null, "summary": "We present MultiObjectiveAlgorithms.jl, an open-source Julia library for\nsolving multi-objective optimization problems written in JuMP.\nMultiObjectiveAlgorithms.jl implements a number of different solution\nalgorithms, which all rely on an iterative scalarization of the problem from a\nmulti-objective optimization problem to a sequence of single-objective\nsubproblems. As part of this work, we extended JuMP to support vector-valued\nobjective functions. Because it is based on JuMP, MultiObjectiveAlgorithms.jl\ncan use a wide variety of commercial and open-source solvers to solve the\nsingle-objective subproblems, and it supports problem classes ranging from\nlinear, to conic, semi-definite, and general nonlinear.\nMultiObjectiveAlgorithms.jl is available at\nhttps://github.com/jump-dev/MultiObjectiveAlgorithms.jl under a MPL-2 license."}
{"id": "2507.05792", "categories": ["math.NT", "math.KT"], "pdf": "https://arxiv.org/pdf/2507.05792", "abs": "https://arxiv.org/abs/2507.05792", "authors": ["Wenhuan Huang"], "title": "On Generators of Bloch groups of CM fields", "comment": null, "summary": "This article generalizes the result of Burns et al (2022), to find an\nalgorithm to find some elements generating a full-rank subgroup of the\ntorsion-free part of Bloch group of a certain CM number field, and compute the\nrank of it."}
{"id": "2507.05297", "categories": ["cs.AI", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.05297", "abs": "https://arxiv.org/abs/2507.05297", "authors": ["Zijun Meng"], "title": "Fuzzy Classification Aggregation for a Continuum of Agents", "comment": null, "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean."}
{"id": "2507.06007", "categories": ["math.LO"], "pdf": "https://arxiv.org/pdf/2507.06007", "abs": "https://arxiv.org/abs/2507.06007", "authors": ["Matteo De Berardinis", "Silvio Ghilardi"], "title": "A Proof Theory for Profinite Modal Algebras", "comment": null, "summary": "In a previous paper, we showed that profinite $L$-algebras (where $L$ is a\nvariety of modal algebras generated by its finite members) are monadic over\n$\\mathbf{Set}$. This monadicity result suggests that profinite $L$-algebras\ncould be presented as Lindenbaum algebras for propositional theories in\ninfinitary versions of propositional modal calculi. In this paper we identify\nsuch calculi as modal enrichments of Maehara-Takeuti's infinitary extension of\nthe sequent calculus $\\mathbf{LK}$. We also investigate correspondences between\nsyntactic properties of the calculi and regularity/exactness properties of the\nopposite category of profinite $L$-algebras."}
{"id": "2507.05689", "categories": ["math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.05689", "abs": "https://arxiv.org/abs/2507.05689", "authors": ["Ming Gao", "Yuhao Wang", "Bryon Aragam"], "title": "Optimal structure learning and conditional independence testing", "comment": null, "summary": "We establish a fundamental connection between optimal structure learning and\noptimal conditional independence testing by showing that the minimax optimal\nrate for structure learning problems is determined by the minimax rate for\nconditional independence testing in these problems. This is accomplished by\nestablishing a general reduction between these two problems in the case of\npoly-forests, and demonstrated by deriving optimal rates for several examples,\nincluding Bernoulli, Gaussian and nonparametric models. Furthermore, we show\nthat the optimal algorithm in these settings is a suitable modification of the\nPC algorithm. This theoretical finding provides a unified framework for\nanalyzing the statistical complexity of structure learning through the lens of\nminimax testing."}
{"id": "2507.05445", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05445", "abs": "https://arxiv.org/abs/2507.05445", "authors": ["Daniel Jones", "Giorgio Severi", "Martin Pouliot", "Gary Lopez", "Joris de Gruyter", "Santiago Zanella-Beguelin", "Justin Song", "Blake Bullwinkel", "Pamela Cortez", "Amanda Minnich"], "title": "A Systematization of Security Vulnerabilities in Computer Use Agents", "comment": null, "summary": "Computer Use Agents (CUAs), autonomous systems that interact with software\ninterfaces via browsers or virtual machines, are rapidly being deployed in\nconsumer and enterprise environments. These agents introduce novel attack\nsurfaces and trust boundaries that are not captured by traditional threat\nmodels. Despite their growing capabilities, the security boundaries of CUAs\nremain poorly understood. In this paper, we conduct a systematic threat\nanalysis and testing of real-world CUAs under adversarial conditions. We\nidentify seven classes of risks unique to the CUA paradigm, and analyze three\nconcrete exploit scenarios in depth: (1) clickjacking via visual overlays that\nmislead interface-level reasoning, (2) indirect prompt injection that enables\nRemote Code Execution (RCE) through chained tool use, and (3) CoT exposure\nattacks that manipulate implicit interface framing to hijack multi-step\nreasoning. These case studies reveal deeper architectural flaws across current\nCUA implementations. Namely, a lack of input provenance tracking, weak\ninterface-action binding, and insufficient control over agent memory and\ndelegation. We conclude by proposing a CUA-specific security evaluation\nframework and design principles for safe deployment in adversarial and\nhigh-stakes settings."}
{"id": "2507.05697", "categories": ["math.CO", "math.AT", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.05697", "abs": "https://arxiv.org/abs/2507.05697", "authors": ["Asaf Cohen Antonir", "Yuval Peled", "Asaf Shapira", "Mykhaylo Tyomkyn", "Maksim Zhukovskii"], "title": "When does a tree activate the random graph?", "comment": "Comments are welcome!", "summary": "Let $F$ and $G$ be two graphs. A spanning subgraph $H$ of $G$ is called\nweakly $F$-saturated if one can add to $H$ the edges of $G \\setminus H$ in some\norder, so that whenever a new edge is added, a new copy of $F$ is formed.\nObtaining lower bounds for the minimum size $\\mathrm{wsat}(G,F)$ of such an $H$\nis a classical problem in extremal combinatorics. In particular, in the past 40\nyears, various algebraic tools have been developed to prove lower bounds on the\nweak saturation number $\\mathrm{wsat}(G,F)$. Our paper uncovers a new\nconnection of weak saturation to topology of clique complexes, that allows to\nprove tight lower bounds in some cases when the algebraic tools are not\nefficient.\n  It is easy to see that the smallest $K_3$-saturating graphs in $K_n$ are\ntrees, thus $\\mathrm{wsat}(K_n,K_3)=n-1$. In 2017, Kor\\'andi and Sudakov proved\nthat this is also the case in dense random graphs $G\\sim G_{n,p}$,\n$p=\\mathrm{const}\\in(0,1)$, and posed the question of determining the smallest\n$p$ for which $G_{n,p}$ contains a $K_3$-saturating tree with high probability.\nUsing the new topological connection, we show that this critical $p$ is of\norder $n^{-1/3-o(1)}$.\n  Inspired by Gromov's local-to-global principle for hyperbolic groups, we\nfurther develop our topological approach and determine the critical probability\nup to a constant factor, for trees with diameter at most $n^{c}$, for some\n$c>0$.\n  The new connection also enables us to improve the best known upper bound on\nthe threshold probability for simple connectivity of the 2-dimensional clique\ncomplex of $G_{n,p}$, due to Kahle."}
{"id": "2507.05562", "categories": ["math.OC", "cs.LG", "math.FA", "90C25, 65K05, 37N40, 46N10, 34A60, 62J07", "G.1.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.05562", "abs": "https://arxiv.org/abs/2507.05562", "authors": ["Gabriel P. Langlois", "Jérôme Darbon"], "title": "Exact and efficient basis pursuit denoising via differential inclusions and a selection principle", "comment": "50 pages, 2 figures, submitted", "summary": "Basis pursuit denoising (BPDN) is a cornerstone of compressive sensing,\nstatistics and machine learning. While various algorithms for BPDN have been\nproposed, they invariably suffer from drawbacks and must either favor\nefficiency at the expense of accuracy or vice versa. As such, state-of-the-art\nalgorithms remain ineffective for high-dimensional applications that require\naccurate solutions within a reasonable amount of computational time. In this\nwork, we address this issue and propose an exact and efficient algorithm for\nBPDN using differential inclusions. Specifically, we prove that a selection\nprinciple from the theory of differential inclusions turns the dual problem of\nBPDN into calculating the trajectory of an \\emph{integrable} projected\ndynamical system, that is, whose trajectory and asymptotic limit can be\ncomputed exactly. Our analysis naturally yields an exact algorithm, numerically\nup to machine precision, that is amenable to computing regularization paths and\nvery fast. Numerical experiments confirm that our algorithm outperforms the\nstate-of-the-art algorithms in both accuracy and efficiency. Moreover, we show\nthat the global continuation of solutions (in terms of the hyperparameter and\ndata) of the projected dynamical system yields a rigorous homotopy algorithm\nfor BPDN, as well as a novel greedy algorithm for computing feasible solutions\nto basis pursuit in strongly polynomial time. Beyond this work, we expect that\nour results and analysis can be adapted to compute exact or approximate\nsolutions to a broader class of polyhedral-constrained optimization problems."}
{"id": "2507.05862", "categories": ["math.NT", "11A05, 11R04, 11Y40"], "pdf": "https://arxiv.org/pdf/2507.05862", "abs": "https://arxiv.org/abs/2507.05862", "authors": ["Gustav Kjærbye Bagger", "Andrew R. Booker", "Bryce Kerr", "Kevin McGown", "Valeriia Starichkova", "Tim Trudgian"], "title": "The determination of norm-Euclidean cyclic cubic fields", "comment": "24 pages", "summary": "It is known on the Generalised Riemann Hypothesis that there are precisely\n$13$ cyclic cubic fields that are norm-Euclidean. Unconditionally, there is a\ngap between analytic estimates which hold for all sufficiently large conductors\nand computational techniques. In this paper, we establish new results\nconcerning explicit bounds for cubic non-residues and refine previous\ncomputational techniques, enabling us to completely characterise all\nnorm-Euclidean cyclic cubic fields."}
{"id": "2507.05488", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05488", "abs": "https://arxiv.org/abs/2507.05488", "authors": ["Subhasis Dasgupta", "Jon Stephens", "Amarnath Gupta"], "title": "OLG++: A Semantic Extension of Obligation Logic Graph", "comment": null, "summary": "We present OLG++, a semantic extension of the Obligation Logic Graph (OLG)\nfor modeling regulatory and legal rules in municipal and interjurisdictional\ncontexts. OLG++ introduces richer node and edge types, including spatial,\ntemporal, party group, defeasibility, and logical grouping constructs, enabling\nnuanced representations of legal obligations, exceptions, and hierarchies. The\nmodel supports structured reasoning over rules with contextual conditions,\nprecedence, and complex triggers. We demonstrate its expressiveness through\nexamples from food business regulations, showing how OLG++ supports legal\nquestion answering using property graph queries. OLG++ also improves over\nLegalRuleML by providing native support for subClassOf, spatial constraints,\nand reified exception structures. Our examples show that OLG++ is more\nexpressive than prior graph-based models for legal knowledge representation."}
{"id": "2507.05958", "categories": ["math.ST", "stat.TH", "62-08, 62G20, 62P30, 65C05"], "pdf": "https://arxiv.org/pdf/2507.05958", "abs": "https://arxiv.org/abs/2507.05958", "authors": ["Haythem Boucharif", "Jérôme Morio", "Paul Rochet"], "title": "Importance sampling for Sobol' indices estimation", "comment": null, "summary": "We propose a new importance sampling framework for the estimation and\nanalysis of Sobol' indices. We show that a Sobol' index defined under a\nreference input distribution can be consistently estimated from samples drawn\nfrom other sampling distributions by reweighting the estimator appropriately to\naccount for the distribution change. We derive the optimal sampling\ndistribution that minimizes the asymptotic variance and demonstrate its strong\nimpact on estimation accuracy. Beyond variance reduction, the framework\nsupports distributional sensitivity analysis via reverse importance sampling,\nenabling robust exploration of input distribution uncertainty with negligible\nadditional computational cost."}
{"id": "2507.05512", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05512", "abs": "https://arxiv.org/abs/2507.05512", "authors": ["Gehao Zhang", "Eugene Bagdasarian", "Juan Zhai", "Shiqing Ma"], "title": "Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice", "comment": null, "summary": "Distinguishing AI-generated code from human-written code is becoming crucial\nfor tasks such as authorship attribution, content tracking, and misuse\ndetection. Based on this, N-gram-based watermarking schemes have emerged as\nprominent, which inject secret watermarks to be detected during the generation.\n  However, their robustness in code content remains insufficiently evaluated.\nMost claims rely solely on defenses against simple code transformations or code\noptimizations as a simulation of attack, creating a questionable sense of\nrobustness. In contrast, more sophisticated schemes already exist in the\nsoftware engineering world, e.g., code obfuscation, which significantly alters\ncode while preserving functionality. Although obfuscation is commonly used to\nprotect intellectual property or evade software scanners, the robustness of\ncode watermarking techniques against such transformations remains largely\nunexplored.\n  In this work, we formally model the code obfuscation and prove the\nimpossibility of N-gram-based watermarking's robustness with only one intuitive\nand experimentally verified assumption, distribution consistency, satisfied.\nGiven the original false positive rate of the watermarking detection, the ratio\nthat the detector failed on the watermarked code after obfuscation will\nincrease to 1 - fpr.\n  The experiments have been performed on three SOTA watermarking schemes, two\nLLMs, two programming languages, four code benchmarks, and four obfuscators.\nAmong them, all watermarking detectors show coin-flipping detection abilities\non obfuscated codes (AUROC tightly surrounds 0.5). Among all models,\nwatermarking schemes, and datasets, both programming languages own obfuscators\nthat can achieve attack effects with no detection AUROC higher than 0.6 after\nthe attack. Based on the theoretical and practical observations, we also\nproposed a potential path of robust code watermarking."}
{"id": "2507.05775", "categories": ["math.CO", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.05775", "abs": "https://arxiv.org/abs/2507.05775", "authors": ["Anne-Laure Basdevant", "Lucas Gerin", "Maxime Marivain"], "title": "Longest increasing subsequences for distributions with atoms, and an inhomogeneous Hammersley process", "comment": null, "summary": "A famous result by Hammersley and Versik-Kerov states that the length $L_n$\nof the longest increasing subsequence among $n$ iid continuous random variables\ngrows like $2\\sqrt{n}$. We investigate here the asymptotic behavior of $L_n$\nfor distributions with atoms. For purely discrete random variables, we\ncharacterize the asymptotic order of $L_n$ through a variational problem and\nprovide explicit estimates for classical distributions. The proofs rely on a\ncoupling with an inhomogeneous version of the discrete-time continuous-space\nHammersley process. This reveals that, in contrast to the continuous case, the\ndiscrete setting exhibits a wide range of growth rates between $\\mathcal{O}(1)$\nand $o(\\sqrt{n})$, depending on the tail behavior of the distribution. We can\nthen easily deduce the asymptotics of $L_n$ for a completely arbitrary\ndistribution."}
{"id": "2507.05610", "categories": ["math.OC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.05610", "abs": "https://arxiv.org/abs/2507.05610", "authors": ["Devansh Gupta", "Meisam Razaviyayn", "Vatsal Sharan"], "title": "On the Inherent Privacy of Zeroth Order Projected Gradient Descent", "comment": "Accepted at AISTATS'25", "summary": "Differentially private zeroth-order optimization methods have recently gained\npopularity in private fine tuning of machine learning models due to their\nreduced memory requirements. Current approaches for privatizing zeroth-order\nmethods rely on adding Gaussian noise to the estimated zeroth-order gradients.\nHowever, since the search direction in the zeroth-order methods is inherently\nrandom, researchers including Tang et al. (2024) and Zhang et al. (2024a) have\nraised an important question: is the inherent noise in zeroth-order estimators\nsufficient to ensure the overall differential privacy of the algorithm? This\nwork settles this question for a class of oracle-based optimization algorithms\nwhere the oracle returns zeroth-order gradient estimates. In particular, we\nshow that for a fixed initialization, there exist strongly convex objective\nfunctions such that running (Projected) Zeroth-Order Gradient Descent (ZO-GD)\nis not differentially private. Furthermore, we show that even with random\ninitialization and without revealing (initial and) intermediate iterates, the\nprivacy loss in ZO-GD can grow superlinearly with the number of iterations when\nminimizing convex objective functions."}
{"id": "2507.05905", "categories": ["math.NT", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.05905", "abs": "https://arxiv.org/abs/2507.05905", "authors": ["Jiyoung Han", "Seul Bee Lee"], "title": "Moment formulas of Siegel transforms with congruence conditions in dimension 2", "comment": "20 Pages", "summary": "We compute the first and second moment formulas for Siegel transforms related\nto problems counting primitive lattice points in the real plane with congruence\nconditions. As applications, we derive an analog of Schmidt's random counting\ntheorem and the quantitative Khintchine theorem for irrational numbers,\napproximated by rational numbers $p/q$, where we place a congruence-conditional\nconstraint on the vector $(p,q)$."}
{"id": "2507.05495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05495", "abs": "https://arxiv.org/abs/2507.05495", "authors": ["Prahaladh Chandrahasan", "Jiahe Jin", "Zhihan Zhang", "Tevin Wang", "Andy Tang", "Lucy Mo", "Morteza Ziyadi", "Leonardo F. R. Ribeiro", "Zimeng Qiu", "Markus Dreyer", "Akari Asai", "Chenyan Xiong"], "title": "Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents", "comment": null, "summary": "Effectively evaluating deep research agents that autonomously search the web,\nanalyze information, and generate reports remains a major challenge,\nparticularly when it comes to assessing long reports and giving detailed\nfeedback on their intermediate steps. To address these gaps, we introduce Deep\nResearch Comparator, a platform that offers a holistic framework for deep\nresearch agent hosting, side-by-side comparison, fine-grained human feedback\ncollection, and ranking calculation. Given a user query, our platform displays\nthe final reports from two different agents along with their intermediate steps\nduring generation. Annotators can evaluate the overall quality of final reports\nbased on side-by-side comparison, and also provide detailed feedback separately\nby assessing intermediate steps or specific text spans within the final report.\nFurthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This\nscaffold serves as a baseline that facilitates the easy integration of various\nlarge language models to transform them into deep research agents for\nevaluation. To demonstrate the platform's utility for deep research agent\ndevelopment, we have collected real user preference data from 17 annotators on\nthree deep research agents. A demo video of our platform can be found at\nhttps://www.youtube.com/watch?v=g4d2dnbdseg."}
{"id": "2507.06098", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.06098", "abs": "https://arxiv.org/abs/2507.06098", "authors": ["Fabienne Comte", "Nicolas Marie"], "title": "Nonparametric Estimation in SDE Models Involving an Explanatory Process", "comment": "39 pages, 3 figures", "summary": "This paper deals with the process $X = (X_t)_{t\\in [0,T]}$ defined by the\nstochastic differential equation (SDE) $dX_t = (a(X_t) + b(Y_t))dt\n+\\sigma(X_t)dW_1(t)$, where $W_1$ is a Brownian motion and $Y$ is an exogenous\nprocess. The first task - of probabilistic nature - is to properly define the\nmodel, to prove the existence and uniqueness of the solution of such an\nequation, and then to establish the existence and a suitable control of a\ndensity with respect to the Lebesgue measure of the distribution of $(X_t,Y_t)$\n($t > 0$). In the second part of the paper, a risk bound and a rate of\nconvergence in specific Sobolev spaces are established for a copies-based\nprojection least squares estimator of the $\\mathbb R^2$-valued function\n$(a,b)$. Moreover, a model selection procedure making the adequate\nbias-variance compromise both in theory and practice is investigated."}
{"id": "2507.05524", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05524", "abs": "https://arxiv.org/abs/2507.05524", "authors": ["Sara Chennoufi", "Yufei Han", "Gregory Blanc", "Emiliano De Cristofaro", "Christophe Kiennert"], "title": "PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing", "comment": null, "summary": "In distributed networks, participants often face diverse and fast-evolving\ncyberattacks. This makes techniques based on Federated Learning (FL) a\npromising mitigation strategy. By only exchanging model updates, FL\nparticipants can collaboratively build detection models without revealing\nsensitive information, e.g., network structures or security postures. However,\nthe effectiveness of FL solutions is often hindered by significant data\nheterogeneity, as attack patterns often differ drastically across organizations\ndue to varying security policies. To address these challenges, we introduce\nPROTEAN, a Prototype Learning-based framework geared to facilitate\ncollaborative and privacy-preserving intrusion detection. PROTEAN enables\naccurate detection in environments with highly non-IID attack distributions and\npromotes direct knowledge sharing by exchanging class prototypes of different\nattack types among participants. This allows organizations to better understand\nattack techniques not present in their data collections. We instantiate PROTEAN\non two cyber intrusion datasets collected from IIoT and 5G-connected\nparticipants and evaluate its performance in terms of utility and privacy,\ndemonstrating its effectiveness in addressing data heterogeneity while\nimproving cyber attack understanding in federated intrusion detection systems\n(IDSs)."}
{"id": "2507.05821", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.05821", "abs": "https://arxiv.org/abs/2507.05821", "authors": ["Ted Dobson", "Ademir Hujdurović", "Wilfried Imrich", "Ronald Ortner"], "title": "On cubic vertex-transitive graphs of given girth", "comment": null, "summary": "A set of vertices of a graph is distinguishing if the only automorphism that\npreserves it is the identity. The minimal size of such sets, if they exist, is\nthe distinguishing cost. The distinguishing costs of vertex transitive cubic\ngraphs are well known if they are 1-arc-transitive, or if they have two edge\norbits and either have girth 3 or vertex-stabilizers of order 1 or 2.\n  There are many results about vertex-transitive cubic graphs of girth 4 with\ntwo edge orbits, but for larger girth almost nothing is known about %the\nexistence or the distinguishing costs of such graphs. We prove that cubic\nvertex-transitive graphs of girth 5 with two edge orbits have distinguishing\ncost 2, and prove the non-existence of infinite 3-arc-transitive cubic graphs\nof girth 6."}
{"id": "2507.05623", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.05623", "abs": "https://arxiv.org/abs/2507.05623", "authors": ["Xi Chen", "Jinyan Fan"], "title": "A derivative-free regularization algorithm for equality constrained nonlinear least squares problems", "comment": "22 pages", "summary": "In this paper, we study the equality constrained nonlinear least squares\nproblem, where the Jacobian matrices of the objective function and constraints\nare unavailable or expensive to compute. We approximate the Jacobian matrices\nvia orthogonal spherical smoothing and propose a derivative-free regularization\nalgorithm for solving the problem. At each iteration, a regularized augmented\nLagrangian subproblem is solved to obtain a Newton-like step. If a sufficient\ndecrease in the merit function of the approximate KKT system is achieved, the\nstep is accepted, otherwise a derivative-free LM algorithm is applied to get\nanother step to satisfy the sufficient decrease condition. It is shown that the\nalgorithm either finds an approximate KKT point with arbitrary high probability\nor converges to a stationary point of constraints violation almost surely."}
{"id": "2507.06130", "categories": ["math.NT", "Primary 11R27, Secondary 11R33"], "pdf": "https://arxiv.org/pdf/2507.06130", "abs": "https://arxiv.org/abs/2507.06130", "authors": ["Sergio Ricardo Zapata Ceballos", "Sara Chari", "Erik Holmes", "Fatemeh Jalalvand", "Rahinatou Yuh Njah Nchiwo", "Kelly O'Connor", "Fabian Ramirez", "Sameera Vemulapalli"], "title": "Unit lattices of $D_4$-quartic number fields with signature $(2,1)$", "comment": "Comments welcome!", "summary": "There has been a recent surge of interest on distributions of shapes of unit\nlattices in number fields, due to both their applications to number theory and\nthe lack of known results.\n  In this work we focus on $D_4$-quartic fields with signature $(2,1)$; such\nfields have a rank $2$ unit group. Viewing the unit lattice as a point of\n$GL_2(\\mathbb{Z})\\backslash \\mathfrak{h}$, we prove that every lattice which\narises this way must correspond to a transcendental point on the boundary of a\ncertain fundamental domain of $GL_2(\\mathbb{Z})\\backslash \\mathfrak{h}$.\nMoreover, we produce three explicit (algebraic) points of\n$GL_2(\\mathbb{Z})\\backslash \\mathfrak{h}$ which are limit points of the set of\n(points associated to) unit lattices of $D_4$-quartic fields with signature\n$(2,1)$."}
{"id": "2507.05515", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05515", "abs": "https://arxiv.org/abs/2507.05515", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Pablo Cesar", "Chuang Yu", "Zhaochun Ren", "Junxiao Wang"], "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality", "comment": "20 pages", "summary": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community."}
{"id": "2507.06166", "categories": ["math.ST", "math.PR", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.06166", "abs": "https://arxiv.org/abs/2507.06166", "authors": ["Omar Al-Ghattas", "Jiaheng Chen", "Daniel Sanz-Alonso"], "title": "On the Estimation of Gaussian Moment Tensors", "comment": "13 pages", "summary": "This paper studies two estimators for Gaussian moment tensors: the standard\nsample moment estimator and a plug-in estimator based on Isserlis's theorem. We\nestablish dimension-free, non-asymptotic error bounds that demonstrate and\nquantify the advantage of Isserlis's estimator for tensors of even order $p>2$.\nOur bounds hold in operator and entrywise maximum norms, and apply to symmetric\nand asymmetric tensors."}
{"id": "2507.05558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05558", "abs": "https://arxiv.org/abs/2507.05558", "authors": ["Arthur Gervais", "Liyi Zhou"], "title": "AI Agent Smart Contract Exploit Generation", "comment": null, "summary": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a $6000 exploit\nvalue, while defenders require $60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense."}
{"id": "2507.05824", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.05824", "abs": "https://arxiv.org/abs/2507.05824", "authors": ["Michal Parnas", "Adi Shraibman"], "title": "A Study of the Binary and Boolean Rank of Matrices with Small Constant Real Rank", "comment": null, "summary": "We initiate the study of the binary and Boolean rank of $0,1$ matrices that\nhave a small rank over the reals. The relationship between these three rank\nfunctions is an important open question, and here we prove that when the real\nrank $d$ is a small constant, the gap between the real and the binary and\nBoolean rank is a small constant. We give tight upper and lower bounds on the\nBoolean and binary rank of matrices with real rank $1 \\leq d \\leq 4$, as well\nas determine the size of the maximal isolation set in each case. Furthermore,\nwe prove that for $d = 3,4$, the circulant matrix defined by a row with $d-1$\nconsecutive ones followed by $d-1$ zeros, is the only matrix of size\n$(2d-2)\\times (2d-2)$ with real rank $d$ and Boolean and binary rank and\nmaximal isolation set of size $2d-2$, and this matrix achieves the maximal gap\npossible between the real and the binary and Boolean rank for these values of\n$d$.\n  Our results can also be interpreted in other equivalent terms, such as\nfinding the minimal number of bicliques needed to partition or cover the edges\nof a bipartite graph whose reduced adjacency matrix has real rank $1 \\leq d\n\\leq 4$. We use a combination of combinatorial and algebraic techniques\ncombined with the assistance of a computer program."}
{"id": "2507.05669", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.05669", "abs": "https://arxiv.org/abs/2507.05669", "authors": ["A. A. Vyguzov", "F. S. Stonyakin"], "title": "A Fully Adaptive Frank-Wolfe Algorithm for Relatively Smooth Problems and Its Application to Centralized Distributed Optimization", "comment": null, "summary": "We study the Frank-Wolfe algorithm for constrained optimization problems with\nrelatively smooth and relatively strongly convex objectives. Building upon our\nprevious work, we propose a fully adaptive variant of the Frank-Wolfe method\nthat dynamically adjusts the step size based on both the relative smoothness\nconstant and the Triangle Scaling Exponent (TSE) of the Bregman divergence. Our\nmethod does not require prior knowledge of the function parameters and\nguarantees convergence using only local information. We establish a linear\nconvergence rate under relative strong convexity and provide a detailed\ntheoretical analysis of the proposed adaptive step-size rule.\n  Furthermore, we demonstrate how relative smoothness and strong convexity\nnaturally arise in the setting of centralized distributed optimization. Under a\nvariance-type assumption on the gradients, we show that the global objective\nbecomes relatively strongly convex with respect to the Bregman divergence\ngenerated by a local function. This structure allows us to apply our adaptive\nFrank-Wolfe algorithm, leading to provable acceleration due to an improved\nrelative condition number. We support our theoretical findings with numerical\nexperiments, showing that the proposed method outperforms both non-adaptive and\npartially adaptive variants, especially in distributed settings."}
{"id": "2507.06158", "categories": ["math.NT", "cs.FL", "11A63, 68Q45"], "pdf": "https://arxiv.org/pdf/2507.06158", "abs": "https://arxiv.org/abs/2507.06158", "authors": ["Anjelo Gabriel R. Cruz", "Manuel Joseph C. Loquias", "Jörg M. Thuswaldner"], "title": "Addition Automata and Attractors of Digit Systems Corresponding to Expanding Rational Matrices", "comment": "20 pages, 11 figures", "summary": "Let $A$ be an expanding $2 \\times 2$ matrix with rational entries and\n$\\mathbb{Z}^2[A]$ be the smallest $A$-invariant $\\mathbb{Z}$-module containing\n$\\mathbb{Z}^2$. Let $\\mathcal{D}$ be a finite subset of $\\mathbb{Z}^2[A]$ which\nis a complete residue system of $\\mathbb{Z}^2[A]/A\\mathbb{Z}^2[A]$. The pair\n$(A,\\mathcal{D})$ is called a {\\em digit system} with {\\em base} $A$ and {\\em\ndigit set} $\\mathcal{D}$. It is well known that every vector $x \\in\n\\mathbb{Z}^2[A]$ can be written uniquely in the form \\[ x = d_0 + Ad_1 + \\cdots\n+ A^kd_k + A^{k+1}p, \\] with $k\\in \\mathbb{N}$ minimal, $d_0,\\dots,d_k \\in\n\\mathcal{D}$, and $p$ taken from a finite set of {\\em periodic elements}, the\nso-called {\\em attractor} of $(A,\\mathcal{D})$. If $p$ can always be chosen to\nbe $0$ we say that $(A,\\mathcal{D})$ has the {\\em finiteness property}.\n  In the present paper we introduce finite-state transducer automata which\nrealize the addition of the vectors $\\pm(1,0)^\\top$ and $\\pm(0,1)^\\top$ to a\ngiven vector $x\\in \\mathbb{Z}^2[A]$ in a number system $(A,\\mathcal{D})$ with\ncollinear digit set. These automata are applied to characterize all pairs\n$(A,\\mathcal{D})$ that have the finiteness property and, more generally, to\ncharacterize the attractors of these digit systems."}
{"id": "2507.05519", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05519", "abs": "https://arxiv.org/abs/2507.05519", "authors": ["Gopal Gupta", "Abhiramon Rajasekharan", "Alexis R. Tudor", "Elmer Salazar", "Joaquín Arias"], "title": "Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System", "comment": null, "summary": "We consider the problem of implementing deontic modal logic. We show how\n(deontic) modal operators can be expressed elegantly using default negation\n(negation-as-failure) and strong negation present in answer set programming\n(ASP). We propose using global constraints of ASP to represent obligations and\nimpermissibilities of deontic modal logic. We show that our proposed\nrepresentation results in the various paradoxes of deontic modal logic being\nelegantly resolved."}
{"id": "2507.06226", "categories": ["math.ST", "math.PR", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.06226", "abs": "https://arxiv.org/abs/2507.06226", "authors": ["Moïse Blanchard", "Adam Quinn Jaffe", "Nikita Zhivotovskiy"], "title": "Consistency and Inconsistency in $K$-Means Clustering", "comment": "36 pages, 1 figure, 1 table. Comments welcome", "summary": "A celebrated result of Pollard proves asymptotic consistency for $k$-means\nclustering when the population distribution has finite variance. In this work,\nwe point out that the population-level $k$-means clustering problem is, in\nfact, well-posed under the weaker assumption of a finite expectation, and we\ninvestigate whether some form of asymptotic consistency holds in this setting.\nAs we illustrate in a variety of negative results, the complete story is quite\nsubtle; for example, the empirical $k$-means cluster centers may fail to\nconverge even if there exists a unique set of population $k$-means cluster\ncenters. A detailed analysis of our negative results reveals that inconsistency\narises because of an extreme form of cluster imbalance, whereby the presence of\noutlying samples leads to some empirical $k$-means clusters possessing very few\npoints. We then give a collection of positive results which show that some\nforms of asymptotic consistency, under only the assumption of finite\nexpectation, may be recovered by imposing some a priori degree of balance among\nthe empirical $k$-means clusters."}
{"id": "2507.05576", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.05576", "abs": "https://arxiv.org/abs/2507.05576", "authors": ["Mehdi Elahi", "Mohamed R. Elshamy", "Abdel-Hameed Badawy", "Ahmad Patooghy"], "title": "iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips", "comment": null, "summary": "Thermal Trojan attacks present a pressing concern for the security and\nreliability of System-on-Chips (SoCs), especially in mobile applications. The\nsituation becomes more complicated when such attacks are more evasive and\noperate sporadically to stay hidden from detection mechanisms. In this paper,\nwe introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'\nthermal information in a random time-triggered manner. According to our\nexperiments, iThermTroj attack can easily bypass available threshold-based\nthermal Trojan detection solutions. We investigate SoC vulnerabilities to\nvariations of iThermTroj through an in-depth analysis of Trojan activation and\nduration scenarios. We also propose a set of tiny Machine Learning classifiers\nfor run-time anomaly detection to protect SoCs against such intermittent\nthermal Trojan attacks. Compared to existing methods, our approach improves the\nattack detection rate by 29.4\\%, 17.2\\%, and 14.3\\% in scenarios where\niThermTroj manipulates up to 80\\%, 60\\%, and 40\\% of SoC's thermal data,\nrespectively. Additionally, our method increases the full protection resolution\nto 0.8 degrees Celsius, meaning that any temperature manipulations exceeding\n$\\pm 0.8$ degrees will be detected with 100\\% accuracy."}
{"id": "2507.05827", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.05827", "abs": "https://arxiv.org/abs/2507.05827", "authors": ["G. Gutin", "M. A. Nielsen", "A. Yeo", "Y. Zhou"], "title": "Judicious Partitions in Edge-Weighted Graphs with Bounded Maximum Weighted Degree", "comment": null, "summary": "In this paper, we investigate bounds for the following judicious\n$k$-partitioning problem: Given an edge-weighted graph $G$, find a\n$k$-partition $(V_1,V_2,\\dots ,V_k)$ of $V(G)$ such that the total weight of\nedges in the heaviest induced subgraph, $\\max_{i=1}^k w(G[V_i])$, is minimized.\nIn our bounds, we also take into account the weight $w(V_1,V_2,\\dots,V_k)$ of\nthe cut induced by the partition (i.e., the total weight of edges with\nendpoints in different parts) and show the existence of a partition satisfying\ntight bounds for both quantities simultaneously. We establish such tight bounds\nfor the case $k=2$ and, to the best of our knowledge, present the first (even\nfor unweighted graphs) completely tight bound for $k=3$. We also show that, in\ngeneral, these results cannot be extended to $k \\geq 4$ without introducing an\nadditional lower-order term, and we propose a corresponding conjecture.\nMoreover, we prove that there always exists a $k$-partition satisfying $\\max\n\\left\\{ w(G[V_i]) : i \\in [k] \\right\\} \\leq \\frac{w(G)}{k^2} + \\frac{k -\n1}{2k^2} \\Delta_w(G),$ where $\\Delta_w(G)$ denotes the maximum weighted degree\nof $G$. This bound is tight for every integer $k\\geq 2$."}
{"id": "2507.05893", "categories": ["math.OC", "62G05, 90C35 (Primary) 62M20, 62M10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.05893", "abs": "https://arxiv.org/abs/2507.05893", "authors": ["Edward J. Anderson", "Dominic S. T. Keehan"], "title": "Nonstationary Distribution Estimation via Wasserstein Probability Flows", "comment": "28 pages, 8 figures", "summary": "We study the problem of estimating a sequence of evolving probability\ndistributions from historical data, where the underlying distribution changes\nover time in a nonstationary and nonparametric manner. To capture gradual\nchanges, we introduce a model that penalises large deviations between\nconsecutive distributions using the Wasserstein distance. This leads to a\nmethod in which we estimate the underlying series of distributions by\nmaximizing the log-likelihood of the observations with a penalty applied to the\nsum of the Wasserstein distances between consecutive distributions. We show how\nthis can be reduced to a simple network-flow problem enabling efficient\ncomputation. We call this the Wasserstein Probability Flow method. We derive\nsome properties of the optimal solutions and carry out numerical tests in\ndifferent settings. Our results suggest that the Wasserstein Probability Flow\nmethod is a promising tool for applications such as nonstationary stochastic\noptimization."}
{"id": "2507.06162", "categories": ["math.NT", "68Q45, 11B39", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.06162", "abs": "https://arxiv.org/abs/2507.06162", "authors": ["Rob Burns"], "title": "Shifting Zeckendorf and Chung-Graham representations", "comment": "17 pages, 3 figures", "summary": "We re-prove some results about integers whose Zeckendorf and Chung-Graham\nrepresentations satisfy certain conditions. We use properties of the shift\noperator and use the software package {\\tt Walnut}."}
{"id": "2507.05520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05520", "abs": "https://arxiv.org/abs/2507.05520", "authors": ["Karishma Thakrar", "Shreyas Basavatia", "Akshay Daftardar"], "title": "Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis", "comment": "2025 ImageCLEF MEDIQA-MAGIC Challenge", "summary": "The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized\nby researchers from Microsoft, Stanford University, and the Hospital Clinic of\nBarcelona, focuses on multimodal dermatology question answering and\nsegmentation, using real-world patient queries and images. This work addresses\nthe Closed Visual Question Answering (CVQA) task, where the goal is to select\nthe correct answer to multiple-choice clinical questions based on both\nuser-submitted images and accompanying symptom descriptions. The proposed\napproach combines three core components: (1) fine-tuning open-source multimodal\nmodels from the Qwen, Gemma, and LLaMA families on the competition dataset, (2)\nintroducing a structured reasoning layer that reconciles and adjudicates\nbetween candidate model outputs, and (3) incorporating agentic\nretrieval-augmented generation (agentic RAG), which adds relevant information\nfrom the American Academy of Dermatology's symptom and condition database to\nfill in gaps in patient context. The team achieved second place with a\nsubmission that scored sixth, demonstrating competitive performance and high\naccuracy. Beyond competitive benchmarks, this research addresses a practical\nchallenge in telemedicine: diagnostic decisions must often be made\nasynchronously, with limited input and with high accuracy and interpretability.\nBy emulating the systematic reasoning patterns employed by dermatologists when\nevaluating skin conditions, this architecture provided a pathway toward more\nreliable automated diagnostic support systems."}
{"id": "2507.05622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05622", "abs": "https://arxiv.org/abs/2507.05622", "authors": ["Shuo Shao", "Yiming Li", "Mengren Zheng", "Zhiyang Hu", "Yukun Chen", "Boheng Li", "Yu He", "Junfeng Guo", "Tianwei Zhang", "Dacheng Tao", "Zhan Qin"], "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective", "comment": null, "summary": "The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench."}
{"id": "2507.05836", "categories": ["math.CO", "05C45", "G.2.2"], "pdf": "https://arxiv.org/pdf/2507.05836", "abs": "https://arxiv.org/abs/2507.05836", "authors": ["Alexey Pokrovskiy", "Xiaoan Yang"], "title": "Hamiltonicity and structure of connected biclaw-free graphs", "comment": null, "summary": "We show that for sufficiently large $d$, every balanced bipartite, connected\nbiclaw-free graph with minimum degree $\\geq d$ is Hamiltonian. This confirms a\nconjecture of Flandrin, Fouquet, and Li."}
{"id": "2507.06012", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.06012", "abs": "https://arxiv.org/abs/2507.06012", "authors": ["Alberto Guastalla", "Roberto Aringhieri", "Pierre Hosteins"], "title": "Heuristic approaches for a new variant of the Team Orienteering Problem", "comment": null, "summary": "In this paper we tackle the Team Orienteering Problem with Service Times,\nMandatory Nodes and Incompatibilities, introduced in~\\cite{Guastalla2024} and\narising from two real-world healthcare applications. We propose two heuristic\nalgorithms in the form of a Variable Descent Neighbourhood algorithm and a\nmatheuristic based on a Cuts Separation approach. For the former, we also\nprovide a multi-thread version exploiting its intrinsic capability to be\nparallelised. Both algorithms include a specific heuristic routine to provide a\nstarting feasible solution, since finding a feasible solution has been proved\nto be NP-complete. The results of our heuristic algorithms are compared with an\nexact cutting plane approach and have complementary strengths and weaknesses.\nThey are also evaluated on existing TOP benchmarks against TOP state-of-the-art\nalgorithms, demonstrating their competitiveness on general grounds."}
{"id": "2507.05528", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05528", "abs": "https://arxiv.org/abs/2507.05528", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "comment": "14 pages", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced."}
{"id": "2507.05630", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05630", "abs": "https://arxiv.org/abs/2507.05630", "authors": ["Sarthak Choudhary", "Divyam Anshumaan", "Nils Palumbo", "Somesh Jha"], "title": "How Not to Detect Prompt Injections with an LLM", "comment": null, "summary": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, in which adversaries embed malicious instructions within seemingly\nbenign user inputs to manipulate the LLM's intended behavior. Recent defenses\nbased on $\\textit{known-answer detection}$ (KAD) have achieved near-perfect\nperformance by using an LLM to classify inputs as clean or contaminated. In\nthis work, we formally characterize the KAD framework and uncover a structural\nvulnerability in its design that invalidates its core security premise. We\ndesign a methodical adaptive attack, $\\textit{DataFlip}$, to exploit this\nfundamental weakness. It consistently evades KAD defenses with detection rates\nas low as $1.5\\%$ while reliably inducing malicious behavior with success rates\nof up to $88\\%$, without needing white-box access to the LLM or any\noptimization procedures."}
{"id": "2507.05842", "categories": ["math.CO", "05D15", "G.2.2"], "pdf": "https://arxiv.org/pdf/2507.05842", "abs": "https://arxiv.org/abs/2507.05842", "authors": ["Alexey Pokrovskiy"], "title": "Bounded diameter monochromatic component covers", "comment": null, "summary": "Ryser conjectured that every $r$-edge-coloured complete graph can be covered\nby $r-1$ monochromatic trees. Motivated by a question of Austin in analysis,\nMili\\'cevi\\'c predicted something stronger -- that every $r$-edge-coloured\ncomplete graph can be covered by $r-1$ monochromatic trees \\emph{of bounded\ndiameter}. Here we show that the two conjectures are equivalent. As immediate\ncorollaries we obtain new results about Mili\\'cevi\\'c's Conjecture, most\nnotably that it is true for $r=5$. We also obtain several new cases of a\ngeneralization of Mili\\'cevi\\'c's Conjecture to non-complete graphs due to\nDeBiasio-Kamel-McCourt-Sheats."}
{"id": "2507.06024", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.06024", "abs": "https://arxiv.org/abs/2507.06024", "authors": ["Michael Konopik", "Sigrid Leyendecker", "Sofya Maslovskaya", "Sina Ober-Blöbaum", "Rodrigo T. Sato Martín de Almagro"], "title": "New Lagrangian framework for optimality conditions in optimal control of second order systems", "comment": null, "summary": "It has been shown recently that optimal control problems with the dynamical\nconstraint given by a second order system admit a regular Lagrangian\nformulation. This implies that the optimality conditions can be obtained in a\nnew form based on the variational approach. In this paper we extend the first\norder necessary optimality conditions obtained previously to second order\noptimality conditions and discuss the role of the new Lagrangian."}
{"id": "2507.05538", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05538", "abs": "https://arxiv.org/abs/2507.05538", "authors": ["Subhabrata Majumdar", "Brian Pendleton", "Abhishek Gupta"], "title": "Red Teaming AI Red Teaming", "comment": null, "summary": "Red teaming has evolved from its origins in military applications to become a\nwidely adopted methodology in cybersecurity and AI. In this paper, we take a\ncritical look at the practice of AI red teaming. We argue that despite its\ncurrent popularity in AI governance, there exists a significant gap between red\nteaming's original intent as a critical thinking exercise and its narrow focus\non discovering model-level flaws in the context of generative AI. Current AI\nred teaming efforts focus predominantly on individual model vulnerabilities\nwhile overlooking the broader sociotechnical systems and emergent behaviors\nthat arise from complex interactions between models, users, and environments.\nTo address this deficiency, we propose a comprehensive framework\noperationalizing red teaming in AI systems at two levels: macro-level system\nred teaming spanning the entire AI development lifecycle, and micro-level model\nred teaming. Drawing on cybersecurity experience and systems theory, we further\npropose a set of recommendations. In these, we emphasize that effective AI red\nteaming requires multifunctional teams that examine emergent risks, systemic\nvulnerabilities, and the interplay between technical and social factors."}
{"id": "2507.05649", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05649", "abs": "https://arxiv.org/abs/2507.05649", "authors": ["Kaixiang Zhao", "Joseph Yousry Attalla", "Qian Lou", "Yushun Dong"], "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning", "comment": "Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics."}
{"id": "2507.05987", "categories": ["math.CO", "math.AG", "14T15, 14H40"], "pdf": "https://arxiv.org/pdf/2507.05987", "abs": "https://arxiv.org/abs/2507.05987", "authors": ["Felix Röhrle", "Thomas Saillez"], "title": "Tropical Donagi theorem", "comment": "33 pages", "summary": "The tropical $n$-gonal construction was introduced in recent work by the\nfirst author and D.~Zakharov and structural results for $n = 2,3$ were\nestablished. In this article we explore the construction for $n = 4$ and prove\na tropical analogue of Donagi's theorem which states that the tetragonal\nconstruction is a triality which preserves Prym varieties. This confirms the\nspeculations in previous work and establishes new results on the\nnon-injectivity of the tropical Prym-Torelli morphism. Finally, we demonstrate\nthat the tropical $n$-gonal construction is poorly behaved under edge\ncontractions, thus preventing any immediate moduli-theoretic perspective."}
{"id": "2507.06118", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.06118", "abs": "https://arxiv.org/abs/2507.06118", "authors": ["Ying Hu", "Guomin Liu", "Shanjian Tang"], "title": "Relationship between maximum principle and dynamic programming principle for recursive optimal control problem of stochastic evolution equations", "comment": null, "summary": "This paper aims to study the relationship between the maximum principle and\nthe dynamic programming principle for recursive optimal control problem of\nstochastic evolution equations, where the control domain is not necessarily\nconvex and the value function may be nonsmooth. By making use of the notion of\nconditionally expected operator-valued backward stochastic integral equations,\nwe establish a connection between the first and second-order adjoint processes\nin MP and the general derivatives of the value function. Under certain\nadditional assumptions, the value function is shown to be $C^{1,1}$-regular.\nFurthermore, we discuss the smooth case and present several applications of our\nresults."}
{"id": "2507.05541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05541", "abs": "https://arxiv.org/abs/2507.05541", "authors": ["Shovito Barua Soumma", "Asiful Arefeen", "Stephanie M. Carpenter", "Melanie Hingle", "Hassan Ghasemzadeh"], "title": "SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation", "comment": "In review", "summary": "Counterfactual explanations (CFs) offer human-centric insights into machine\nlearning predictions by highlighting minimal changes required to alter an\noutcome. Therefore, CFs can be used as (i) interventions for abnormality\nprevention and (ii) augmented data for training robust models. In this work, we\nexplore large language models (LLMs), specifically GPT-4o-mini, for generating\nCFs in a zero-shot and three-shot setting. We evaluate our approach on two\ndatasets: the AI-Readi flagship dataset for stress prediction and a public\ndataset for heart disease detection. Compared to traditional methods such as\nDiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high\nplausibility (up to 99%), strong validity (up to 0.99), and competitive\nsparsity. Moreover, using LLM-generated CFs as augmented samples improves\ndownstream classifier performance (an average accuracy gain of 5%), especially\nin low-data regimes. This demonstrates the potential of prompt-based generative\ntechniques to enhance explainability and robustness in clinical and\nphysiological prediction tasks. Code base: github.com/anonymous/SenseCF."}
{"id": "2507.05660", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05660", "abs": "https://arxiv.org/abs/2507.05660", "authors": ["Aravind Cheruvu", "Shravya Kanchi", "Sifat Muhammad Abdullah", "Nicholas Kong", "Daphne Yao", "Murtuza Jadliwala", "Bimal Viswanath"], "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data", "comment": "Pre-print", "summary": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL)."}
{"id": "2507.06120", "categories": ["math.CO", "05E45, 52B35, 52B12"], "pdf": "https://arxiv.org/pdf/2507.06120", "abs": "https://arxiv.org/abs/2507.06120", "authors": ["Shuai Huang", "Jasper Miller", "Daniel Rose-Levine", "Steven Simon"], "title": "A non-face characterization of spheres on few vertices", "comment": "6 pages", "summary": "We give a relatively simple characterization of simplicial $d$-spheres on\n$d+4$ vertices; our criteria are in terms of the intersection patterns of a\nsimplicial complex's minimal non-faces. Namely, let $\\Sigma$ be a simplicial\ncomplex with vertex set $[d+4]$ and let $\\mathcal{F}=\\{A_0,\\ldots, A_{n-1}\\}$\nbe its family of minimal non-faces, indices taken in $\\mathbb{Z}_n$. Then\n$\\Sigma$ is a $d$-sphere if and only if the following hold: $n\\geq 3$ is odd,\nsuccessive $A_i$ are disjoint, and the alternating $(\\frac{n-1}{2})$-fold\nintersections $A_i\\cap A_{i+2} \\cap A_{i+4} \\cap \\cdots \\cap A_{i+n-3}$\npartition $[d+4]$."}
{"id": "2507.06199", "categories": ["math.OC", "cs.NA", "math.NA", "90C55, 65K05, 49M37, 49M41"], "pdf": "https://arxiv.org/pdf/2507.06199", "abs": "https://arxiv.org/abs/2507.06199", "authors": ["Dane S. Grundvig", "Matthias Heinkenschloss"], "title": "A Generalized $\\ell_1$-Merit Function SQP Method Using Function Approximations with Tunable Accuracy", "comment": null, "summary": "This paper develops a generalization of the line-search sequential quadratic\nprogramming (SQP) algorithm with $\\ell_1$-merit function that uses objective\nand constraint function approximations with tunable accuracy to solve smooth\nequality-constrained optimization problems. The evaluation of objective and\nconstraint functions and their gradients is potentially computationally\nexpensive, but it is assumed that one can construct effective, computationally\ninexpensive models of these functions. This paper specifies how these models\ncan be used to generate new iterates. At each iteration, the models have to\nsatisfy function error and relative gradient error tolerances determined by the\nalgorithm based on its progress. Moreover, bounds for the model errors are used\nto explore regions where the combined objective function and constraint models\nare sufficiently accurate. The algorithm has the same first-order global\nconvergence properties as a line-search SQP algorithm with $\\ell_1$-merit\nfunction, but only uses objective and constraint function models and the model\nerror bounds. The algorithm is applied to a discretized boundary control\nproblem in which the evaluation of the objective and constraint functions\nrequires the solution of the Boussinesq partial differential equation (PDE).\nThe models are constructed from projection-based reduced-order models of the\nBoussinesq PDE."}
{"id": "2507.05566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05566", "abs": "https://arxiv.org/abs/2507.05566", "authors": ["David Bensaïd", "Noam Rotstein", "Roy Velich", "Daniel Bensaïd", "Ron Kimmel"], "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively."}
{"id": "2507.05683", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.05683", "abs": "https://arxiv.org/abs/2507.05683", "authors": ["Steven Duplij", "Qiang Guo"], "title": "Polyadic encryption", "comment": "revtex 4.2, 9 pages", "summary": "A novel original procedure of encryption/decryption based on the polyadic\nalgebraic structures and on signal processing methods is proposed. First, we\nuse signals with integer amplitudes to send information. Then we use polyadic\ntechniques to transfer the plaintext into series of special integers. The\nreceiver restores the plaintext using special rules and systems of equations."}
{"id": "2507.06169", "categories": ["math.CO", "cs.DM", "05C75 (Primary) 05C83 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.06169", "abs": "https://arxiv.org/abs/2507.06169", "authors": ["Maria Chudnovsky", "David Fischer", "Sepehr Hajebi", "Sophie Spirkl", "Bartosz Walczak"], "title": "A simple layered-wheel-like construction", "comment": "16 pages, 2 figures", "summary": "In recent years, there has been significant interest in characterizing the\ninduced subgraph obstructions to bounded treewidth and pathwidth. While this\nhas recently been resolved for pathwidth, the case of treewidth remains open,\nand prior work has reduced the problem to understanding the layered-wheel-like\nobstructions -- graphs that contain large complete minor models with each\nbranching set inducing a path; exclude large walls as induced minors; exclude\nlarge complete bipartite graphs as induced minors; and exclude large complete\nsubgraphs.\n  There are various constructions of such graphs, but they are all rather\ninvolved. In this paper, we present a simple construction of layered-wheel-like\ngraphs with arbitrarily large treewidth. Three notable features of our\nconstruction are: (a) the vertices of degree at least four can be made to be\narbitrarily far apart; (b) the girth can be made to be arbitrarily large; and\n(c) every outerstring induced subgraph of the graphs from our construction has\ntreewidth bounded by an absolute constant. In contrast, among several\npreviously known constructions of layered wheels, none achieves (a); at most\none satisfies either (b) or (c); and none satisfies both (b) and (c)\nsimultaneously.\n  In particular, this is related to a former conjecture of Trotignon, that\nevery graph with large enough treewidth, excluding large walls and large\ncomplete bipartite graphs as induced minors, and large complete subgraphs, must\ncontain an outerstring induced subgraph of large treewidth. Our construction\nprovides the first counterexample to this conjecture that can also be made to\nhave arbitrarily large girth."}
{"id": "2507.05587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05587", "abs": "https://arxiv.org/abs/2507.05587", "authors": ["Elija Perrier"], "title": "Towards Measurement Theory for Artificial Intelligence", "comment": "Under review for Iliad Conference 2025", "summary": "We motivate and outline a programme for a formal theory of measurement of\nartificial intelligence. We argue that formalising measurement for AI will\nallow researchers, practitioners, and regulators to: (i) make comparisons\nbetween systems and the evaluation methods applied to them; (ii) connect\nfrontier AI evaluations with established quantitative risk analysis techniques\ndrawn from engineering and safety science; and (iii) foreground how what counts\nas AI capability is contingent upon the measurement operations and scales we\nelect to use. We sketch a layered measurement stack, distinguish direct from\nindirect observables, and signpost how these ingredients provide a pathway\ntoward a unified, calibratable taxonomy of AI phenomena."}
{"id": "2507.05728", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05728", "abs": "https://arxiv.org/abs/2507.05728", "authors": ["Ruofei Wang", "Peiqi Duan", "Boxin Shi", "Renjie Wan"], "title": "Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset", "comment": "Accepted by ICCV2025", "summary": "With more event datasets being released online, safeguarding the event\ndataset against unauthorized usage has become a serious concern for data\nowners. Unlearnable Examples are proposed to prevent the unauthorized\nexploitation of image datasets. However, it's unclear how to create unlearnable\nasynchronous event streams to prevent event misuse. In this work, we propose\nthe first unlearnable event stream generation method to prevent unauthorized\ntraining from event datasets. A new form of asynchronous event error-minimizing\nnoise is proposed to perturb event streams, tricking the unauthorized model\ninto learning embedded noise instead of realistic features. To be compatible\nwith the sparse event, a projection strategy is presented to sparsify the noise\nto render our unlearnable event streams (UEvs). Extensive experiments\ndemonstrate that our method effectively protects event data from unauthorized\nexploitation, while preserving their utility for legitimate use. We hope our\nUEvs contribute to the advancement of secure and trustworthy event dataset\nsharing. Code is available at: https://github.com/rfww/uevs."}
{"id": "2507.06184", "categories": ["math.CO", "05C50"], "pdf": "https://arxiv.org/pdf/2507.06184", "abs": "https://arxiv.org/abs/2507.06184", "authors": ["Fenglei Tian", "Dein Wong"], "title": "On the multiplicity of 1 as a Laplacian eigenvalue of a graph", "comment": null, "summary": "Let $G$ be a graph with $p(G)$ pendant vertices and $q(G)$ quasi-pendant\nvertices. Denote by $m_{L(G)}(\\lambda)$ the multiplicity of $\\lambda$ as a\nLaplacian eigenvalue of $G$. Let $\\overline{G}$ be the reduced graph of $G$,\nwhich can be obtained from $G$ by deleting some pendant vertices such that\n$p(\\overline{G})=q(\\overline{G})$. We first prove that\n$m_{L(G)}(1)=p(G)-q(G)+m_{L(\\overline{G})}(1)$. Since deleting pendant path\n$P_3$ does not change the multiplicity of Laplacian eigenvalue 1 of a graph, we\nfurther focus on reduced graphs without pendant path $P_3$. Let $T$ be a\nreduced tree on $n(\\geq 6)$ vertices without pendant path $P_3$, then it is\nproved that $$m_{L(T)}(1)\\leq \\frac{n-6}{4},$$ and all the trees attaining the\nupper bound are characterized completely. As an application, for a reduced\nunicyclic graph $G$ of order $n\\geq 10$ without pendant path $P_3$, we get\n$$m_{L(G)}(1)\\leq \\frac{n}{4},$$ and all the unicyclic graphs attaining the\nupper bound are determined completely."}
{"id": "2507.05591", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05591", "abs": "https://arxiv.org/abs/2507.05591", "authors": ["Wei Zhang", "Juan Chen", "En Zhu", "Wenhong Cheng", "YunPeng Li", "Yanbo J. Wang"], "title": "MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models", "comment": null, "summary": "Automated depression diagnosis aims to analyze multimodal information from\ninterview videos to predict participants' depression scores. Previous studies\noften lack clear explanations of how these scores were determined, limiting\ntheir adoption in clinical practice. While the advent of LLMs provides a\npossible pathway for explainable depression diagnosis, current LLMs capable of\nprocessing multimodal data lack training on interview data, resulting in poor\ndiagnostic performance when used directly. In this paper, we propose a novel\nmultimodal large language model (MLlm-DR) that can understand multimodal\ninformation inputs and supports explainable depression diagnosis. MLlm-DR\nintegrates a smaller LLMs and a lightweight query module (LQ-former).\nSpecifically, the smaller LLMs is designed to generate depression scores and\ncorresponding evaluation rationales. To enhance its logical reasoning for\ndomain-specific tasks while maintaining practicality, we constructed a robust\ntraining dataset to fine-tune it. Meanwhile, the LQ-former captures\ndepression-related features from speech and visual data, aiding the model's\nability to process multimodal information, to achieve comprehensive depression\ndiagnosis. Our approach achieves state-of-the-art results on two\ninterview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its\neffectiveness and superiority."}
{"id": "2507.05794", "categories": ["cs.CR", "cs.AI", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05794", "abs": "https://arxiv.org/abs/2507.05794", "authors": ["Avi Shaked", "Nan Messe"], "title": "Automated Reasoning for Vulnerability Management by Design", "comment": null, "summary": "For securing systems, it is essential to manage their vulnerability posture\nand design appropriate security controls. Vulnerability management allows to\nproactively address vulnerabilities by incorporating pertinent security\ncontrols into systems designs. Current vulnerability management approaches do\nnot support systematic reasoning about the vulnerability postures of systems\ndesigns. To effectively manage vulnerabilities and design security controls, we\npropose a formally grounded automated reasoning mechanism. We integrate the\nmechanism into an open-source security design tool and demonstrate its\napplication through an illustrative example driven by real-world challenges.\nThe automated reasoning mechanism allows system designers to identify\nvulnerabilities that are applicable to a specific system design, explicitly\nspecify vulnerability mitigation options, declare selected controls, and thus\nsystematically manage vulnerability postures."}
{"id": "2507.06220", "categories": ["math.CO", "math.RT", "05E10 (Primary) 05E05 20C15 05A30 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.06220", "abs": "https://arxiv.org/abs/2507.06220", "authors": ["Álvaro Gutiérrez", "Michał Szwej"], "title": "A proof of the $q$-Foulkes conjecture for Gaussian coefficients when $a$ divides $c$", "comment": "14 pages, 1 figure. Comments welcome", "summary": "Foulkes' conjecture has several generalisations due to Doran,\nAbdesselam--Chipalkatti, Bergeron, and Troyka. For the special linear Lie\nalgebra $\\mathfrak{sl}_2(\\mathbb{C})$, these assert that given $a \\le c \\le d\n\\le b$ with $ab=cd$, the $\\mathfrak{sl}_2(\\mathbb{C})$-representation\n$\\mathrm{Sym}^a\\mathrm{Sym}^b\\mathbb{C}^2$ is a subrepresentation of\n$\\mathrm{Sym}^c\\mathrm{Sym}^d\\mathbb{C}^2$. We present a short proof in the\ncase where $a$ divides $c$ or $d$, which includes all prime values of $a$. This\nis the first proof in this family of conjectures valid for infinitely many\nvalues of $a$; previously only the cases $a=2$ and $a=3$ were known."}
{"id": "2507.05613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05613", "abs": "https://arxiv.org/abs/2507.05613", "authors": ["Lei Fan", "Fangxue Liu", "Cheng Chen"], "title": "Domain adaptation of large language models for geotechnical applications", "comment": null, "summary": "Recent developments in large language models (LLMs) are opening up new\nopportunities in geotechnical engineering and engineering geology. While\ngeneral-purpose LLMs possess broad capabilities, effective application in\ngeotechnics often requires domain-specific adaptation. Such tailored LLMs are\nincreasingly employed to streamline geotechnical workflows. This paper presents\nthe first survey of the adaptation and application of LLMs in geotechnical\nengineering. It outlines key methodologies for adaptation to geotechnical\ndomain, including prompt engineering, retrieval-augmented generation,\ndomain-adaptive pretraining, and fine-tuning. The survey examines the\nstate-of-the-art applications of geotechnical-adapted LLMs, including\ngeological interpretation, subsurface characterization, site planning, design\ncalculations, numerical modeling, safety and risk assessment, and educational\ntutoring. It also analyzes benefits and limitations of geotechnical-adapted\nLLMs, and identifies promising directions for future research in this\ninterdisciplinary discipline. The findings serve as a valuable resource for\npractitioners seeking to integrate LLMs into geotechnical practice, while also\nproviding a foundation to stimulate further investigation within the academic\ncommunity."}
{"id": "2507.05872", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05872", "abs": "https://arxiv.org/abs/2507.05872", "authors": ["Berkay Kemal Balioglu", "Alireza Khodaie", "Mehmet Emre Gursoy"], "title": "LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods", "comment": null, "summary": "Local differential privacy (LDP) has become a prominent notion for\nprivacy-preserving data collection. While numerous LDP protocols and\npost-processing (PP) methods have been developed, selecting an optimal\ncombination under different privacy budgets and datasets remains a challenge.\nMoreover, the lack of a comprehensive and extensible LDP benchmarking toolkit\nraises difficulties in evaluating new protocols and PP methods. To address\nthese concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an\nopen-source, extensible, and multi-threaded toolkit for LDP researchers and\npractitioners. LDP$^3$ contains implementations of several LDP protocols, PP\nmethods, and utility metrics in a modular and extensible design. Its modular\ndesign enables developers to conveniently integrate new protocols and PP\nmethods. Furthermore, its multi-threaded nature enables significant reductions\nin execution times via parallelization. Experimental evaluations demonstrate\nthat: (i) using LDP$^3$ to select a good protocol and post-processing method\nsubstantially improves utility compared to a bad or random choice, and (ii) the\nmulti-threaded design of LDP$^3$ brings substantial benefits in terms of\nefficiency."}
{"id": "2507.05624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05624", "abs": "https://arxiv.org/abs/2507.05624", "authors": ["Wei Zhang", "Juan Chen", "Yanbo J. Wang", "En Zhu", "Xuan Yang", "Yiduo Wang"], "title": "ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion", "comment": null, "summary": "Multimodal emotion and intent recognition is essential for automated\nhuman-computer interaction, It aims to analyze users' speech, text, and visual\ninformation to predict their emotions or intent. One of the significant\nchallenges is that missing modalities due to sensor malfunctions or incomplete\ndata. Traditional methods that attempt to reconstruct missing information often\nsuffer from over-coupling and imprecise generation processes, leading to\nsuboptimal outcomes. To address these issues, we introduce an Attention-based\nDiffusion model for Missing Modalities feature Completion (ADMC). Our framework\nindependently trains feature extraction networks for each modality, preserving\ntheir unique characteristics and avoiding over-coupling. The Attention-based\nDiffusion Network (ADN) generates missing modality features that closely align\nwith authentic multimodal distribution, enhancing performance across all\nmissing-modality scenarios. Moreover, ADN's cross-modal generation offers\nimproved recognition even in full-modality contexts. Our approach achieves\nstate-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating\nits effectiveness in both missing and complete modality scenarios."}
{"id": "2507.05875", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05875", "abs": "https://arxiv.org/abs/2507.05875", "authors": ["Alireza Khodaie", "Berkay Kemal Balioglu", "Mehmet Emre Gursoy"], "title": "Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform", "comment": null, "summary": "Local differential privacy (LDP) has recently gained prominence as a powerful\nparadigm for collecting and analyzing sensitive data from users' devices.\nHowever, the inherent perturbation added by LDP protocols reduces the utility\nof the collected data. To mitigate this issue, several post-processing (PP)\nmethods have been developed. Yet, the comparative performance of PP methods\nunder diverse settings remains underexplored. In this paper, we present an\nextensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility\nmetrics, and 6 datasets to evaluate the behaviors and optimality of PP methods\nunder diverse conditions. Through extensive experiments, we show that while PP\ncan substantially improve utility when the privacy budget is small (i.e.,\nstrict privacy), its benefit diminishes as the privacy budget grows. Moreover,\nour findings reveal that the optimal PP method depends on multiple factors,\nincluding the choice of LDP protocol, privacy budget, data characteristics\n(such as distribution and domain size), and the specific utility metric. To\nadvance research in this area and assist practitioners in identifying the most\nsuitable PP method for their setting, we introduce LDP$^3$, an open-source\nbenchmark platform. LDP$^3$ contains all methods used in our experimental\nanalysis, and it is designed in a modular, extensible, and multi-threaded way\nfor future use and development."}
{"id": "2507.05629", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05629", "abs": "https://arxiv.org/abs/2507.05629", "authors": ["Yuan An", "John Liu", "Niyam Acharya", "Ruhma Hashmi"], "title": "Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses", "comment": null, "summary": "Retrieval practice is a well-established pedagogical technique known to\nsignificantly enhance student learning and knowledge retention. However,\ngenerating high-quality retrieval practice questions is often time-consuming\nand labor intensive for instructors, especially in rapidly evolving technical\nsubjects. Large Language Models (LLMs) offer the potential to automate this\nprocess by generating questions in response to prompts, yet the effectiveness\nof LLM-generated retrieval practice on student learning remains to be\nestablished. In this study, we conducted an empirical study involving two\ncollege-level data science courses, with approximately 60 students. We compared\nlearning outcomes during one week in which students received LLM-generated\nmultiple-choice retrieval practice questions to those from a week in which no\nsuch questions were provided. Results indicate that students exposed to\nLLM-generated retrieval practice achieved significantly higher knowledge\nretention, with an average accuracy of 89%, compared to 73% in the week without\nsuch practice. These findings suggest that LLM-generated retrieval questions\ncan effectively support student learning and may provide a scalable solution\nfor integrating retrieval practice into real-time teaching. However, despite\nthese encouraging outcomes and the potential time-saving benefits, cautions\nmust be taken, as the quality of LLM-generated questions can vary. Instructors\nmust still manually verify and revise the generated questions before releasing\nthem to students."}
{"id": "2507.06008", "categories": ["cs.CR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06008", "abs": "https://arxiv.org/abs/2507.06008", "authors": ["Jungeun Lim", "Stephan A. Fahrenkrog-Petersen", "Xixi Lu", "Jan Mendling", "Minseok Song"], "title": "The Impact of Event Data Partitioning on Privacy-aware Process Discovery", "comment": null, "summary": "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques."}
{"id": "2507.05638", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.05638", "abs": "https://arxiv.org/abs/2507.05638", "authors": ["Litian Zhang", "Xiaoming Zhang", "Bingyu Yan", "Ziyi Zhou", "Bo Zhang", "Zhenyu Guan", "Xi Zhang", "Chaozhuo Li"], "title": "LLMs are Introvert", "comment": null, "summary": "The exponential growth of social media and generative AI has transformed\ninformation dissemination, fostering connectivity but also accelerating the\nspread of misinformation. Understanding information propagation dynamics and\ndeveloping effective control strategies is essential to mitigate harmful\ncontent. Traditional models, such as SIR, provide basic insights but\ninadequately capture the complexities of online interactions. Advanced methods,\nincluding attention mechanisms and graph neural networks, enhance accuracy but\ntypically overlook user psychology and behavioral dynamics. Large language\nmodels (LLMs), with their human-like reasoning, offer new potential for\nsimulating psychological aspects of information spread. We introduce an\nLLM-based simulation environment capturing agents' evolving attitudes,\nemotions, and responses. Initial experiments, however, revealed significant\ngaps between LLM-generated behaviors and authentic human dynamics, especially\nin stance detection and psychological realism. A detailed evaluation through\nSocial Information Processing Theory identified major discrepancies in\ngoal-setting and feedback evaluation, stemming from the lack of emotional\nprocessing in standard LLM training. To address these issues, we propose the\nSocial Information Processing-based Chain of Thought (SIP-CoT) mechanism\nenhanced by emotion-guided memory. This method improves the interpretation of\nsocial cues, personalization of goals, and evaluation of feedback. Experimental\nresults confirm that SIP-CoT-enhanced LLM agents more effectively process\nsocial information, demonstrating behaviors, attitudes, and emotions closer to\nreal human interactions. In summary, this research highlights critical\nlimitations in current LLM-based propagation simulations and demonstrates how\nintegrating SIP-CoT and emotional memory significantly enhances the social\nintelligence and realism of LLM agents."}
{"id": "2507.06039", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06039", "abs": "https://arxiv.org/abs/2507.06039", "authors": ["Oleksii Oleksenko", "Flavien Solt", "Cédric Fournet", "Jana Hofmann", "Boris Köpf", "Stavros Volos"], "title": "Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks", "comment": "Accepted at IEEE SP 2025; delayed due to embargo; to appear at IEEE\n  SP 2026", "summary": "CPUs provide isolation mechanisms like virtualization and privilege levels to\nprotect software. Yet these focus on architectural isolation while typically\noverlooking microarchitectural side channels, exemplified by Meltdown and\nForeshadow. Software must therefore supplement architectural defenses with\nad-hoc microarchitectural patches, which are constantly evolving as new attacks\nemerge and defenses are proposed. Such reactive approach makes ensuring\ncomplete isolation a daunting task, and leaves room for errors and oversights.\n  We address this problem by developing a tool that stress tests\nmicroarchitectural isolation between security domains such as virtual machines,\nkernel, and processes, with the goal of detecting flaws in the isolation\nboundaries. The tool extends model-based relational testing (MRT) methodology\nto enable detection of cross-domain information leakage. We design a new test\ncase generator and execution sandbox to handle multi-domain execution, new\nleakage models to encode expected leaks, and new analysis techniques to manage\nnondeterminism.\n  We use this tool to perform an in-depth testing campaign on six x86-64 CPUs\nfor leakage across different isolation boundaries. The testing campaign exposed\nfour new leaks and corroborated numerous known ones, with only two false\npositives throughout the entire campaign. These results show critical gaps in\ncurrent isolation mechanisms as well as validate a robust methodology for\ndetecting microarchitectural flaws. As such, this approach enables a shift from\nreactive patching to proactive security validation in processor design."}
{"id": "2507.05651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05651", "abs": "https://arxiv.org/abs/2507.05651", "authors": ["Tianxing Wu", "Lizhe Cao", "Shuang Wang", "Jiming Wang", "Shutong Zhu", "Yerong Wu", "Yuqing Feng"], "title": "City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data", "comment": "9 pages, accepted by IJCAI 2025", "summary": "To advance the United Nations Sustainable Development Goal on promoting\nsustained, inclusive, and sustainable economic growth, foreign direct\ninvestment (FDI) plays a crucial role in catalyzing economic expansion and\nfostering innovation. Precise city-level FDI prediction is quite important for\nlocal government and is commonly studied based on economic data (e.g., GDP).\nHowever, such economic data could be prone to manipulation, making predictions\nless reliable. To address this issue, we try to leverage large-scale judicial\ndata which reflects judicial performance influencing local investment security\nand returns, for city-level FDI prediction. Based on this, we first build an\nindex system for the evaluation of judicial performance over twelve million\npublicly available adjudication documents according to which a tabular dataset\nis reformulated. We then propose a new Tabular Learning method on Judicial Data\n(TLJD) for city-level FDI prediction. TLJD integrates row data and column data\nin our built tabular dataset for judicial performance indicator encoding, and\nutilizes a mixture of experts model to adjust the weights of different\nindicators considering regional variations. To validate the effectiveness of\nTLJD, we design cross-city and cross-time tasks for city-level FDI predictions.\nExtensive experiments on both tasks demonstrate the superiority of TLJD (reach\nto at least 0.92 R2) over the other ten state-of-the-art baselines in different\nevaluation metrics."}
{"id": "2507.06043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06043", "abs": "https://arxiv.org/abs/2507.06043", "authors": ["Xiaohu Li", "Yunfeng Ning", "Zepeng Bao", "Mayi Xu", "Jianhao Chen", "Tieyun Qian"], "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations", "comment": null, "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."}
{"id": "2507.05716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05716", "abs": "https://arxiv.org/abs/2507.05716", "authors": ["Dipayan Sengupta", "Saumya Panda"], "title": "Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology", "comment": "13 pages, 3 tables", "summary": "Background: Evaluating AI-generated treatment plans is a key challenge as AI\nexpands beyond diagnostics, especially with new reasoning models. This study\ncompares plans from human experts and two AI models (a generalist and a\nreasoner), assessed by both human peers and a superior AI judge.\n  Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI\n(o3) generated treatment plans for five complex dermatology cases. The\nanonymized, normalized plans were scored in two phases: 1) by the ten human\nexperts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical\nrubric.\n  Results: A profound 'evaluator effect' was observed. Human experts scored\npeer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16;\np=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th\n(mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI\nplans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It\nranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.\n  Conclusions: The perceived quality of a clinical plan is fundamentally\ndependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by\nhuman experts, was judged as superior by a sophisticated AI, revealing a deep\ngap between experience-based clinical heuristics and data-driven algorithmic\nlogic. This paradox presents a critical challenge for AI integration,\nsuggesting the future requires synergistic, explainable human-AI systems that\nbridge this reasoning gap to augment clinical care."}
{"id": "2507.06064", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06064", "abs": "https://arxiv.org/abs/2507.06064", "authors": ["Oleksandr Kurbatov", "Kyrylo Baybula", "Yaroslava Chopa", "Sergey Kozlov", "Oleg Komendant", "Illia Dovgopoly", "Dmitrii Kurbatov", "Zakhar Naumets", "Yulia Artikulova", "Pavel Kravchenko", "Volodymyr Dubinin", "Lasha Antadze", "Yaroslav Panasenko", "Mykhailo Velykodnyi"], "title": "Wrapless: The trustless lending protocol on top of Bitcoin", "comment": null, "summary": "This paper presents Wrapless -- a lending protocol that enables the\ncollateralization of bitcoins without requiring a trusted wrapping mechanism.\nThe protocol facilitates a \"loan channel\" on the Bitcoin blockchain, allowing\nbitcoins to be locked as collateral for loans issued on any blockchain that\nsupports Turing-complete smart contracts. The protocol is designed in a way\nthat makes it economically irrational for each involved party to manipulate the\nloan rules. There is still a significant research area to bring the protocol\ncloser to traditional AMM financial instruments."}
{"id": "2507.05755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05755", "abs": "https://arxiv.org/abs/2507.05755", "authors": ["Lukas Kuhn", "Florian Buettner"], "title": "An autonomous agent for auditing and improving the reliability of clinical AI models", "comment": null, "summary": "The deployment of AI models in clinical practice faces a critical challenge:\nmodels achieving expert-level performance on benchmarks can fail\ncatastrophically when confronted with real-world variations in medical imaging.\nMinor shifts in scanner hardware, lighting or demographics can erode accuracy,\nbut currently reliability auditing to identify such catastrophic failure cases\nbefore deployment is a bespoke and time-consuming process. Practitioners lack\naccessible and interpretable tools to expose and repair hidden failure modes.\nHere we introduce ModelAuditor, a self-reflective agent that converses with\nusers, selects task-specific metrics, and simulates context-dependent,\nclinically relevant distribution shifts. ModelAuditor then generates\ninterpretable reports explaining how much performance likely degrades during\ndeployment, discussing specific likely failure modes and identifying root\ncauses and mitigation strategies. Our comprehensive evaluation across three\nreal-world clinical scenarios - inter-institutional variation in\nhistopathology, demographic shifts in dermatology, and equipment heterogeneity\nin chest radiography - demonstrates that ModelAuditor is able correctly\nidentify context-specific failure modes of state-of-the-art models such as the\nestablished SIIM-ISIC melanoma classifier. Its targeted recommendations recover\n15-25% of performance lost under real-world distribution shift, substantially\noutperforming both baseline models and state-of-the-art augmentation methods.\nThese improvements are achieved through a multi-agent architecture and execute\non consumer hardware in under 10 minutes, costing less than US$0.50 per audit."}
{"id": "2507.06092", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06092", "abs": "https://arxiv.org/abs/2507.06092", "authors": ["Shravya Kanchi", "Neal Mangaokar", "Aravind Cheruvu", "Sifat Muhammad Abdullah", "Shirin Nilizadeh", "Atul Prakash", "Bimal Viswanath"], "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI", "comment": null, "summary": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks."}
{"id": "2507.05765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05765", "abs": "https://arxiv.org/abs/2507.05765", "authors": ["Bruno Jammes", "Edgar Hernando Sepúlveda-Oviedo", "Corinne Alonso"], "title": "Real-time monitoring of the SoH of lithium-ion batteries", "comment": "in French language, Symposium de G{\\'e}nie {\\'E}lectrique SGE 2025,\n  Jul 2025, Toulouse, France", "summary": "Real-time monitoring of the state of health (SoH) of batteries remains a\nmajor challenge, particularly in microgrids where operational constraints limit\nthe use of traditional methods. As part of the 4BLife project, we propose an\ninnovative method based on the analysis of a discharge pulse at the end of the\ncharge phase. The parameters of the equivalent electrical model describing the\nvoltage evolution across the battery terminals during this current pulse are\nthen used to estimate the SoH. Based on the experimental data acquired so far,\nthe initial results demonstrate the relevance of the proposed approach. After\ntraining using the parameters of two batteries with a capacity degradation of\naround 85%, we successfully predicted the degradation of two other batteries,\ncycled down to approximately 90% SoH, with a mean absolute error of around 1%\nin the worst case, and an explainability score of the estimator close to 0.9.\nIf these performances are confirmed, this method can be easily integrated into\nbattery management systems (BMS) and paves the way for optimized battery\nmanagement under continuous operation."}
{"id": "2507.06112", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.06112", "abs": "https://arxiv.org/abs/2507.06112", "authors": ["Antoine Geimer", "Clementine Maurice"], "title": "Fun with flags: How Compilers Break and Fix Constant-Time Code", "comment": "11 pages", "summary": "Developers rely on constant-time programming to prevent timing side-channel\nattacks. But these efforts can be undone by compilers, whose optimizations may\nsilently reintroduce leaks. While recent works have measured the extent of such\nleakage, they leave developers without actionable insights: which optimization\npasses are responsible, and how to disable them without modifying the compiler\nremains unclear.\n  In this paper, we conduct a qualitative analysis of how compiler\noptimizations break constant-time code. We construct a dataset of\ncompiler-introduced constant-time violations and analyze the internals of two\nwidely used compilers, GCC and LLVM, to identify the specific optimization\npasses responsible. Our key insight is that a small set of passes are at the\nroot of most leaks. To the best of our knowledge, we are also the first to\ncharacterize how the interactions between these passes contribute to leakage.\nBased on this analysis, we propose an original and practical mitigation that\nrequires no source code modification or custom compiler: disabling selected\noptimization passes via compiler flags. We show that this approach\nsignificantly reduces leakage with minimal performance overhead, offering an\nimmediately deployable defense for developers."}
{"id": "2507.05791", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05791", "abs": "https://arxiv.org/abs/2507.05791", "authors": ["Yan Yang", "Dongxu Li", "Yutong Dai", "Yuhao Yang", "Ziyang Luo", "Zirui Zhao", "Zhiyuan Hu", "Junzhe Huang", "Amrita Saha", "Zeyuan Chen", "Ran Xu", "Liyuan Pan", "Caiming Xiong", "Junnan Li"], "title": "GTA1: GUI Test-time Scaling Agent", "comment": null, "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here."}
{"id": "2507.05538", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.05538", "abs": "https://arxiv.org/abs/2507.05538", "authors": ["Subhabrata Majumdar", "Brian Pendleton", "Abhishek Gupta"], "title": "Red Teaming AI Red Teaming", "comment": null, "summary": "Red teaming has evolved from its origins in military applications to become a\nwidely adopted methodology in cybersecurity and AI. In this paper, we take a\ncritical look at the practice of AI red teaming. We argue that despite its\ncurrent popularity in AI governance, there exists a significant gap between red\nteaming's original intent as a critical thinking exercise and its narrow focus\non discovering model-level flaws in the context of generative AI. Current AI\nred teaming efforts focus predominantly on individual model vulnerabilities\nwhile overlooking the broader sociotechnical systems and emergent behaviors\nthat arise from complex interactions between models, users, and environments.\nTo address this deficiency, we propose a comprehensive framework\noperationalizing red teaming in AI systems at two levels: macro-level system\nred teaming spanning the entire AI development lifecycle, and micro-level model\nred teaming. Drawing on cybersecurity experience and systems theory, we further\npropose a set of recommendations. In these, we emphasize that effective AI red\nteaming requires multifunctional teams that examine emergent risks, systemic\nvulnerabilities, and the interplay between technical and social factors."}
{"id": "2507.05816", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05816", "abs": "https://arxiv.org/abs/2507.05816", "authors": ["Shuai Zhao", "Yulin Zhang", "Luwei Xiao", "Xinyi Wu", "Yanhao Jia", "Zhongliang Guo", "Xiaobao Wu", "Cong-Duy Nguyen", "Guoming Zhang", "Anh Tuan Luu"], "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity", "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems."}
{"id": "2507.05868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05868", "abs": "https://arxiv.org/abs/2507.05868", "authors": ["Aloïs Rautureau", "Éric Piette"], "title": "CogniPlay: a work-in-progress Human-like model for General Game Playing", "comment": "5 pages, 1 figure", "summary": "While AI systems have equaled or surpassed human performance in a wide\nvariety of games such as Chess, Go, or Dota 2, describing these systems as\ntruly \"human-like\" remains far-fetched. Despite their success, they fail to\nreplicate the pattern-based, intuitive decision-making processes observed in\nhuman cognition. This paper presents an overview of findings from cognitive\npsychology and previous efforts to model human-like behavior in artificial\nagents, discusses their applicability to General Game Playing (GGP) and\nintroduces our work-in-progress model based on these observations: CogniPlay."}
{"id": "2507.05886", "categories": ["cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.05886", "abs": "https://arxiv.org/abs/2507.05886", "authors": ["Aaron Bembenek"], "title": "Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better", "comment": "6 pages, 4 figures", "summary": "There is growing excitement about building software verifiers, synthesizers,\nand other Automated Reasoning (AR) tools by combining traditional symbolic\nalgorithms and Large Language Models (LLMs). Unfortunately, the current\npractice for constructing such neurosymbolic AR systems is an ad hoc\nprogramming model that does not have the strong guarantees of traditional\nsymbolic algorithms, nor a deep enough synchronization of neural networks and\nsymbolic reasoning to unlock the full potential of LLM-powered reasoning. I\npropose Neurosymbolic Transition Systems as a principled computational model\nthat can underlie infrastructure for building neurosymbolic AR tools. In this\nmodel, symbolic state is paired with intuition, and state transitions operate\nover symbols and intuition in parallel. I argue why this new paradigm can scale\nlogical reasoning beyond current capabilities while retaining the strong\nguarantees of symbolic algorithms, and I sketch out how the computational model\nI propose can be reified in a logic programming language."}
{"id": "2507.05891", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05891", "abs": "https://arxiv.org/abs/2507.05891", "authors": ["Robert Leppich", "Michael Stenger", "André Bauer", "Samuel Kounev"], "title": "Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection", "comment": null, "summary": "With the advent of Transformers, time series forecasting has seen significant\nadvances, yet it remains challenging due to the need for effective sequence\nrepresentation, memory construction, and accurate target projection. Time\nseries forecasting remains a challenging task, demanding effective sequence\nrepresentation, meaningful information extraction, and precise future\nprojection. Each dataset and forecasting configuration constitutes a distinct\ntask, each posing unique challenges the model must overcome to produce accurate\npredictions. To systematically address these task-specific difficulties, this\nwork decomposes the time series forecasting pipeline into three core stages:\ninput sequence representation, information extraction and memory construction,\nand final target projection. Within each stage, we investigate a range of\narchitectural configurations to assess the effectiveness of various modules,\nsuch as convolutional layers for feature extraction and self-attention\nmechanisms for information extraction, across diverse forecasting tasks,\nincluding evaluations on seven benchmark datasets. Our models achieve\nstate-of-the-art forecasting accuracy while greatly enhancing computational\nefficiency, with reduced training and inference times and a lower parameter\ncount. The source code is available at\nhttps://github.com/RobertLeppich/REP-Net."}
{"id": "2507.05894", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05894", "abs": "https://arxiv.org/abs/2507.05894", "authors": ["Fathinah Izzati", "Xinyue Li", "Yuxuan Wu", "Gus Xia"], "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation", "comment": null, "summary": "Humans can imagine various atmospheres and settings when listening to music,\nenvisioning movie scenes that complement each piece. For example, slow,\nmelancholic music might evoke scenes of heartbreak, while upbeat melodies\nsuggest celebration. This paper explores whether a Music Language Model, e.g.\nMU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),\nwhich requires cross-modal information from video and music to train. To\nimprove upon existing music captioning models which focusing solely on musical\nelements, we introduce MusiScene, a music captioning model designed to imagine\nscenes that complement each music. In this paper, (1) we construct a\nlarge-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music\nUnderstanding LLaMA for the MSI task to create MusiScene, and (3) we conduct\ncomprehensive evaluations and prove that our MusiScene is more capable of\ngenerating contextually relevant captions compared to MU-LLaMA. We leverage the\ngenerated MSI captions to enhance Video Background Music Generation (VBMG) from\ntext."}
{"id": "2507.05934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05934", "abs": "https://arxiv.org/abs/2507.05934", "authors": ["Baojiao Xiong", "Boheng Chen", "Chengzhi Wang", "Daxiong Luo", "Dongsheng Xu", "Dongyang Liu", "Fan Yang", "Fangyuan Li", "Fei Teng", "Feng Wang", "Fukang Qin", "Fuquan Peng", "Guanxin Tan", "Guozhi Wang", "Haibo Yu", "Haohao Gao", "Heng Liu", "Hongbo Yang", "Hongjian Zou", "Houzheng Shen", "Hu Meng", "Huan Li", "Hui Tan", "Jiali Chen", "Jianzhao Chen", "Jinliang Zhu", "Kai Wang", "Lei Wu", "Liangbing Liu", "Liuyang Bian", "Liyan He", "Long Liu", "Peiwen Li", "Penggang Shi", "Qi Ding", "Rui Hu", "Shuai Cao", "Shuai Ren", "Shuang Peng", "Teng Xie", "Weiji Chen", "Weilin Xiang", "Weixin Wu", "Xi Yin", "Xiaoxin Chen", "Xu Chen", "Yafei Wen", "Yan Hu", "Yanzhou Yang", "Yina Xie", "Yinghao Chen", "Yixuan Liao", "Yu Geng", "Yuanjiang Ouyang", "Yuanzhuo Yang", "Yuehua He", "Yushuai Peng", "Zhaoxiong Wang", "Zheng Wang", "Zhibo Zhou", "Ziyang Wu"], "title": "BlueLM-2.5-3B Technical Report", "comment": null, "summary": "We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large\nLanguage Model (MLLM) designed for efficient edge-device deployment, offering\nstrong general-purpose and reasoning capabilities. To the best of our\nknowledge, this is the first 3B-scale MLLM to support both thinking and\nnon-thinking modes, while also enabling explicit control over thinking token\nbudget. BlueLM-2.5-3B is developed through diversified data curation, key data\nresampling, hybrid heterogeneous reinforcement learning, and a high-performance\ntraining infrastructure. Our model achieves superior multimodal capacity while\npreserving competitive pure-text performance with only 2.9 billion parameters.\nWe conduct comprehensive evaluations across a broad range of multimodal and\ntext-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable\nperformance to Qwen3-4B on text-only benchmarks, and trails the larger\nKimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In\nnon-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal\nbenchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency.\nAll of the aforementioned performance is achieved with substantially less total\ntraining data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to\nthe advancement of high-performance, on-device MLLMs and provides meaningful\ninsights to the research community."}
{"id": "2507.05938", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05938", "abs": "https://arxiv.org/abs/2507.05938", "authors": ["Yucheng Sheng", "Jiacheng Wang", "Xingyu Zhou", "Le Liang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "A Wireless Foundation Model for Multi-Task Prediction", "comment": null, "summary": "With the growing complexity and dynamics of the mobile communication\nnetworks, accurately predicting key system parameters, such as channel state\ninformation (CSI), user location, and network traffic, has become essential for\na wide range of physical (PHY)-layer and medium access control (MAC)-layer\ntasks. Although traditional deep learning (DL)-based methods have been widely\napplied to such prediction tasks, they often struggle to generalize across\ndifferent scenarios and tasks. In response, we propose a unified foundation\nmodel for multi-task prediction in wireless networks that supports diverse\nprediction intervals. The proposed model enforces univariate decomposition to\nunify heterogeneous tasks, encodes granularity for interval awareness, and uses\na causal Transformer backbone for accurate predictions. Additionally, we\nintroduce a patch masking strategy during training to support arbitrary input\nlengths. After trained on large-scale datasets, the proposed foundation model\ndemonstrates strong generalization to unseen scenarios and achieves zero-shot\nperformance on new tasks that surpass traditional full-shot baselines."}
{"id": "2507.05976", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.05976", "abs": "https://arxiv.org/abs/2507.05976", "authors": ["Alessandro Umbrico", "Guido Bologna", "Luca Coraci", "Francesca Fracasso", "Silvia Gola", "Gabriella Cortellessa"], "title": "Enhancing the Interpretability of Rule-based Explanations through Information Retrieval", "comment": null, "summary": "The lack of transparency of data-driven Artificial Intelligence techniques\nlimits their interpretability and acceptance into healthcare decision-making\nprocesses. We propose an attribution-based approach to improve the\ninterpretability of Explainable AI-based predictions in the specific context of\narm lymphedema's risk assessment after lymph nodal radiotherapy in breast\ncancer. The proposed method performs a statistical analysis of the attributes\nin the rule-based prediction model using standard metrics from Information\nRetrieval techniques. This analysis computes the relevance of each attribute to\nthe prediction and provides users with interpretable information about the\nimpact of risk factors. The results of a user study that compared the output\ngenerated by the proposed approach with the raw output of the Explainable AI\nmodel suggested higher levels of interpretability and usefulness in the context\nof predicting lymphedema risk."}
{"id": "2507.05984", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.05984", "abs": "https://arxiv.org/abs/2507.05984", "authors": ["Zhijun Guo", "Alvina Lai", "Julia Ive", "Alexandru Petcu", "Yutong Wang", "Luyuan Qi", "Johan H Thygesen", "Kezhi Li"], "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening", "comment": null, "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening."}
{"id": "2507.06013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06013", "abs": "https://arxiv.org/abs/2507.06013", "authors": ["Kushal Gajjar", "Harshit Sikchi", "Arpit Singh Gautam", "Marc Hammons", "Saurabh Jha"], "title": "CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation", "comment": null, "summary": "Translating natural language into SQL (Text-to-SQL) remains a core challenge\nat the intersection of language understanding and structured data access.\nAlthough large language models (LLMs) have improved fluency, generating correct\nand executable SQL, especially for complex queries, continues to be\nchallenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL)\nframework and model that produces accurate SQL using a lightweight reward\nsignal based on execution correctness and format-tag compliance. By avoiding\nintermediate supervision, hybrid pipelines and complex reward shaping, our\nmethod encourages stable learning and stronger alignment with the ultimate task\nobjective-producing executable programs. CogniSQL-R1-Zero achieves\nstate-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench,\noutperforming prior supervised and instruction-tuned baselines including SFT\nCodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a\nsignificantly smaller 7B backbone. This result underscores the scalability and\nefficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs\n(40 GB VRAM each). To support further research in efficient and interpretable\nText-to-SQL modeling, we release two curated datasets: (i) a collection of\n5,024 reasoning traces with varying context lengths, and (ii) a\npositive-sampled corpus of 36,356 corpus of weakly supervised queries, each\nannotated with six semantically diverse reasoning paths. Together, these\ncontributions advance scalable, execution-aligned Text-to-SQL generation."}
{"id": "2507.06029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06029", "abs": "https://arxiv.org/abs/2507.06029", "authors": ["Courtney Ford", "Mark T. Keane"], "title": "Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions", "comment": "7 pages, 5 figures, 1 table. Accepted at IJCAI 2025 Workshop on\n  User-Aligned Assessment of Adaptive AI Systems", "summary": "Explainable AI (XAI) methods often struggle to generate clear, interpretable\noutputs for users without domain expertise. We introduce Feature-Guided\nNeighbor Selection (FGNS), a post hoc method that enhances interpretability by\nselecting class-representative examples using both local and global feature\nimportance. In a user study (N = 98) evaluating Kannada script classifications,\nFGNS significantly improved non-experts' ability to identify model errors while\nmaintaining appropriate agreement with correct predictions. Participants made\nfaster and more accurate decisions compared to those given traditional k-NN\nexplanations. Quantitative analysis shows that FGNS selects neighbors that\nbetter reflect class characteristics rather than merely minimizing\nfeature-space distance, leading to more consistent selection and tighter\nclustering around class prototypes. These results support FGNS as a step toward\nmore human-aligned model assessment, although further work is needed to address\nthe gap between explanation quality and perceived trust."}
{"id": "2507.06042", "categories": ["cs.AI", "03B42, 03B48"], "pdf": "https://arxiv.org/pdf/2507.06042", "abs": "https://arxiv.org/abs/2507.06042", "authors": ["Tommaso Flaminio", "Lluis Godo", "Ramón Pino Pérez", "Lluis Subirana"], "title": "On Lockean beliefs that are deductively closed and minimal change", "comment": "18 pages, to appear in the proceedings of JELIA 2025", "summary": "Within the formal setting of the Lockean thesis, an agent belief set is\ndefined in terms of degrees of confidence and these are described in\nprobabilistic terms. This approach is of established interest, notwithstanding\nsome limitations that make its use troublesome in some contexts, like, for\ninstance, in belief change theory. Precisely, Lockean belief sets are not\ngenerally closed under (classical) logical deduction. The aim of the present\npaper is twofold: on one side we provide two characterizations of those belief\nsets that are closed under classical logic deduction, and on the other we\npropose an approach to probabilistic update that allows us for a minimal\nrevision of those beliefs, i.e., a revision obtained by making the fewest\npossible changes to the existing belief set while still accommodating the new\ninformation. In particular, we show how we can deductively close a belief set\nvia a minimal revision."}
{"id": "2507.06057", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06057", "abs": "https://arxiv.org/abs/2507.06057", "authors": ["Bo Pang", "Yalu Ouyang", "Hangfei Xu", "Ziqi Jia", "Panpan Li", "Shengzhao Wen", "Lu Wang", "Shiyong Li", "Yanpeng Wang"], "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models", "comment": null, "summary": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models -- C32B, S32B, R32B -- from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation"}
{"id": "2507.06077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06077", "abs": "https://arxiv.org/abs/2507.06077", "authors": ["Iman Rahimi", "Isha Patel"], "title": "AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study", "comment": null, "summary": "This paper tackles the urgent need for efficient energy management in\nhealthcare facilities, where fluctuating demands challenge operational\nefficiency and sustainability. Traditional methods often prove inadequate,\ncausing inefficiencies and higher costs. To address this, the study presents an\nAI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm\n(GA), and SHAP (Shapley Additive Explanations), specifically designed for\nhealthcare energy management. Although LSTM is widely used for time-series\nforecasting, its application in healthcare energy prediction remains\nunderexplored. The results reveal that LSTM significantly outperforms ARIMA and\nProphet models in forecasting complex, non-linear demand patterns. LSTM\nachieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)\nof 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:\n87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm\nis applied to optimize model parameters and improve load balancing strategies,\nenabling adaptive responses to real-time energy fluctuations. SHAP analysis\nfurther enhances model transparency by explaining the influence of different\nfeatures on predictions, fostering trust in decision-making processes. This\nintegrated LSTM-GA-SHAP approach offers a robust solution for improving\nforecasting accuracy, boosting energy efficiency, and advancing sustainability\nin healthcare facilities. Future research may explore real-time deployment and\nhybridization with reinforcement learning for continuous optimization. Overall,\nthe study establishes a solid foundation for using AI in healthcare energy\nmanagement, highlighting its scalability, efficiency, and resilience potential."}
{"id": "2507.06134", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06134", "abs": "https://arxiv.org/abs/2507.06134", "authors": ["Sanidhya Vijayvargiya", "Aditya Bharat Soni", "Xuhui Zhou", "Zora Zhiruo Wang", "Nouha Dziri", "Graham Neubig", "Maarten Sap"], "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety", "comment": "19 pages, 10 figures", "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment."}
{"id": "2507.06187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06187", "abs": "https://arxiv.org/abs/2507.06187", "authors": ["Scott Geng", "Hamish Ivison", "Chun-Liang Li", "Maarten Sap", "Jerry Li", "Ranjay Krishna", "Pang Wei Koh"], "title": "The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains", "comment": "COLM 2025", "summary": "Improvements in language models are often driven by improving the quality of\nthe data we train them on, which can be limiting when strong supervision is\nscarce. In this work, we show that paired preference data consisting of\nindividually weak data points can enable gains beyond the strength of each\nindividual data point. We formulate the delta learning hypothesis to explain\nthis phenomenon, positing that the relative quality delta between points\nsuffices to drive learning via preference tuning--even when supervised\nfinetuning on the weak data hurts. We validate our hypothesis in controlled\nexperiments and at scale, where we post-train 8B models on preference data\ngenerated by pairing a small 3B model's responses with outputs from an even\nsmaller 1.5B model to create a meaningful delta. Strikingly, on a standard\n11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the\nperformance of Tulu 3, a state-of-the-art open model tuned from the same base\nmodel while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta\nlearning enables simpler and cheaper open recipes for state-of-the-art\npost-training. To better understand delta learning, we prove in logistic\nregression that the performance gap between two weak teacher models provides\nuseful signal for improving a stronger student. Overall, our work shows that\nmodels can learn surprisingly well from paired data that might typically be\nconsidered weak."}
{"id": "2507.06213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06213", "abs": "https://arxiv.org/abs/2507.06213", "authors": ["Clément Yvernes", "Emilie Devijver", "Marianne Clausel", "Eric Gaussier"], "title": "Identifiability in Causal Abstractions: A Hierarchy of Criteria", "comment": "Accepted at the CAR Workshop at UAI2025", "summary": "Identifying the effect of a treatment from observational data typically\nrequires assuming a fully specified causal diagram. However, such diagrams are\nrarely known in practice, especially in complex or high-dimensional settings.\nTo overcome this limitation, recent works have explored the use of causal\nabstractions-simplified representations that retain partial causal information.\nIn this paper, we consider causal abstractions formalized as collections of\ncausal diagrams, and focus on the identifiability of causal queries within such\ncollections. We introduce and formalize several identifiability criteria under\nthis setting. Our main contribution is to organize these criteria into a\nstructured hierarchy, highlighting their relationships. This hierarchical view\nenables a clearer understanding of what can be identified under varying levels\nof causal knowledge. We illustrate our framework through examples from the\nliterature and provide tools to reason about identifiability when full causal\nknowledge is unavailable."}
{"id": "2507.06221", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.06221", "abs": "https://arxiv.org/abs/2507.06221", "authors": ["Yuxuan Lu", "Yifan Wu", "Jason Hartline", "Michael J. Curry"], "title": "Aligned Textual Scoring Rules", "comment": null, "summary": "Scoring rules elicit probabilistic predictions from a strategic agent by\nscoring the prediction against a ground truth state. A scoring rule is proper\nif, from the agent's perspective, reporting the true belief maximizes the\nexpected score. With the development of language models, Wu and Hartline (2024)\nproposes a reduction from textual information elicitation to the numerical\n(i.e. probabilistic) information elicitation problem, which achieves provable\nproperness for textual elicitation. However, not all proper scoring rules are\nwell aligned with human preference over text. Our paper designs the Aligned\nScoring rule (ASR) for text by optimizing and minimizing the mean squared error\nbetween a proper scoring rule and a reference score (e.g. human score). Our\nexperiments show that our ASR outperforms previous methods in aligning with\nhuman preference while maintaining properness."}
{"id": "2507.05512", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05512", "abs": "https://arxiv.org/abs/2507.05512", "authors": ["Gehao Zhang", "Eugene Bagdasarian", "Juan Zhai", "Shiqing Ma"], "title": "Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice", "comment": null, "summary": "Distinguishing AI-generated code from human-written code is becoming crucial\nfor tasks such as authorship attribution, content tracking, and misuse\ndetection. Based on this, N-gram-based watermarking schemes have emerged as\nprominent, which inject secret watermarks to be detected during the generation.\n  However, their robustness in code content remains insufficiently evaluated.\nMost claims rely solely on defenses against simple code transformations or code\noptimizations as a simulation of attack, creating a questionable sense of\nrobustness. In contrast, more sophisticated schemes already exist in the\nsoftware engineering world, e.g., code obfuscation, which significantly alters\ncode while preserving functionality. Although obfuscation is commonly used to\nprotect intellectual property or evade software scanners, the robustness of\ncode watermarking techniques against such transformations remains largely\nunexplored.\n  In this work, we formally model the code obfuscation and prove the\nimpossibility of N-gram-based watermarking's robustness with only one intuitive\nand experimentally verified assumption, distribution consistency, satisfied.\nGiven the original false positive rate of the watermarking detection, the ratio\nthat the detector failed on the watermarked code after obfuscation will\nincrease to 1 - fpr.\n  The experiments have been performed on three SOTA watermarking schemes, two\nLLMs, two programming languages, four code benchmarks, and four obfuscators.\nAmong them, all watermarking detectors show coin-flipping detection abilities\non obfuscated codes (AUROC tightly surrounds 0.5). Among all models,\nwatermarking schemes, and datasets, both programming languages own obfuscators\nthat can achieve attack effects with no detection AUROC higher than 0.6 after\nthe attack. Based on the theoretical and practical observations, we also\nproposed a potential path of robust code watermarking."}
{"id": "2507.05558", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05558", "abs": "https://arxiv.org/abs/2507.05558", "authors": ["Arthur Gervais", "Liyi Zhou"], "title": "AI Agent Smart Contract Exploit Generation", "comment": null, "summary": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a $6000 exploit\nvalue, while defenders require $60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense."}
{"id": "2507.05622", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05622", "abs": "https://arxiv.org/abs/2507.05622", "authors": ["Shuo Shao", "Yiming Li", "Mengren Zheng", "Zhiyang Hu", "Yukun Chen", "Boheng Li", "Yu He", "Junfeng Guo", "Tianwei Zhang", "Dacheng Tao", "Zhan Qin"], "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective", "comment": null, "summary": "The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench."}
{"id": "2507.05630", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05630", "abs": "https://arxiv.org/abs/2507.05630", "authors": ["Sarthak Choudhary", "Divyam Anshumaan", "Nils Palumbo", "Somesh Jha"], "title": "How Not to Detect Prompt Injections with an LLM", "comment": null, "summary": "LLM-integrated applications and agents are vulnerable to prompt injection\nattacks, in which adversaries embed malicious instructions within seemingly\nbenign user inputs to manipulate the LLM's intended behavior. Recent defenses\nbased on $\\textit{known-answer detection}$ (KAD) have achieved near-perfect\nperformance by using an LLM to classify inputs as clean or contaminated. In\nthis work, we formally characterize the KAD framework and uncover a structural\nvulnerability in its design that invalidates its core security premise. We\ndesign a methodical adaptive attack, $\\textit{DataFlip}$, to exploit this\nfundamental weakness. It consistently evades KAD defenses with detection rates\nas low as $1.5\\%$ while reliably inducing malicious behavior with success rates\nof up to $88\\%$, without needing white-box access to the LLM or any\noptimization procedures."}
{"id": "2507.05649", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05649", "abs": "https://arxiv.org/abs/2507.05649", "authors": ["Kaixiang Zhao", "Joseph Yousry Attalla", "Qian Lou", "Yushun Dong"], "title": "DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning", "comment": "Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics."}
{"id": "2507.05660", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05660", "abs": "https://arxiv.org/abs/2507.05660", "authors": ["Aravind Cheruvu", "Shravya Kanchi", "Sifat Muhammad Abdullah", "Nicholas Kong", "Daphne Yao", "Murtuza Jadliwala", "Bimal Viswanath"], "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data", "comment": "Pre-print", "summary": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL)."}
{"id": "2507.05794", "categories": ["cs.CR", "cs.AI", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05794", "abs": "https://arxiv.org/abs/2507.05794", "authors": ["Avi Shaked", "Nan Messe"], "title": "Automated Reasoning for Vulnerability Management by Design", "comment": null, "summary": "For securing systems, it is essential to manage their vulnerability posture\nand design appropriate security controls. Vulnerability management allows to\nproactively address vulnerabilities by incorporating pertinent security\ncontrols into systems designs. Current vulnerability management approaches do\nnot support systematic reasoning about the vulnerability postures of systems\ndesigns. To effectively manage vulnerabilities and design security controls, we\npropose a formally grounded automated reasoning mechanism. We integrate the\nmechanism into an open-source security design tool and demonstrate its\napplication through an illustrative example driven by real-world challenges.\nThe automated reasoning mechanism allows system designers to identify\nvulnerabilities that are applicable to a specific system design, explicitly\nspecify vulnerability mitigation options, declare selected controls, and thus\nsystematically manage vulnerability postures."}
{"id": "2507.06008", "categories": ["cs.CR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06008", "abs": "https://arxiv.org/abs/2507.06008", "authors": ["Jungeun Lim", "Stephan A. Fahrenkrog-Petersen", "Xixi Lu", "Jan Mendling", "Minseok Song"], "title": "The Impact of Event Data Partitioning on Privacy-aware Process Discovery", "comment": null, "summary": "Information systems support the execution of business processes. The event\nlogs of these executions generally contain sensitive information about\ncustomers, patients, and employees. The corresponding privacy challenges can be\naddressed by anonymizing the event logs while still retaining utility for\nprocess discovery. However, trading off utility and privacy is difficult: the\nhigher the complexity of event log, the higher the loss of utility by\nanonymization. In this work, we propose a pipeline that combines anonymization\nand event data partitioning, where event abstraction is utilized for\npartitioning. By leveraging event abstraction, event logs can be segmented into\nmultiple parts, allowing each sub-log to be anonymized separately. This\npipeline preserves privacy while mitigating the loss of utility. To validate\nour approach, we study the impact of event partitioning on two anonymization\ntechniques using three real-world event logs and two process discovery\ntechniques. Our results demonstrate that event partitioning can bring\nimprovements in process discovery utility for directly-follows-based\nanonymization techniques."}
{"id": "2507.06043", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06043", "abs": "https://arxiv.org/abs/2507.06043", "authors": ["Xiaohu Li", "Yunfeng Ning", "Zepeng Bao", "Mayi Xu", "Jianhao Chen", "Tieyun Qian"], "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations", "comment": null, "summary": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."}
{"id": "2507.06092", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06092", "abs": "https://arxiv.org/abs/2507.06092", "authors": ["Shravya Kanchi", "Neal Mangaokar", "Aravind Cheruvu", "Sifat Muhammad Abdullah", "Shirin Nilizadeh", "Atul Prakash", "Bimal Viswanath"], "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI", "comment": null, "summary": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks."}
