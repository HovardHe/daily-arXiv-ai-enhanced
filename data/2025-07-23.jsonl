{"id": "2507.16055", "categories": ["math.OC", "cs.NA", "math.DG", "math.NA", "90C25, 49Q99, 49M30, 65K10"], "pdf": "https://arxiv.org/pdf/2507.16055", "abs": "https://arxiv.org/abs/2507.16055", "authors": ["Ronny Bergmann", "Hajg Jasa", "Paula John", "Max Pfeffer"], "title": "The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization", "comment": null, "summary": "We consider a class of (possibly strongly) geodesically convex optimization\nproblems on Hadamard manifolds, where the objective function splits into the\nsum of a smooth and a possibly nonsmooth function. We introduce an intrinsic\nconvex Riemannian proximal gradient (CRPG) method that employs the manifold\nproximal map for the nonsmooth step, without operating in the embedding or\ntangent space. A sublinear convergence rate for convex problems and a linear\nconvergence rate for strongly convex problems is established, and we derive\nfundamental proximal gradient inequalities that generalize the Euclidean case.\nOur numerical experiments on hyperbolic spaces and manifolds of symmetric\npositive definite matrices demonstrate substantial computational advantages\nover existing methods."}
{"id": "2507.16340", "categories": ["math.ST", "stat.ME", "stat.TH", "62G32, 62H25"], "pdf": "https://arxiv.org/pdf/2507.16340", "abs": "https://arxiv.org/abs/2507.16340", "authors": ["Alexis Boulin", "Axel Bücher"], "title": "Structured linear factor models for tail dependence", "comment": "34 pages", "summary": "A common object to describe the extremal dependence of a $d$-variate random\nvector $X$ is the stable tail dependence function $L$. Various parametric\nmodels have emerged, with a popular subclass consisting of those stable tail\ndependence functions that arise for linear and max-linear factor models with\nheavy tailed factors. The stable tail dependence function is then parameterized\nby a $d \\times K$ matrix $A$, where $K$ is the number of factors and where $A$\ncan be interpreted as a factor loading matrix. We study estimation of $L$ under\nan additional assumption on $A$ called the `pure variable assumption'. Both $K\n\\in \\{1, \\dots, d\\}$ and $A \\in [0, \\infty)^{d \\times K}$ are treated as\nunknown, which constitutes an unconventional parameter space that does not fit\ninto common estimation frameworks. We suggest two algorithms that allow to\nestimate $K$ and $A$, and provide finite sample guarantees for both algorithms.\nRemarkably, the guarantees allow for the case where the dimension $d$ is larger\nthan the sample size $n$. The results are illustrated with numerical\nexperiments."}
{"id": "2507.15859", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15859", "abs": "https://arxiv.org/abs/2507.15859", "authors": ["Harsha Sammangi", "Aditya Jagatha", "Giridhar Reddy Bojja", "Jun Liu"], "title": "Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios", "comment": "10 Pages", "summary": "AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the\ncurrent traditional centralized healthcare architecture poses numerous issues,\nincluding data privacy, delay, and security. Here, we present an AI-enabled\ndecentralized IoT architecture that can address such challenges during a\npandemic and critical care settings. This work presents our architecture to\nenhance the effectiveness of the current available federated learning,\nblockchain, and edge computing approach, maximizing data privacy, minimizing\nlatency, and improving other general system metrics. Experimental results\ndemonstrate transaction latency, energy consumption, and data throughput orders\nof magnitude lower than competitive cloud solutions."}
{"id": "2507.16216", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16216", "abs": "https://arxiv.org/abs/2507.16216", "authors": ["Jochen Trumpf", "Behzad Zamani", "Chris Manzie"], "title": "Conservative fusion of unbiased partial state estimates: CI is optimal", "comment": "Submitted to Automatica", "summary": "We show that Covariance Intersection (CI) is optimal amongst all conservative\nunbiased linear fusion rules also in the general case of information fusion of\ntwo unbiased partial state estimates, significantly generalizing the known\noptimality result for fusion of full state estimates. In fact, we prove the\nmuch stronger result that three different optimization problems are equivalent,\nnamely the abstract optimal conservative unbiased linear information fusion\nproblem with respect to a strictly isotone cost function, the scalar Covariance\nIntersection (CI) problem, and a simple semi-definite program (SDP). We provide\na general solvability condition for these problems as well as equations\ncharacterizing the optimal solutions for the matrix determinant and matrix\ntrace cost functions."}
{"id": "2507.16529", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.16529", "abs": "https://arxiv.org/abs/2507.16529", "authors": ["Valentinian Lungu", "Joni Shaska", "Ioannis Kontoyiannis", "Urbashi Mitra"], "title": "Bayesian causal discovery: Posterior concentration and optimal detection", "comment": null, "summary": "We consider the problem of Bayesian causal discovery for the standard model\nof linear structural equations with equivariant Gaussian noise. A uniform prior\nis placed on the space of directed acyclic graphs (DAGs) over a fixed set of\nvariables and, given the graph, independent Gaussian priors are placed on the\nassociated linear coefficients of pairwise interactions. We show that the rate\nat which the posterior on model space concentrates on the true underlying DAG\ndepends critically on its nature: If it is maximal, in the sense that adding\nany one new edge would violate acyclicity, then its posterior probability\nconverges to 1 exponentially fast (almost surely) in the sample size $n$.\nOtherwise, it converges at a rate no faster than $1/\\sqrt{n}$. This sharp\ndichotomy is an instance of the important general phenomenon that avoiding\noverfitting is significantly harder than identifying all of the structure that\nis present in the model. We also draw a new connection between the posterior\ndistribution on model space and recent results on optimal hypothesis testing in\nthe related problem of edge detection. Our theoretical findings are illustrated\nempirically through simulation experiments."}
{"id": "2507.15984", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15984", "abs": "https://arxiv.org/abs/2507.15984", "authors": ["I Putu Arya Dharmaadi", "Mohannad Alhanahnah", "Van-Thuan Pham", "Fadi Mohsen", "Fatih Turkmen"], "title": "BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications", "comment": "Under peer-review", "summary": "Broken Access Control (BAC) remains one of the most critical and widespread\nvulnerabilities in web applications, allowing attackers to access unauthorized\nresources or perform privileged actions. Despite its severity, BAC is\nunderexplored in automated testing due to key challenges: the lack of reliable\noracles and the difficulty of generating semantically valid attack requests. We\nintroduce BACFuzz, the first gray-box fuzzing framework specifically designed\nto uncover BAC vulnerabilities, including Broken Object-Level Authorization\n(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web\napplications. BACFuzz combines LLM-guided parameter selection with runtime\nfeedback and SQL-based oracle checking to detect silent authorization flaws. It\nemploys lightweight instrumentation to capture runtime information that guides\ntest generation, and analyzes backend SQL queries to verify whether\nunauthorized inputs flow into protected operations. Evaluated on 20 real-world\nweb applications, including 15 CVE cases and 2 known benchmarks, BACFuzz\ndetects 16 of 17 known issues and uncovers 26 previously unknown BAC\nvulnerabilities with low false positive rates. All identified issues have been\nresponsibly disclosed, and artifacts will be publicly released."}
{"id": "2507.16259", "categories": ["math.OC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16259", "abs": "https://arxiv.org/abs/2507.16259", "authors": ["Yineng Sun", "Armin Fügenschuh", "Vikrant Vaze"], "title": "Physics-aware Truck and Drone Delivery Planning Using Optimization & Machine Learning", "comment": null, "summary": "Combining an energy-efficient drone with a high-capacity truck for last-mile\npackage delivery can benefit operators and customers by reducing delivery times\nand environmental impact. However, directly integrating drone flight dynamics\ninto the combinatorially hard truck route planning problem is challenging.\nSimplified models that ignore drone flight physics can lead to suboptimal\ndelivery plans. We propose an integrated formulation for the joint problem of\ntruck route and drone trajectory planning and a new end-to-end solution\napproach that combines optimization and machine learning to generate\nhigh-quality solutions in practical online runtimes. Our solution method trains\nneural network predictors based on offline solutions to the drone trajectory\noptimization problem instances to approximate drone flight times, and uses\nthese approximations to optimize the overall truck-and-drone delivery plan by\naugmenting an existing order-first-split-second heuristic. Our method\nexplicitly incorporates key kinematics and energy equations in drone trajectory\noptimization, and thereby outperforms state-of-the-art benchmarks that ignore\ndrone flight physics. Extensive experimentation using synthetic datasets and\nreal-world case studies shows that the integration of drone trajectories into\npackage delivery planning substantially improves system performance in terms of\ntour duration and drone energy consumption. Our modeling and computational\nframework can help delivery planners achieve annual savings worth millions of\ndollars while also benefiting the environment."}
{"id": "2507.16734", "categories": ["math.ST", "cs.IT", "math.IT", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.16734", "abs": "https://arxiv.org/abs/2507.16734", "authors": ["Zeyu Jia", "Yury Polyanskiy"], "title": "Gaussian Sequence Model: Sample Complexities of Testing, Estimation and LFHT", "comment": null, "summary": "We study the Gaussian sequence model, i.e. $X \\sim N(\\mathbf{\\theta},\nI_\\infty)$, where $\\mathbf{\\theta} \\in \\Gamma \\subset \\ell_2$ is assumed to be\nconvex and compact. We show that goodness-of-fit testing sample complexity is\nlower bounded by the square-root of the estimation complexity, whenever\n$\\Gamma$ is orthosymmetric. We show that the lower bound is tight when $\\Gamma$\nis also quadratically convex, thus significantly extending validity of the\ntesting-estimation relationship from [GP24]. Using similar methods, we also\ncompletely characterize likelihood-free hypothesis testing (LFHT) complexity\nfor $\\ell_p$-bodies, discovering new types of tradeoff between the numbers of\nsimulation and observation samples."}
{"id": "2507.15997", "categories": ["cs.CR", "cs.HC", "68-XX 68-XX 68-XX"], "pdf": "https://arxiv.org/pdf/2507.15997", "abs": "https://arxiv.org/abs/2507.15997", "authors": ["Onyinye Dibia", "Mengyi Lu", "Prianka Bhattacharjee", "Joseph P. Near", "Yuanyuan Feng"], "title": "\"We Need a Standard\": Toward an Expert-Informed Privacy Label for Differential Privacy", "comment": "13 pages, 5 figures", "summary": "The increasing adoption of differential privacy (DP) leads to public-facing\nDP deployments by both government agencies and companies. However, real-world\nDP deployments often do not fully disclose their privacy guarantees, which vary\ngreatly between deployments. Failure to disclose certain DP parameters can lead\nto misunderstandings about the strength of the privacy guarantee, undermining\nthe trust in DP. In this work, we seek to inform future standards for\ncommunicating the privacy guarantees of DP deployments. Based on\nsemi-structured interviews with 12 DP experts, we identify important DP\nparameters necessary to comprehensively communicate DP guarantees, and describe\nwhy and how they should be disclosed. Based on expert recommendations, we\ndesign an initial privacy label for DP to comprehensively communicate privacy\nguarantees in a standardized format."}
{"id": "2507.16264", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16264", "abs": "https://arxiv.org/abs/2507.16264", "authors": ["Rajiv Sambharya", "Jinho Bok", "Nikolai Matni", "George Pappas"], "title": "Learning Acceleration Algorithms for Fast Parametric Convex Optimization with Certified Robustness", "comment": null, "summary": "We develop a machine-learning framework to learn hyperparameter sequences for\naccelerated first-order methods (e.g., the step size and momentum sequences in\naccelerated gradient descent) to quickly solve parametric convex optimization\nproblems with certified robustness. We obtain a strong form of robustness\nguarantee -- certification of worst-case performance over all parameters within\na set after a given number of iterations -- through regularization-based\ntraining. The regularization term is derived from the performance estimation\nproblem (PEP) framework based on semidefinite programming, in which the\nhyperparameters appear as problem data. We show how to use gradient-based\ntraining to learn the hyperparameters for several first-order methods:\naccelerated versions of gradient descent, proximal gradient descent, and\nalternating direction method of multipliers. Through various numerical examples\nfrom signal processing, control, and statistics, we demonstrate that the\nquality of the solution can be dramatically improved within a budget of\niterations, while also maintaining strong robustness guarantees. Notably, our\napproach is highly data-efficient in that we only use ten training instances in\nall of the numerical examples."}
{"id": "2507.16776", "categories": ["math.ST", "econ.EM", "stat.TH", "62G15, 62J05"], "pdf": "https://arxiv.org/pdf/2507.16776", "abs": "https://arxiv.org/abs/2507.16776", "authors": ["Alexis Derumigny", "Lucas Girard", "Yannick Guyonvarch"], "title": "Can we have it all? Non-asymptotically valid and asymptotically exact confidence intervals for expectations and linear regressions", "comment": "69 pages", "summary": "We contribute to bridging the gap between large- and finite-sample inference\nby studying confidence sets (CSs) that are both non-asymptotically valid and\nasymptotically exact uniformly (NAVAE) over semi-parametric statistical models.\nNAVAE CSs are not easily obtained; for instance, we show they do not exist over\nthe set of Bernoulli distributions. We first derive a generic sufficient\ncondition: NAVAE CSs are available as soon as uniform asymptotically exact CSs\nare. Second, building on that connection, we construct closed-form NAVAE\nconfidence intervals (CIs) in two standard settings -- scalar expectations and\nlinear combinations of OLS coefficients -- under moment conditions only. For\nexpectations, our sole requirement is a bounded kurtosis. In the OLS case, our\nmoment constraints accommodate heteroskedasticity and weak exogeneity of the\nregressors. Under those conditions, we enlarge the Central Limit Theorem-based\nCIs, which are asymptotically exact, to ensure non-asymptotic guarantees. Those\nmodifications vanish asymptotically so that our CIs coincide with the classical\nones in the limit. We illustrate the potential and limitations of our approach\nthrough a simulation study."}
{"id": "2507.16040", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16040", "abs": "https://arxiv.org/abs/2507.16040", "authors": ["Xinyuan Zhang", "Anrin Chakraborti", "Michael Reiter"], "title": "Blocklisted Oblivious Pseudorandom Functions", "comment": null, "summary": "An oblivious pseudorandom function (OPRF) is a protocol by which a client and\nserver interact to evaluate a pseudorandom function on a key provided by the\nserver and an input provided by the client, without divulging the key or input\nto the other party. We extend this notion by enabling the server to specify a\nblocklist, such that OPRF evaluation succeeds only if the client's input is not\non the blocklist. More specifically, our design gains performance by embedding\nthe client input into a metric space, where evaluation continues only if this\nembedding does not cluster with blocklist elements. Our framework exploits this\nstructure to separate the embedding and blocklist check to enable efficient\nimplementations of each, but then must stitch these phases together through\ncryptographic means. Our framework also supports subsequent evaluation of the\nOPRF on the same input more efficiently. We demonstrate the use of our design\nfor password blocklisting in augmented password-authenticated key exchange, and\nto MAC only executables that are not similar to ones on a blocklist of known\nmalware."}
{"id": "2507.16648", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.16648", "abs": "https://arxiv.org/abs/2507.16648", "authors": ["Eleon Bach", "Yann Disser", "Sophie Huiberts", "Nils Mosis"], "title": "An unconditional lower bound for the active-set method in convex quadratic maximization", "comment": null, "summary": "We prove that the active-set method needs an exponential number of iterations\nin the worst-case to maximize a convex quadratic function subject to linear\nconstraints, regardless of the pivot rule used. This substantially improves\nover the best previously known lower bound [IPCO 2025], which needs objective\nfunctions of polynomial degrees $\\omega(\\log d)$ in dimension $d$, to a bound\nusing a convex polynomial of degree 2. In particular, our result firmly\nresolves the open question [IPCO 2025] of whether a constant degree suffices,\nand it represents significant progress towards linear objectives, where the\nactive-set method coincides with the simplex method and a lower bound for all\npivot rules would constitute a major breakthrough.\n  Our result is based on a novel extended formulation, recursively constructed\nusing deformed products. Its key feature is that it projects onto a polygonal\napproximation of a parabola while preserving all of its exponentially many\nvertices. We define a quadratic objective that forces the active-set method to\nfollow the parabolic boundary of this projection, without allowing any\nshortcuts along chords corresponding to edges of its full-dimensional preimage."}
{"id": "2507.15992", "categories": ["math.NT", "11G18, 11G20, 11G30, 11G15"], "pdf": "https://arxiv.org/pdf/2507.15992", "abs": "https://arxiv.org/abs/2507.15992", "authors": ["Pietro Mercuri", "Oana Padurariu", "Frederick Saia", "Claudio Stirpe"], "title": "Point counts, automorphisms, and gonalities of Shimura curves", "comment": "32 pages. Comments welcome!", "summary": "We implement an algorithm to compute the number of points over finite fields\nfor the Shimura curves $X_0^D(N)$ and their Atkin--Lehner quotients. Our\ncomputations result in many examples of curves which attain the largest known\npoint counts among curves of specified genus over a finite field of given\ncardinality. To illustrate the utility of our point counts algorithm in\naddressing arithmetic questions, we prove that all automorphisms are\nAtkin--Lehner for many curves $X_0^D(N)$ with $DN\\leq 10000$, and we determine\nall tetragonal curves $X_0^D(N)$ up to a small number of possible exceptions."}
{"id": "2507.15908", "categories": ["math.CO", "math.CA", "math.NT", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.15908", "abs": "https://arxiv.org/abs/2507.15908", "authors": ["Paul Melotti"], "title": "Distribution of roots of Eulerian polynomials", "comment": null, "summary": "We show that the empirical measures of roots of Eulerian polynomials converge\nto a certain log-Cauchy distribution. To do so, we show that the moments of the\nroots of a related family of polynomials not only converge, but are in fact\nultimately constant. These asymptotic moments are expressed in terms of\nN\\\"orlund's numbers."}
{"id": "2507.15865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15865", "abs": "https://arxiv.org/abs/2507.15865", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua"], "title": "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing\nthe problem-solving capabilities of large language models (LLMs). However, the\ntheoretical foundations of learning from CoT data remain underdeveloped, and\nexisting approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement\nLearning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) --\noften fail on complex reasoning tasks. In this work, we identify core obstacles\nthat hinder effective CoT learning, including distribution drift, lack of\nembedded search, and exponential inference costs. We introduce the Diligent\nLearner, a new learning paradigm that explicitly models reasoning as a\ndepth-first search guided by a validator and supports backtracking upon\nfailure. Under two mild and realistic assumptions, we prove that the Diligent\nLearner can efficiently learn from CoT data while existing methods fail to do\nso. This framework offers a path toward building scalable and reliable\nreasoning systems trained on naturally occurring, incomplete data -- paving the\nway for the development of Large Reasoning Models (LRMs) with robust,\ninterpretable problem-solving abilities."}
{"id": "2507.16272", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16272", "abs": "https://arxiv.org/abs/2507.16272", "authors": ["Elvira Moreno", "Venkat Chandrasekaran"], "title": "Spectral Methods for Polynomial Optimization", "comment": null, "summary": "We present a hierarchy of tractable relaxations to obtain lower bounds on the\nminimum value of a polynomial over a constraint set defined by polynomial\nequations. In contrast to previous convex relaxation techniques for this\nproblem, our method is based on computing the smallest generalized eigenvalue\nof a pair of matrices derived from the problem data, which can be accomplished\nfor large problem instances using off-the-shelf software. We characterize the\nalgebraic structure in a problem that facilitates the application of our\nframework, and we observe that our method is applicable for all polynomial\noptimization problems with bounded constraint sets. Our construction also\nyields a nested sequence of structured convex outer approximations of a bounded\nalgebraic variety with the property that linear optimization over each\napproximation reduces to an eigenvalue computation. Finally, we present\nnumerical experiments on representative problems in which we demonstrate the\nscalability of our approach compared to convex relaxation methods derived from\nsums-of-squares certificates of nonnegativity."}
{"id": "2507.16370", "categories": ["cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.16370", "abs": "https://arxiv.org/abs/2507.16370", "authors": ["Lucas de Lara"], "title": "Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning", "comment": null, "summary": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples."}
{"id": "2507.16060", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16060", "abs": "https://arxiv.org/abs/2507.16060", "authors": ["Eyasu Getahun Chekole", "Howard Halim", "Jianying Zhou"], "title": "MFAz: Historical Access Based Multi-Factor Authorization", "comment": null, "summary": "Unauthorized access remains one of the critical security challenges in the\nrealm of cybersecurity. With the increasing sophistication of attack\ntechniques, the threat of unauthorized access is no longer confined to the\nconventional ones, such as exploiting weak access control policies. Instead,\nadvanced exploitation strategies, such as session hijacking-based attacks, are\nbecoming increasingly prevalent, posing serious security concerns. Session\nhijacking enables attackers to take over an already established session between\nlegitimate peers in a stealthy manner, thereby gaining unauthorized access to\nprivate resources. Unfortunately, traditional access control mechanisms, such\nas static access control policies, are insufficient to prevent session\nhijacking or other advanced exploitation techniques. In this work, we propose a\nnew multi-factor authorization (MFAz) scheme that proactively mitigates\nunauthorized access attempts both conventional and advanced unauthorized access\nattacks. The proposed scheme employs fine-grained access control rules (ARs)\nand verification points (VPs) that are systematically generated from\nhistorically granted accesses as the first and second authorization factors,\nrespectively. As a proof-of-concept, we implement the scheme using different\ntechniques. We leverage bloom filter to achieve runtime and storage efficiency,\nand blockchain to make authorization decisions in a temper-proof and\ndecentralized manner. To the best of our knowledge, this is the first formal\nintroduction of a multi-factor authorization scheme, which is orthogonal to the\nmulti-factor authentication (MFA) schemes. The effectiveness of our proposed\nscheme is experimentally evaluated using a smart-city testbed involving\ndifferent devices with varying computational capacities. The experimental\nresults reveal high effectiveness of the scheme both in security and\nperformance guarantees."}
{"id": "2507.16759", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.16759", "abs": "https://arxiv.org/abs/2507.16759", "authors": ["Sergey Kurapov", "Maxim Davidovsky"], "title": "Algorithmic methods of finite discrete structures. Topological graph drawing (part IV)", "comment": "67 pages, in Ukrainian language, 83 figures, a preprint of monography", "summary": "The chapter presents mathematical models intended for creating a topological\ndrawing of a non-separable non-planar graph based on the methods of G. Ringel's\nvertex rotation theory. The induced system of cycles generates a topological\ndrawing of a certain thickness. A method for determining the location of\nimaginary vertices by finding the intersection of connections on a plane is\npresented. A topological drawing of a maximum planar subgraph is used as a\nbasis."}
{"id": "2507.16135", "categories": ["math.NT", "11A07"], "pdf": "https://arxiv.org/pdf/2507.16135", "abs": "https://arxiv.org/abs/2507.16135", "authors": ["Chris Bispels", "Matthew Cohen", "Joshua Harrington", "Joshua Lowrance", "Kaelyn Pontes", "Leif Schaumann", "Tony W. H. Wong"], "title": "A further investigation on covering systems with odd moduli", "comment": null, "summary": "Erd\\H{o}s first introduced the idea of covering systems in 1950. Since then,\nmuch of the work in this area has concentrated on identifying covering systems\nthat meet specific conditions on their moduli. Among the central open problems\nin this field is the well-known odd covering problem. In this paper, we\ninvestigate a variant of that problem, where one odd integer is permitted to\nappear multiple times as a modulus in the covering system, while all remaining\nmoduli are distinct odd integers greater than 1."}
{"id": "2507.15986", "categories": ["math.CO", "05E0, 05C60"], "pdf": "https://arxiv.org/pdf/2507.15986", "abs": "https://arxiv.org/abs/2507.15986", "authors": ["Michael Gonzalez", "Rosa Orellana", "Mario Tomba"], "title": "On the reconstruction of trees from their chromatic symmetric functions", "comment": "This FPSAC extended abstract contains a new proof that diameter 5\n  graphs satisfy the Tree Isomorphism Conjecture", "summary": "We study Stanley's chromatic symmetric function (CSF) for trees when\nexpressed in the star basis. We use the deletion-near-contraction (DNC)\nalgorithm to compute coefficients that occur in the CSF in the star basis. In\nparticular, one of our main results determines the smallest partition in\nlexicographic order that occurs as an indexing partition in the CSF, and we\nalso give a formula for its coefficient. In addition to describing properties\nof trees encoded in the coefficients of the star basis, we give an algorithm\nfor reconstructing trees of diameter less than six."}
{"id": "2507.15866", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15866", "abs": "https://arxiv.org/abs/2507.15866", "authors": ["Marek Vlk", "Premysl Sucha", "Jaroslaw Rudy", "Radoslaw Idzikowski"], "title": "Purchase and Production Optimization in a Meat Processing Plant", "comment": "25 pages, 5 figures", "summary": "The food production industry, especially the meat production sector, faces\nmany challenges that have even escalated due to the recent outbreak of the\nenergy crisis in the European Union. Therefore, efficient use of input\nmaterials is an essential aspect affecting the profit of such companies. This\npaper addresses an optimization problem concerning the purchase and subsequent\nmaterial processing we solved for a meat processing company. Unlike the\nmajority of existing papers, we do not concentrate on how this problem concerns\nsupply chain management, but we focus purely on the production stage. The\nproblem involves the concept of alternative ways of material processing, stock\nof material with different expiration dates, and extra constraints widely\nneglected in the current literature, namely, the minimum order quantity and the\nminimum percentage in alternatives. We prove that each of these two constraints\nmakes the problem \\mbox{$\\mathcal{NP}$-hard}, and hence we design a simple\niterative approach based on integer linear programming that allows us to solve\nreal-life instances even using an open-source integer linear programming\nsolver. Another advantage of this approach is that it mitigates numerical\nissues, caused by the extensive range of data values, we experienced with a\ncommercial solver. The results obtained using real data from the meat\nprocessing company showed that our algorithm can find the optimum solution in a\nfew seconds for all considered use cases."}
{"id": "2507.16315", "categories": ["math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.16315", "abs": "https://arxiv.org/abs/2507.16315", "authors": ["Felix Benning"], "title": "A Distributional View of High Dimensional Optimization", "comment": "Most chapters reproduces work that was conducted during my PhD. The\n  review of classical worst-case optimization and Bayesian Optimization is\n  unpublished and may present a novel perspective. While it is not difficult to\n  do, building Machine Learning Theory from exchangeable data is also fairly\n  non-standard and offers an intuitive explanation for many canonical loss\n  functions", "summary": "This PhD thesis presents a distributional view of optimization in place of a\nworst-case perspective. We motivate this view with an investigation of the\nfailure point of classical optimization. Subsequently we consider the\noptimization of a randomly drawn objective function. This is the setting of\nBayesian Optimization. After a review of Bayesian optimization we outline how\nsuch a distributional view may explain predictable progress of optimization in\nhigh dimension. It further turns out that this distributional view provides\ninsights into optimal step size control of gradient descent. To enable these\nresults, we develop mathematical tools to deal with random input to random\nfunctions and a characterization of non-stationary isotropic covariance\nkernels. Finally, we outline how assumptions about the data, specifically\nexchangability, can lead to random objective functions in machine learning and\nanalyze their landscape."}
{"id": "2507.16134", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16134", "abs": "https://arxiv.org/abs/2507.16134", "authors": ["Baofu Han", "Bing Li", "Yining Qi", "Raja Jurdak", "Kaibin Huang", "Chau Yuen"], "title": "DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT", "comment": null, "summary": "Privacy-Preserving Federated Learning (PPFL) has emerged as a secure\ndistributed Machine Learning (ML) paradigm that aggregates locally trained\ngradients without exposing raw data. To defend against model poisoning threats,\nseveral robustness-enhanced PPFL schemes have been proposed by integrating\nanomaly detection. Nevertheless, they still face two major challenges: (1) the\nreliance on heavyweight encryption techniques results in substantial\ncommunication and computation overhead; and (2) single-strategy defense\nmechanisms often fail to provide sufficient robustness against adaptive\nadversaries. To overcome these challenges, we propose DP2Guard, a lightweight\nPPFL framework that enhances both privacy and robustness. DP2Guard leverages a\nlightweight gradient masking mechanism to replace costly cryptographic\noperations while ensuring the privacy of local gradients. A hybrid defense\nstrategy is proposed, which extracts gradient features using singular value\ndecomposition and cosine similarity, and applies a clustering algorithm to\neffectively identify malicious gradients. Additionally, DP2Guard adopts a trust\nscore-based adaptive aggregation scheme that adjusts client weights according\nto historical behavior, while blockchain records aggregated results and trust\nscores to ensure tamper-proof and auditable training. Extensive experiments\nconducted on two public datasets demonstrate that DP2Guard effectively defends\nagainst four advanced poisoning attacks while ensuring privacy with reduced\ncommunication and computation costs."}
{"id": "2507.16138", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2507.16138", "abs": "https://arxiv.org/abs/2507.16138", "authors": ["Theresa C. Anderson", "Adam Bertelli", "Evan M. O'Dorney"], "title": "The structure of the double discriminant", "comment": "8 pages", "summary": "For a polynomial $f(x) = \\sum_{i=0}^n a_i x^i$, we study the double\ndiscriminant $DD_{n,k} = \\operatorname{disc}_{a_k} \\operatorname{disc}_x f(x)$,\nwhich appears in the proof of the van der Waerden--Bhargava theorem. We\nconjecture that $DD_{n,k}$ is the product of a square, a cube, and possibly a\nlinear monomial and we prove this when $k=0$. We also investigate the\n(typically large and smooth) outlying integer constant in the factorization of\n$DD_{n,k}$."}
{"id": "2507.16009", "categories": ["math.CO", "51E05, 51E10"], "pdf": "https://arxiv.org/pdf/2507.16009", "abs": "https://arxiv.org/abs/2507.16009", "authors": ["Taras Banakh", "Ivan Hetman", "Alex Ravsky"], "title": "New Steiner systems $S(2,6,v)$ with block length 6", "comment": "11 pages", "summary": "In this paper various Steiner systems $S(2,k,v)$ for $k = 6$ are collected\nand enumerated for specific constructions. In particular, two earlier unknown\ntypes of $1$-rotational designs are found for the groups $SL(2,5)$ and\n$((\\mathbb Z_3 \\times \\mathbb Z_3) \\rtimes \\mathbb Z_3) \\times \\mathbb Z_5$.\nAlso new Steiner systems $S(2,6,96), S(2,6,106), S(2,6,111)$ are listed."}
{"id": "2507.15874", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15874", "abs": "https://arxiv.org/abs/2507.15874", "authors": ["Yin Wu", "Daniel Slieter", "Vivek Subramanian", "Ahmed Abouelazm", "Robin Bohn", "J. Marius Zöllner"], "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM", "comment": null, "summary": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios."}
{"id": "2507.16412", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16412", "abs": "https://arxiv.org/abs/2507.16412", "authors": ["Qingxin Meng", "Yiwei Wu"], "title": "Discrete-Time LQ Stochastic Two Person Nonzero Sum Difference Games With Random Coefficients:~Closed-Loop Nash Equilibrium", "comment": null, "summary": "This paper investigates closed-loop Nash equilibria for discrete-time\nlinear-quadratic (LQ) stochastic nonzero-sum difference games with random\ncoefficients. Unlike existing works, we consider randomness in both state\ndynamics and cost functionals, leading to a complex structure of fully coupled\ncross-coupled stochastic Riccati equations (CCREs). The key contributions lie\nin characterizing the equilibrium via state-feedback strategies derived by\ndecoupling stochastic Hamiltonian systems governed by two symmetric CCREs-these\nrandom coefficients induce a higher-order nonlinear backward stochastic\ndifference equation (BS$\\triangle$E) system, fundamentally differing from\ndeterministic counterparts. Under minimal regularity conditions, we establish\nnecessary and sufficient conditions for closed-loop Nash equilibrium existence,\ncontingent on the regular solvability of CCREs without requiring strong\nassumptions. Solutions are constructed using a dynamic programming principle\n(DPP), linking equilibrium strategies to coupled Lyapunov-type equations. Our\nanalysis resolves critical challenges in modeling inherent randomness and\nprovides a unified framework for dynamic decision-making under uncertainty."}
{"id": "2507.16164", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.16164", "abs": "https://arxiv.org/abs/2507.16164", "authors": ["Eldor Abdukhamidov", "Tamer Abuhmed", "Joanna C. S. Santos", "Mohammed Abuhamad"], "title": "Attacking interpretable NLP systems", "comment": null, "summary": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples."}
{"id": "2507.16225", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2507.16225", "abs": "https://arxiv.org/abs/2507.16225", "authors": ["David Savitt"], "title": "An elementary proof of Newman's eta-quotient theorem", "comment": "12 pages. To appear, Research in Number Theory", "summary": "Let eta(z) be the Dedekind eta function. Newman studied the modularity of\neta-quotients, giving necessary and sufficient conditions for a function of the\nform \\prod_{0 < m | N} eta(mz)^{r_m} to be a (weakly) holomorphic modular form\nof level N. We explain a proof of Newman's theorem, developed while teaching a\nclass for talented high school students at Canada/USA Mathcamp. The key\nobservation is that although Gamma_1(N) is not generated by its upper\ntriangular and lower triangular subgroups, it is generated by those subgroups\ntogether with any congruence subgroup. Modularity with respect to some\ncongruence subgroup is established using one simple identity involving the\nmultiplier system of eta(z), whose proof is elementary in the sense that it\navoids the use of Dedekind sums."}
{"id": "2507.16169", "categories": ["math.CO", "05C69 (Primary) 05C12, 05B30, 05C15 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.16169", "abs": "https://arxiv.org/abs/2507.16169", "authors": ["Briana Foster-Greenwood", "Christine Uhl"], "title": "Metric Dimension of a Direct Product of Three Complete Graphs: The Middle Cone Family", "comment": "2 figures, 2 tables", "summary": "In previous work, we determined the metric dimension for a direct product of\nthree isomorphic complete graphs. Turning to the case where the complete graphs\nmay have different orders, there are three families we refer to as the upper,\nlower, and middle cones. We determine the metric dimension and\nlocation-total-domination number for a family of direct products of three\ncomplete graphs stemming from the middle cone. We explicitly describe minimum\nresolving sets. To verify the sets are resolving, we define a basic landmark\nsystem and show it will be a resolving set if and only if its associated\n3-edge-colored hypergraph avoids three types of forbidden subgraphs. This\ngeneralizes the technique used for three isomorphic factors."}
{"id": "2507.15875", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.15875", "abs": "https://arxiv.org/abs/2507.15875", "authors": ["Jerry Li", "Timothy Oh", "Joseph Hoang", "Vardhit Veeramachaneni"], "title": "Differential Multimodal Transformers", "comment": null, "summary": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities."}
{"id": "2507.16461", "categories": ["math.OC", "34A34, 65K05, 90C30, 93E24"], "pdf": "https://arxiv.org/pdf/2507.16461", "abs": "https://arxiv.org/abs/2507.16461", "authors": ["Bas Symoens", "Morteza Rahimi", "Masoud Ahookhosh"], "title": "Inexact Levenberg-Marquardt methods under Hölder metric subregularity", "comment": "29 pages, 6 figures", "summary": "This paper investigates two inexact Levenberg-Marquardt (LM) methods for\nsolving systems of nonlinear equations. Both approaches compute approximate\nsearch directions by solving the LM linear system inexactly, subject to\nspecific residual-based conditions. The first method uses an adaptive scheme to\nupdate the LM parameter, and we establish its local superlinear convergence\nunder H\\\"older metric subregularity and local H\\\"older continuity of the\ngradient. The second method combines an inexact LM step with a nonmonotone\nquadratic regularization strategy. For this variant, we prove global\nconvergence under the assumption of Lipschitz continuous gradients and derive a\nworst-case global complexity bound, showing that an approximate stationary\npoint can be found in $\\mathcal{O}(\\epsilon^{-2})$ function and gradient\nevaluations. Finally, we justify the use of the LSQR algorithm for efficiently\nsolving the linear systems involved, which is used in our numerical experiment\non several nonlinear systems, including those appearing in real-world\nbiochemical reaction networks, monotone and nonlinear equations, and image\ndeblurring problems."}
{"id": "2507.16203", "categories": ["cs.CR", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16203", "abs": "https://arxiv.org/abs/2507.16203", "authors": ["Rui Guo", "Avinash Ayalasomayajula", "Henian Li", "Jingbo Zhou", "Sujan Kumar Saha", "Farimah Farahmandi"], "title": "SVAgent: AI Agent for Hardware Security Verification Assertion", "comment": null, "summary": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment."}
{"id": "2507.16399", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2507.16399", "abs": "https://arxiv.org/abs/2507.16399", "authors": ["Liqun Qi", "Chunfeng Cui", "Yi Xu"], "title": "The SOS Rank of Biquadratic Forms", "comment": null, "summary": "In 1973, Calder\\'{o}n proved that an $m \\times 2$ positive semidefinite (psd)\nbiquadratic form can always be expressed as the sum of squares (sos) of\n${3m(m+1) \\over 2}$ quadratic forms. Very recently, by applying Hilbert's\ntheorem, we proved that a $2 \\times 2$ psd biquadratic form can always be\nexpressed as the sum of squares of three quadratic forms. This improved\nCalder\\'{o}n's result for $m=2$, and left the sos rank problem of $m \\times 2$\nbiquadratic forms for $m \\ge 3$ to further exploration. In this paper, we show\nthat an $m \\times n$ psd biquadratic form with a nontrivial zero {can} be\nexpressed as an $(m-1) \\times (n-1) \\times 1$ degenerated tripartite quartic\nform. Furthermore, we show that an $m \\times 2$ positive definite (pd)\nbiquadratic form can be expressed as the sum of a square of a quadratic form,\nand an $(m-1) \\times 1 \\times 1$ degenerated tripartite quartic form. Thus, the\nsos rank problem of $m \\times 2$ psd biquadratic forms is reduced to the sos\nrank problem of an $(m-1) \\times 1 \\times 1$ degenerated tripartite quartic\nforms. We then show that an $(m-1) \\times 1 \\times 1$ degenerated tripartite\nquartic form has at least two nontrivial zeros, and the discriminent of a $2\n\\times 1 \\times 1$ degenerated tripartite quartic form can be expressed as the\nsum of the square of three cubic forms."}
{"id": "2507.16275", "categories": ["math.CO", "math.AC"], "pdf": "https://arxiv.org/pdf/2507.16275", "abs": "https://arxiv.org/abs/2507.16275", "authors": ["Nathan Cheung", "Tracy Chin", "Gaku Liu", "Cynthia Vinzant"], "title": "Valuated Delta Matroids and Principal Minors of Hermitian matrices", "comment": "22 pages, 2 figures", "summary": "In this paper we introduce valuated $\\Delta$-matroids, a natural\ngeneralization of two objects of study in matroid theory: valuated matroids and\n$\\Delta$-matroids. We show that these objects exhibit nice properties analogous\nto ordinary valuated matroids. We also show that these objects arise as the\nvaluations of principal minors of a Hermitian matrix over a valued field,\ngeneralizing other forms of $\\Delta$-matroid representability."}
{"id": "2507.15876", "categories": ["cs.AI", "q-fin.PR", "q-fin.ST", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.15876", "abs": "https://arxiv.org/abs/2507.15876", "authors": ["Eric Benhamou", "Jean-Jacques Ohana", "Alban Etienne", "Béatrice Guez", "Ethan Setrouk", "Thomas Jacquot"], "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach", "comment": "13 pages", "summary": "Commodity Trading Advisors (CTAs) have historically relied on trend-following\nrules that operate on vastly different horizons from long-term breakouts that\ncapture major directional moves to short-term momentum signals that thrive in\nfast-moving markets. Despite a large body of work on trend following, the\nrelative merits and interactions of short-versus long-term trend systems remain\ncontroversial. This paper adds to the debate by (i) dynamically decomposing CTA\nreturns into short-term trend, long-term trend and market beta factors using a\nBayesian graphical model, and (ii) showing how the blend of horizons shapes the\nstrategy's risk-adjusted performance."}
{"id": "2507.16496", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16496", "abs": "https://arxiv.org/abs/2507.16496", "authors": ["Salvador Pineda", "Juan Miguel Morales"], "title": "The Sweet Spot of Bound Tightening for Topology Optimization", "comment": null, "summary": "Topology optimization has emerged as a powerful and increasingly relevant\nstrategy for enhancing the flexibility and efficiency of power system\noperations. However, solving these problems is computationally demanding due to\ntheir combinatorial nature and the use of big-M formulations.\nOptimization-based bound tightening (OBBT) is a well-known strategy to improve\nthe solution of mixed-integer linear programs (MILPs) by computing tighter\nbounds for continuous variables. Yet, existing OBBT approaches in topology\noptimization typically relax all switching decisions in the bounding\nsubproblems, leading to excessively loose feasible regions and limited bound\nimprovements. In this work, we propose a topology-aware bound tightening method\nthat uses network structure to determine which switching variables to relax.\nThrough extensive computational experiments on the IEEE 118-bus system, we find\nthat keeping a small subset of switching variables as binary, while relaxing\nthe rest, strikes a sweet spot between the computational effort required to\nsolve the bounding problems and the tightness of the resulting bounds."}
{"id": "2507.16241", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16241", "abs": "https://arxiv.org/abs/2507.16241", "authors": ["Paul R. B. Houssel", "Siamak Layeghy", "Priyanka Singh", "Marius Portmann"], "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models", "comment": null, "summary": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer."}
{"id": "2507.16503", "categories": ["math.NT", "11J13 11J61 11J68"], "pdf": "https://arxiv.org/pdf/2507.16503", "abs": "https://arxiv.org/abs/2507.16503", "authors": ["Yann Bugeaud", "Bernard de Mathan"], "title": "Simultaneous multiplicative rational approximation to a real and a $p$-adic numbers", "comment": "To appear in the Journal of Number Theory", "summary": "We give new examples of pairs composed of a real and a $p$-adic numbers that\nsatisfy a conjecture on simultaneous multiplicative approximation by rational\nnumbers formulated by Einsiedler and Kleinbock in 2007."}
{"id": "2507.16301", "categories": ["math.CO", "math.GR", "05C15 (Primary) 05C25, 05C76 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.16301", "abs": "https://arxiv.org/abs/2507.16301", "authors": ["Amitayu Banerjee", "Alexa Gopaulsingh", "Zalán Molnár"], "title": "On distinguishing coloring and some variants of proper coloring of graphs derived from subdivision operations", "comment": "13 pages containing 5 figures and 2 tables. Furthermore, we added an\n  appendix (of 2 pages) for convenience", "summary": "Let G be a simple, finite, connected, and undirected graph, and T be a finite\ntree. The middle graph M(G) of G is obtained from the subdivision graph S(G)\nafter joining pairs of subdivided vertices that lie on adjacent edges of G and\nthe central graph C(G) of G is obtained from S(G) after joining all\nnon-adjacent vertices of G.\n  We show that if the order of G is at least 4, then Aut(G), Aut(C(G)), and\nAut(M(G)) are isomorphic (as abstract groups) and apply these results to obtain\nnew sharp upper bounds of the distinguishing number and the distinguishing\nindex of C(G) and M(G) inspired by an algorithm due to Kalinowski, Pilsniak,\nand Wozniak from 2016.\n  Furthermore, we study the total distinguishing chromatic number of C(G) and\nS(G), use Latin squares to verify the AVD-total coloring conjecture for central\ngraphs of regular graphs and some other classes of graphs (which is a partial\nprogress towards answering an open question of Panda, Verma, and Keerti from\n2020), and obtain new bounds of the total dominator chromatic number of C(G)\nand C(T)."}
{"id": "2507.15877", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15877", "abs": "https://arxiv.org/abs/2507.15877", "authors": ["Simon Ouellette"], "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning", "comment": null, "summary": "We run a controlled compositional generalization experiment in the ARC-AGI\ndomain: an open-world problem domain in which the ability to generalize\nout-of-distribution is, by design, an essential characteristic for success. We\ncompare neural program synthesis and test-time fine-tuning approaches on this\nexperiment. We find that execution-guided neural program synthesis outperforms\nall reference algorithms in its ability to compose novel solutions. Our\nempirical findings also suggest that the success of TTFT on ARC-AGI lies mainly\nin eliciting in-distribution knowledge that the LLM otherwise fails to rely on\ndirectly."}
{"id": "2507.16519", "categories": ["math.OC", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.16519", "abs": "https://arxiv.org/abs/2507.16519", "authors": ["Huangxin Chen", "Piaopiao Dong", "Dong Wang", "Xiao-Ping Wang"], "title": "A robust and stable phase field method for structural topology optimization", "comment": "26 pages, 15 figures", "summary": "This paper presents a novel phase-field-based methodology for solving minimum\ncompliance problems in topology optimization under fixed external loads and\nbody forces. The proposed framework characterizes the optimal structure through\nan order parameter function, analogous to phase-field models in materials\nscience, where the design domain and its boundary are intrinsically represented\nby the order parameter function. The topology optimization problem is\nreformulated as a constrained minimization problem with respect to this order\nparameter, requiring simultaneous satisfaction of three critical properties:\nbound preservation, volume conservation, and monotonic objective functional\ndecay throughout the optimization process. The principal mathematical challenge\narises from handling domain-dependent body forces, which necessitates the\ndevelopment of a constrained optimization framework. To address this, we\ndevelop an operator-splitting algorithm incorporating Lagrange multipliers,\nenhanced by a novel limiter mechanism. This hybrid approach guarantees strict\nbound preservation, exact volume conservation, and correct objective functional\ndecaying rate. Numerical implementation demonstrates the scheme's robustness\nthrough comprehensive 2D and 3D benchmarks."}
{"id": "2507.16276", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16276", "abs": "https://arxiv.org/abs/2507.16276", "authors": ["Lambard Maxence", "Bertelle Cyrille", "Duvallet Claude"], "title": "From Contracts to Code: Automating Smart Contract Generation with Multi-Level Finite State Machines", "comment": null, "summary": "In an increasingly complex contractual landscape, the demand for\ntransparency, security, and efficiency has intensified. Blockchain technology,\nwith its decentralized and immutable nature, addresses these challenges by\nreducing intermediary costs, minimizing fraud risks, and enhancing system\ncompatibility. Smart contracts, initially conceptualized by Nick Szabo and\nlater implemented on the Ethereum blockchain, automate and secure contractual\nclauses, offering a robust solution for various industries. However, their\ncomplexity and the requirement for advanced programming skills present\nsignificant barriers to widespread adoption. This study introduces a\nmulti-level finite state machine model designed to represent and track the\nexecution of smart contracts. Our model aims to simplify smart contract\ndevelopment by providing a formalized framework that abstracts underlying\ntechnical complexities, making it accessible to professionals without deep\ntechnical expertise. The hierarchical structure of the multi-level finite state\nmachine enhances contract modularity and traceability, facilitating detailed\nrepresentation and evaluation of functional properties. The paper explores the\npotential of this multi-level approach, reviewing existing methodologies and\ntools, and detailing the smart contract generation process with an emphasis on\nreusable components and modularity. We also conduct a security analysis to\nevaluate potential vulnerabilities in our model, ensuring the robustness and\nreliability of the generated smart contracts."}
{"id": "2507.16536", "categories": ["math.NT", "math.CA", "Primary 28A80, 11A55, 11K50, Secondary 26A18, 37E05, 33E20"], "pdf": "https://arxiv.org/pdf/2507.16536", "abs": "https://arxiv.org/abs/2507.16536", "authors": ["Min Woong Ahn"], "title": "Hausdorff dimension of the graph of the error-sum function of continued fractions", "comment": "23 pages", "summary": "We study the unweighted error-sum function $\\mathcal{E}(x) \\coloneqq \\sum_{n\n\\geq 0} ( x- p_n(x)/q_n(x) )$, where $p_n(x)/q_n(x)$ is the $n$th convergent of\nthe continued fraction expansion of $x \\in \\mathbb{R}$. We prove that the\nHausdorff dimension of the graph of $\\mathcal{E}$ is exactly $1$. Our proof is\nnumber-theoretic in nature and involves M\\\"obius inversion, summation over\ncoprime convergent denominators, and precise upper bounds derived via continued\nfraction recurrence relations. As a supplementary result, we rederive the known\nupper bound of $3/2$ for the Hausdorff dimension of the graph of the relative\nerror-sum function $P(x) \\coloneqq \\sum_{n \\geq 0} (q_n(x)x-p_n(x))$."}
{"id": "2507.16309", "categories": ["math.CO", "05C62"], "pdf": "https://arxiv.org/pdf/2507.16309", "abs": "https://arxiv.org/abs/2507.16309", "authors": ["Vinny Susan Prebhath", "Sudev Naduvath"], "title": "$s$-Shunt Intersection Graph of a Graph", "comment": null, "summary": "The intersection graph of a family of sets $\\{S_{1},S_{2},\\ldots,S_{n}\\}$ is\na graph whose vertex set is $\\{S_{1},S_{2},\\ldots,S_{n}\\}$ and two distinct\nvertices are adjacent if the intersection of the corresponding sets is\nnon-empty. Different types of intersection graphs have been studied depending\non the nature of sets taken as the vertex set. A study on a particular type of\nintersection graph called $s$-shunt intersection graph, generated from the\n$s$-arcs of a given graph is initiated in this paper."}
{"id": "2507.15880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15880", "abs": "https://arxiv.org/abs/2507.15880", "authors": ["Andy E. Williams"], "title": "The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture", "comment": null, "summary": "Intelligence-biological, artificial, or collective-requires structural\ncoherence across recursive reasoning processes to scale effectively. As complex\nsystems grow, coherence becomes fragile unless a higher-order structure ensures\nsemantic consistency. This paper introduces the Recursive Coherence Principle\n(RCP): a foundational constraint stating that for any reasoning system of order\nN, composed of systems operating over conceptual spaces of order N-1, semantic\ncoherence is preserved only by a recursively evaluable generalization operator\nthat spans and aligns those lower-order conceptual spaces. Crucially, this\ncoherence enables structural alignment. Without recursive coherence, no system\ncan reliably preserve goals, meanings, or reasoning consistency at scale. We\nformally define the Functional Model of Intelligence (FMI) as the only known\noperator capable of satisfying the RCP at any scale. The FMI is a minimal,\ncomposable architecture with internal functions (evaluation, modeling,\nadaptation, stability, decomposition, bridging) and external functions\n(storage, recall, System 1 and System 2 reasoning) vital for preserving\nsemantic structure across inference and coordination layers. We prove that any\nsystem lacking the FMI will experience recursive coherence breakdown as it\nscales, arguing that common AI issues like misalignment, hallucination, and\ninstability are symptoms of this structural coherence loss. Unlike other\nfoundational principles, RCP uniquely captures the internal, recursive dynamics\nneeded for coherent, alignable intelligence, modeling semantic coherence under\nrecursion. This work significantly impacts AI alignment, advocating a shift\nfrom behavioral constraints to structural coherence, and offers a pathway for\nsafely generalizable, robustly coherent AI at scale."}
{"id": "2507.16560", "categories": ["math.OC", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.16560", "abs": "https://arxiv.org/abs/2507.16560", "authors": ["Garima Gupta", "Jaydev Dabas"], "title": "Study on Control Problem of a Impulsive Neutral Integro-Differential Equations with Fading Memory", "comment": null, "summary": "This article addresses control problems for semilinear impulsive neutral\nintegro-differential equations with memory in a Banach space. It investigates\nthe approximate controllability of linear and semilinear systems and proves the\nestablishment of mild solutions in the semilinear setting. The approach\ninvolves constructing a resolvent family for the corresponding\nintegro-differential equation of linear type without memory. The results for\nthe linear system are established first, then extended to the semilinear\nscenario, followed by a detailed example to illustrate the theoretical\nfindings."}
{"id": "2507.16291", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16291", "abs": "https://arxiv.org/abs/2507.16291", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "title": "Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers", "comment": "Accepted by EAI ICDF2C 2025", "summary": "Voice phishing (vishing) remains a persistent threat in cybersecurity,\nexploiting human trust through persuasive speech. While machine learning\n(ML)-based classifiers have shown promise in detecting malicious call\ntranscripts, they remain vulnerable to adversarial manipulations that preserve\nsemantic content. In this study, we explore a novel attack vector where large\nlanguage models (LLMs) are leveraged to generate adversarial vishing\ntranscripts that evade detection while maintaining deceptive intent. We\nconstruct a systematic attack pipeline that employs prompt engineering and\nsemantic obfuscation to transform real-world vishing scripts using four\ncommercial LLMs. The generated transcripts are evaluated against multiple ML\nclassifiers trained on a real-world Korean vishing dataset (KorCCViD) with\nstatistical testing. Our experiments reveal that LLM-generated transcripts are\nboth practically and statistically effective against ML-based classifiers. In\nparticular, transcripts crafted by GPT-4o significantly reduce classifier\naccuracy (by up to 30.96%) while maintaining high semantic similarity, as\nmeasured by BERTScore. Moreover, these attacks are both time-efficient and\ncost-effective, with average generation times under 9 seconds and negligible\nfinancial cost per query. The results underscore the pressing need for more\nresilient vishing detection frameworks and highlight the imperative for LLM\nproviders to enforce stronger safeguards against prompt misuse in adversarial\nsocial engineering contexts."}
{"id": "2507.16644", "categories": ["math.NT", "11F30 (Primary) 30C50 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.16644", "abs": "https://arxiv.org/abs/2507.16644", "authors": ["Zeyu Huang", "Timothy Huber", "James McLaughlin", "Pengjun Wang", "Yan Xu", "Dongxi Ye"], "title": "Sign-patterns of Certain Infinite Products", "comment": "19 pages", "summary": "The signs of Fourier coefficients of certain eta quotients are determined by\ndissecting expansions for theta functions and by applying a general dissection\nformula for certain classes of quintuple products. A characterization is given\nfor the coefficient sign patterns for \\[\n\\frac{(q^i;q^i)_{\\infty}}{(q^p;q^p)_{\\infty}} \\] for integers \\( i > 1 \\) and\nprimes \\( p > 3 \\). The sign analysis for this quotient addresses and extends a\nconjecture of Bringmann et al. for the coefficients of \\(\n(q^2;q^2)_{\\infty}(q^5;q^5)_{\\infty}^{-1} \\). The sign distribution for\nadditional classes of eta quotients is considered. This addresses multiple\nconjectures posed by Bringmann et al."}
{"id": "2507.16351", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.16351", "abs": "https://arxiv.org/abs/2507.16351", "authors": ["Luyi Li", "Ping Li", "Guiying Yan", "Qiang Zhou"], "title": "Planar Turán number of disjoint union of $C_3$ and $C_5$", "comment": "11 pages, 9 figures", "summary": "The planar Tur\\'an number of $H$, denoted by $ex_{\\mathcal{P}}(n,H)$, is the\nmaximum number of edges in an $n$-vertex $H$-free planar graph. The planar\nTur\\'an number of $k\\geq 3$ vertex-disjoint union of cycles is the trivial\nvalue $3n-6$. Let $C_{\\ell}$ denote the cycle of length $\\ell$ and\n$C_{\\ell}\\cup C_t$ denote the union of disjoint cycles $C_{\\ell}$ and $C_t$.\nThe planar Tur\\'an number $ex_{\\mathcal{P}}(n,H)$ is known if $H=C_{\\ell}\\cup\nC_k$, where $\\ell,k\\in \\{3,4\\}$. In this paper, we determine the value\n$ex_{\\mathcal{P}}(n,C_3\\cup C_5)=\\lfloor\\frac{8n-13}{3}\\rfloor$ and\ncharacterize the extremal graphs when $n$ is sufficiently large."}
{"id": "2507.15885", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15885", "abs": "https://arxiv.org/abs/2507.15885", "authors": ["Pierluca D'Oro", "Caley Drooff", "Joy Chen", "Joseph Tighe"], "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design", "comment": null, "summary": "Large language models have paved the way to powerful and flexible AI agents,\nassisting humans by increasingly integrating into their daily life. This\nflexibility, potential, and growing adoption demands a holistic and\ncross-disciplinary approach to developing, monitoring and discussing the\ncapabilities required for agent-driven user experiences. However, current\nguidance on human-centered AI agent development is scattered: UX heuristics\nfocus on interface behaviors, engineering taxonomies describe internal\npipelines, and ethics checklists address high-level governance. There is no\nconcise, user-facing vocabulary that tells teams what an agent should\nfundamentally be able to do. We introduce ADEPTS, a capability framework\ndefining a set of core user-facing capabilities to provide unified guidance\naround the development of AI agents. ADEPTS is based on six principles for\nhuman-centered agent design, that express the minimal, user-facing capabilities\nan AI agent should demonstrate to be understandable, controllable and\ntrustworthy in everyday use. ADEPTS complements existing frameworks and\ntaxonomies; differently from them, it sits at the interface between technical\nand experience development. By presenting ADEPTS, we aim to condense complex\nAI-UX requirements into a compact framework that is actionable guidance for AI\nresearchers, designers, engineers, and policy reviewers alike. We believe\nADEPTS has the potential of accelerating the improvement of user-relevant agent\ncapabilities, of easing the design of experiences that take advantage of those\ncapabilities, and of providing a shared language to track and discuss progress\naround the development of AI agents."}
{"id": "2507.16582", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.16582", "abs": "https://arxiv.org/abs/2507.16582", "authors": ["Hanxiao Wang", "Jiongmin Yong"], "title": "Mean-Field Stochastic Linear-Quadratic Optimal Controls: Roles of Expectation and Conditional Expectation Operators", "comment": null, "summary": "This paper investigates a mean-field linear-quadratic optimal control problem\nwhere the state dynamics and cost functional incorporate both expectation and\nconditional expectation terms. We explicitly derive the pre-committed,\nna\\\"{\\i}ve, and equilibrium solutions and establish the well-posedness of the\nassociated Riccati equations. This reveals how the expectation and conditional\nexpectation operators influence time-consistency."}
{"id": "2507.16329", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16329", "abs": "https://arxiv.org/abs/2507.16329", "authors": ["Boheng Li", "Junjie Wang", "Yiming Li", "Zhiyang Hu", "Leyi Qi", "Jianshuo Dong", "Run Wang", "Han Qiu", "Zhan Qin", "Tianwei Zhang"], "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling", "comment": "Preprint version. Under review", "summary": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity."}
{"id": "2507.16671", "categories": ["math.NT", "11F55, 11K70"], "pdf": "https://arxiv.org/pdf/2507.16671", "abs": "https://arxiv.org/abs/2507.16671", "authors": ["Kim Klinger-Logan", "Kalani Thalagoda", "Tian An Wong"], "title": "A Dedekind-Rademacher cocycle for Bianchi groups", "comment": "14 pages", "summary": "We construct a generalization of the Dedekind-Rademacher cocycle to\ncongruence subgroups of $\\mathrm{SL}_2(\\mathbb C)$, and derive some of its\nbasic properties. In particular, we show that it parametrizes a family of\n$L$-values and prove the integrality of these values."}
{"id": "2507.16381", "categories": ["math.CO", "math.AT", "05E45, 55U10"], "pdf": "https://arxiv.org/pdf/2507.16381", "abs": "https://arxiv.org/abs/2507.16381", "authors": ["Xiongfeng Zhan", "Xueyi Huang", "Lu Lu"], "title": "Combinatorial Laplacians and relative Homology of complex pairs", "comment": "29 pages", "summary": "As a discretization of the Hodge Laplacian, the combinatorial Laplacian of\nsimplicial complexes has garnered significant attention. In this paper, we\nstudy combinatorial Laplacians for complex pairs $(X, A)$, where $A$ is a\nsubcomplex of a simplicial complex $X$. We establish a relative version of the\nmatrix-tree theorem for complex pairs, which generalizes both the matrix-tree\ntheorem for simplicial complexes proved by Duval, Klivans, and Martin (2009)\nand the result for Dirichlet eigenvalues of graph pairs by Chung (1996).\nFurthermore, we derive several lower bounds for the spectral gaps of complex\npairs and characterize the equality case for one sharp lower bound. As\nby-products, we obtain sufficient conditions for the vanishing of relative\nhomology. Our results demonstrate that the combinatorial Laplacians for complex\npairs are closely related to relative homology."}
{"id": "2507.15895", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15895", "abs": "https://arxiv.org/abs/2507.15895", "authors": ["Lisa Dargasz"], "title": "Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture", "comment": "Master's thesis, April 2025, 122 pages", "summary": "Reinforcement Learning is a machine learning methodology that has\ndemonstrated strong performance across a variety of tasks. In particular, it\nplays a central role in the development of artificial autonomous agents. As\nthese agents become increasingly capable, market readiness is rapidly\napproaching, which means those agents, for example taking the form of humanoid\nrobots or autonomous cars, are poised to transition from laboratory prototypes\nto autonomous operation in real-world environments. This transition raises\nconcerns leading to specific requirements for these systems - among them, the\nrequirement that they are designed to behave ethically. Crucially, research\ndirected toward building agents that fulfill the requirement to behave\nethically - referred to as artificial moral agents(AMAs) - has to address a\nrange of challenges at the intersection of computer science and philosophy.\nThis study explores the development of reason-based artificial moral agents\n(RBAMAs). RBAMAs are build on an extension of the reinforcement learning\narchitecture to enable moral decision-making based on sound normative\nreasoning, which is achieved by equipping the agent with the capacity to learn\na reason-theory - a theory which enables it to process morally relevant\npropositions to derive moral obligations - through case-based feedback. They\nare designed such that they adapt their behavior to ensure conformance to these\nobligations while they pursue their designated tasks. These features contribute\nto the moral justifiability of the their actions, their moral robustness, and\ntheir moral trustworthiness, which proposes the extended architecture as a\nconcrete and deployable framework for the development of AMAs that fulfills key\nethical desiderata. This study presents a first implementation of an RBAMA and\ndemonstrates the potential of RBAMAs in initial experiments."}
{"id": "2507.16640", "categories": ["math.OC", "90C33, 49J40, 65K15, 68Q25"], "pdf": "https://arxiv.org/pdf/2507.16640", "abs": "https://arxiv.org/abs/2507.16640", "authors": ["M. Marques Alves", "Kangming Chen", "Ellen H. Fukuda"], "title": "An inertial iteratively regularized extragradient method for bilevel variational inequality problems", "comment": null, "summary": "We study a bilevel variational inequality problem where the feasible set is\nitself the solution set of another variational inequality. Motivated by the\ndifficulty of computing projections onto such sets, we consider a regularized\nextragradient method, as proposed by Samadi and Yousefian (2025), which\noperates over a simpler constraint set. Building on this framework, we\nintroduce an inertial variant (called IneIREG) that incorporates momentum\nthrough extrapolation steps. We establish iteration-complexity bounds for the\ngeneral (non-strongly monotone) case under both constant and diminishing\nregularization, and derive improved results under strong monotonicity\nassumptions. Our analysis extends and refines the results of the previous work\nby capturing both inertial and regularization effects within a unified\nframework. Preliminary numerical experiments are also presented to illustrate\nthe behavior of the proposed method."}
{"id": "2507.16372", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16372", "abs": "https://arxiv.org/abs/2507.16372", "authors": ["Tian Dong", "Yan Meng", "Shaofeng Li", "Guoxing Chen", "Zhen Liu", "Haojin Zhu"], "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion", "comment": "Accepted by USENIX Security 2025. Please cite this paper as \"Tian\n  Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu. Depth Gives\n  a False Sense of Privacy: LLM Internal States Inversion. In the 34th USENIX\n  Security Symposium (USENIX Security '25).\"", "summary": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design."}
{"id": "2507.16769", "categories": ["math.NT", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.16769", "abs": "https://arxiv.org/abs/2507.16769", "authors": ["Kathrin Bringmann", "Catherine Cossaboom", "William Craig"], "title": "Overpartitions with parts separated by parity", "comment": null, "summary": "In this paper, we generalize Andrews' partitions separated by parity to\noverpartitions in two ways. We investigate the generating functions for 16\noverpartition families whose parts are separated by parity, and we prove\nvarious $q$-series identities for these functions. These identities include\nrelations to modular forms, $q$-hypergeometric series, and mock modular forms."}
{"id": "2507.16387", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.16387", "abs": "https://arxiv.org/abs/2507.16387", "authors": ["Michel Mollard"], "title": "$p$-th order generalized Fibonacci cubes and maximal cubes in Fibonacci $p$-cubes", "comment": "IF-PREPUB", "summary": "The Fibonacci cube $\\Gamma_n$ is the subgraph of the hypercube $Q_n$ induced\nby vertices with no consecutive 1s. We study a one parameter generalization,\np-th order Fibonacci cubes $\\Gamma^{(p)}_n$, which are subgraphs of $Q_n$\ninduced by strings without p consecutive 1s. We show the link between vertices\nof $\\Gamma^{(p)}_n$ and compositions of integers with parts in $\\{1, 2, \\ldots\n, p\\}$. Among other eumerative properties, we study the order, size and cube\npolynomial of $\\Gamma^{(p)}_n$ as well as their generating functions. Many of\nthe given expressions are similar to those for Fibonacci cubes, where the\n$p$-nomial coefficients play the role of binomial coefficients. We also show\nthat maximal induced hypercubes in Fibonacci $p$-cubes $\\Gamma^p_n$ , another\ngeneralization of Fibonacci cubes, are connected to vertices of $(p + 1)$-th\norder Fibonacci cubes. We use this link to determine the maximal cube\npolynomial of Fibonacci $p$-cubes."}
{"id": "2507.15901", "categories": ["cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15901", "abs": "https://arxiv.org/abs/2507.15901", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet"], "title": "Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation", "comment": null, "summary": "The implementation of Artificial Intelligence (AI) in household environments,\nespecially in the form of proactive autonomous agents, brings about\npossibilities of comfort and attention as well as it comes with intra or\nextramural ethical challenges. This article analyzes agentic AI and its\napplications, focusing on its move from reactive to proactive autonomy,\nprivacy, fairness and user control. We review responsible innovation\nframeworks, human-centered design principles, and governance practices to\ndistill practical guidance for ethical smart home systems. Vulnerable user\ngroups such as elderly individuals, children, and neurodivergent who face\nhigher risks of surveillance, bias, and privacy risks were studied in detail in\ncontext of Agentic AI. Design imperatives are highlighted such as tailored\nexplainability, granular consent mechanisms, and robust override controls,\nsupported by participatory and inclusive methodologies. It was also explored\nhow data-driven insights, including social media analysis via Natural Language\nProcessing(NLP), can inform specific user needs and ethical concerns. This\nsurvey aims to provide both a conceptual foundation and suggestions for\ndeveloping transparent, inclusive, and trustworthy agentic AI in household\nautomation."}
{"id": "2507.16675", "categories": ["math.OC", "90C30"], "pdf": "https://arxiv.org/pdf/2507.16675", "abs": "https://arxiv.org/abs/2507.16675", "authors": ["Yassine Kamri", "François Glineur", "Julien M. Hendrickx", "Ion Necoara"], "title": "On the Worst-Case Analysis of Cyclic Block Coordinate Descent type Algorithms", "comment": "39 pages, 7 figures", "summary": "We study the worst-case behavior of Block Coordinate Descent (BCD) type\nalgorithms for unconstrained minimization of coordinate-wise smooth convex\nfunctions. This behavior is indeed not completely understood, and the practical\nsuccess of these algorithms is not fully explained by current convergence\nanalyses. We extend the recently proposed Performance Estimation Problem (PEP)\napproach to convex coordinate-wise smooth functions by proposing necessary\ninterpolation conditions. We then exploit this to obtain improved numerical\nupper bounds on the worst-case convergence rate of three different BCD\nalgorithms, namely Cyclic Coordinate Descent (CCD), Alternating Minimization\n(AM), and a Cyclic version of the Random Accelerated Coordinate Descent\nintroduced in Fercoq and Richt\\'arik (2015) (CACD), substantially outperforming\nthe best current bounds in some situations. In addition, we show the\nconvergence of the CCD algorithm with more natural assumptions in the context\nof convex optimization than those typically made in the literature. Our\nmethodology uncovers a number of phenomena, some of which can be formally\nestablished. These include a scale-invariance property of the worst case of CCD\nwith respect to the coordinate-wise smoothness constants and a lower bound on\nthe worst-case performance of CCD which is equal to the number of blocks times\nthe worst-case of full gradient descent over the class of smooth convex\nfunctions. We also adapt our framework to the analysis of random BCD\nalgorithms, and present numerical results showing that the standard\nacceleration scheme in Fercoq and Richt\\'arik (2015) appears to be inefficient\nfor deterministic algorithms."}
{"id": "2507.16540", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16540", "abs": "https://arxiv.org/abs/2507.16540", "authors": ["Radowanul Haque", "Aftab Ali", "Sally McClean", "Naveed Khan"], "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks", "comment": null, "summary": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage."}
{"id": "2507.15908", "categories": ["math.CO", "math.CA", "math.NT", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.15908", "abs": "https://arxiv.org/abs/2507.15908", "authors": ["Paul Melotti"], "title": "Distribution of roots of Eulerian polynomials", "comment": null, "summary": "We show that the empirical measures of roots of Eulerian polynomials converge\nto a certain log-Cauchy distribution. To do so, we show that the moments of the\nroots of a related family of polynomials not only converge, but are in fact\nultimately constant. These asymptotic moments are expressed in terms of\nN\\\"orlund's numbers."}
{"id": "2507.16464", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.16464", "abs": "https://arxiv.org/abs/2507.16464", "authors": ["Bela Vizvari", "Gergely Kovacs", "Benedek Nagy", "Necet Deniz Turgay"], "title": "Hilbert basis in the face-centered cubic grid -- mathematical proofs", "comment": null, "summary": "The Hilbert basis is fundamental in describing the structure of the integer\npoints of a polyhedral cone. The face-centered cubic grid is one of the densest\npacking of the 3-dimensional space. The cycles of a grid satisfy the constraint\nset of a pointed, polyhedral cone which contains only non-negative integer\nvectors. The Hilbert basis of a grid gives the structure of the basic cycles in\nthe grid. It is shown in this paper that the basic cycles of the FCC grid\nbelong to 11 types. It is also discussed that how many elements are contained\nin the individual types. The proofs of the paper use geometric, combinatorial,\nalgebraic, and operations research methods."}
{"id": "2507.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15974", "abs": "https://arxiv.org/abs/2507.15974", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Weichen Yu", "Chawin Sitawarin", "Vikash Sehwag", "Prateek Mittal"], "title": "Does More Inference-Time Compute Really Help Robustness?", "comment": "Preprint", "summary": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications."}
{"id": "2507.16576", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16576", "abs": "https://arxiv.org/abs/2507.16576", "authors": ["Ahmed Lekssays", "Husrev Taha Sencar", "Ting Yu"], "title": "From Text to Actionable Intelligence: Automating STIX Entity and Relationship Extraction", "comment": "This paper is accepted at RAID 2025", "summary": "Sharing methods of attack and their effectiveness is a cornerstone of\nbuilding robust defensive systems. Threat analysis reports, produced by various\nindividuals and organizations, play a critical role in supporting security\noperations and combating emerging threats. To enhance the timeliness and\nautomation of threat intelligence sharing, several standards have been\nestablished, with the Structured Threat Information Expression (STIX) framework\nemerging as one of the most widely adopted. However, generating STIX-compatible\ndata from unstructured security text remains a largely manual, expert-driven\nprocess. To address this challenge, we introduce AZERG, a tool designed to\nassist security analysts in automatically generating structured STIX\nrepresentations. To achieve this, we adapt general-purpose large language\nmodels for the specific task of extracting STIX-formatted threat data. To\nmanage the complexity, the task is divided into four subtasks: entity detection\n(T1), entity type identification (T2), related pair detection (T3), and\nrelationship type identification (T4). We apply task-specific fine-tuning to\naccurately extract relevant entities and infer their relationships in\naccordance with the STIX specification. To address the lack of training data,\nwe compiled a comprehensive dataset with 4,011 entities and 2,075 relationships\nextracted from 141 full threat analysis reports, all annotated in alignment\nwith the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49%\nfor T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated\ntheir performance against a range of open- and closed-parameter models, as well\nas state-of-the-art methods, demonstrating improvements of 2-25% across tasks."}
{"id": "2507.16469", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.16469", "abs": "https://arxiv.org/abs/2507.16469", "authors": ["Nawaf Shafi Alshammari", "Sergey Kitaev", "Artem Pyatkin"], "title": "On the representation number of grid graphs and cylindric grid graphs", "comment": null, "summary": "The representation number of a graph is the minimum number of copies of each\nvertex required to represent the graph as a word, such that the letters\ncorresponding to vertices $x$ and $y$ alternate if and only if $xy$ is an edge\nin the graph. It is known that path graphs, circle graphs, and ladder graphs\nhave representation number 2, while prism graphs have representation number 3.\n  In this paper, we extend these results by showing that generalizations of the\naforementioned graphs -- namely, the $m \\times n$ grid graphs and $m \\times n$\ncylindrical grid graphs -- have representation number $3$ for $m \\geq 3$ and $m\n\\geq 2$, respectively, and $n\\geq 3$. Furthermore, we discuss toroidal grid\ngraphs in the context of word-representability, which leads to an interesting\nconjecture."}
{"id": "2507.16020", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16020", "abs": "https://arxiv.org/abs/2507.16020", "authors": ["Xi Yang", "Jiachen Wang", "Song Han", "Suining He"], "title": "Micromobility Flow Prediction: A Bike Sharing Station-level Study via Multi-level Spatial-Temporal Attention Neural Network", "comment": "6 pages, UrbComp 2024", "summary": "Efficient use of urban micromobility resources such as bike sharing is\nchallenging due to the unbalanced station-level demand and supply, which causes\nthe maintenance of the bike sharing systems painstaking. Prior efforts have\nbeen made on accurate prediction of bike traffics, i.e., demand/pick-up and\nreturn/drop-off, to achieve system efficiency. However, bike station-level\ntraffic prediction is difficult because of the spatial-temporal complexity of\nbike sharing systems. Moreover, such level of prediction over entire bike\nsharing systems is also challenging due to the large number of bike stations.\nTo fill this gap, we propose BikeMAN, a multi-level spatio-temporal attention\nneural network to predict station-level bike traffic for entire bike sharing\nsystems. The proposed network consists of an encoder and a decoder with an\nattention mechanism representing the spatial correlation between features of\nbike stations in the system and another attention mechanism describing the\ntemporal characteristic of bike station traffic. Through experimental study on\nover 10 millions trips of bike sharing systems (> 700 stations) of New York\nCity, our network showed high accuracy in predicting the bike station traffic\nof all stations in the city."}
{"id": "2507.16585", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16585", "abs": "https://arxiv.org/abs/2507.16585", "authors": ["Ahmed Lekssays", "Hamza Mouhcine", "Khang Tran", "Ting Yu", "Issa Khalil"], "title": "LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models", "comment": "This paper is accepted at USENIX 2025", "summary": "Software vulnerabilities present a persistent security challenge, with over\n25,000 new vulnerabilities reported in the Common Vulnerabilities and Exposures\n(CVE) database in 2024 alone. While deep learning based approaches show promise\nfor vulnerability detection, recent studies reveal critical limitations in\nterms of accuracy and robustness: accuracy drops by up to 45% on rigorously\nverified datasets, and performance degrades significantly under simple code\nmodifications. This paper presents LLMxCPG, a novel framework integrating Code\nProperty Graphs (CPG) with Large Language Models (LLM) for robust vulnerability\ndetection. Our CPG-based slice construction technique reduces code size by\n67.84 to 90.93% while preserving vulnerability-relevant context. Our approach's\nability to provide a more concise and accurate representation of code snippets\nenables the analysis of larger code segments, including entire projects. This\nconcise representation is a key factor behind the improved detection\ncapabilities of our method, as it can now identify vulnerabilities that span\nmultiple functions. Empirical evaluation demonstrates LLMxCPG's effectiveness\nacross verified datasets, achieving 15-40% improvements in F1-score over\nstate-of-the-art baselines. Moreover, LLMxCPG maintains high performance across\nfunction-level and multi-function codebases while exhibiting robust detection\nefficacy under various syntactic code modifications."}
{"id": "2507.16500", "categories": ["math.CO", "05B10, 05C07, 05C12, 05C30, 05C75"], "pdf": "https://arxiv.org/pdf/2507.16500", "abs": "https://arxiv.org/abs/2507.16500", "authors": ["Johan Kok"], "title": "Integer sequences with conjectured relation with certain graph parameters of the family of linear Jaco graphs", "comment": null, "summary": "This experimental study presents some interesting conjectured relations\nbetween some integer sequences and certain graph parameters of the family of\nlinear Jaco graphs $J_n(x)$ where $n = 1,2,3,\\dots$. It appears that\n$\\textit{Golden ratio}$-like floor function terms play an important role in the\nanalysis of the graph structural properties of the family of linear Jaco\ngraphs. The experimental methodology to obtain the conjectures is indeed\ntrivial. However, it is the author's view that the proofs or disproofs of the\nconjectures may be challenging."}
{"id": "2507.16028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16028", "abs": "https://arxiv.org/abs/2507.16028", "authors": ["Tehseen Rug", "Felix Böhmer", "Tessa Pfattheicher"], "title": "From Logic to Language: A Trust Index for Problem Solving with LLMs", "comment": "17 pages, 2 figures", "summary": "Classical computation, grounded in formal, logical systems, has been the\nengine of technological progress for decades, excelling at problems that can be\ndescribed with unambiguous rules. This paradigm, however, leaves a vast ocean\nof human problems -- those characterized by ambiguity, dynamic environments,\nand subjective context -- largely untouched. The advent of Large Language\nModels (LLMs) represents a fundamental shift, enabling computational systems to\nengage with this previously inaccessible domain using natural language. This\npaper introduces a unified framework to understand and contrast these\nproblem-solving paradigms. We define and delineate the problem spaces\naddressable by formal languages versus natural language. While solutions to the\nformer problem class can be evaluated using binary quality measures, the latter\nrequires a much more nuanced definition of approximate solution space taking\ninto account the vagueness, subjectivity and ambiguity inherent to natural\nlanguage. We therefore introduce a vector-valued trust index Q, which reflects\nsolution quality and distinguishes the binary correctness of formal solutions\nfrom the continuous adequacy spectrum characteristic of natural language\nsolutions. Within this framework, we propose two statistical quality\ndimensions. Normalized bi-semantic entropy measures robustness and conceptual\ndiversity of LLM answers given semantic variation in problem formulations.\nEmotional valence maps subjective valuation of a solution to a quantifiable\nmetric that can be maximized by invoking statistical measures. The concepts\nintroduced in this work will provide a more rigorous understanding of the\ncapabilities, limitations, and inherent nature of problem-solving in the age of\nLLMs."}
{"id": "2507.16773", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16773", "abs": "https://arxiv.org/abs/2507.16773", "authors": ["Yue Li", "Xiao Li", "Hao Wu", "Yue Zhang", "Fengyuan Xu", "Xiuzhen Cheng", "Sheng Zhong"], "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs", "comment": null, "summary": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms."}
{"id": "2507.16593", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.16593", "abs": "https://arxiv.org/abs/2507.16593", "authors": ["Rosário Fernandes"], "title": "On a conjecture concerning the extensions of a reciprocal matrix", "comment": null, "summary": "Let $A$ be a reciprocal matrix of order $n$ and $w$ be its Perron\neigenvector. To infer the efficiency of $w$ for $A$, based on the principle of\nPareto optimal decisions, we study the strong connectivity of a certain digraph\nassociated with $A$ and $w$. A reciprocal matrix $B$ of order $n+1$ is an\nextension of $A$ if the matrix $A$ is obtained from $B$ by removing its last\nrow and column. We prove that there is no extension of a reciprocal matrix\nwhose digraph associated with the extension and its Perron eigenvector has a\nsource, as conjectured by Furtado and Johnson in ``Efficiency analysis for the\nPerron vector of a reciprocal matrix\". As an application, considering $n\\geq 5$\nand $A$ a matrix obtained from a consistent one by perturbing four entries\nabove the main diagonal, $x,y,z,a$, and the corresponding reciprocal entries,\nin a way that there is a submatrix of size $2$ containing the four perturbed\nentries and not containing a diagonal entry, we describe the relations among\n$x,y,z,a$ with which $A$ always has efficient Perron eigenvector."}
{"id": "2507.16067", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.16067", "abs": "https://arxiv.org/abs/2507.16067", "authors": ["Jeroen Spaans", "Jesse Heyninck"], "title": "A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)", "comment": "Full version, including proofs and appendices, of paper accepted at\n  IJCAI 2025", "summary": "Constraint Logic Programming (CLP) is a logic programming formalism used to\nsolve problems requiring the consideration of constraints, like resource\nallocation and automated planning and scheduling. It has previously been\nextended in various directions, for example to support fuzzy constraint\nsatisfaction, uncertainty, or negation, with different notions of semiring\nbeing used as a unifying abstraction for these generalizations. None of these\nextensions have studied clauses with negation allowed in the body. We\ninvestigate an extension of CLP which unifies many of these extensions and\nallows negation in the body. We provide semantics for such programs, using the\nframework of approximation fixpoint theory, and give a detailed overview of the\nimpacts of properties of the semirings on the resulting semantics. As such, we\nprovide a unifying framework that captures existing approaches and allows\nextending them with a more expressive language."}
{"id": "2507.16788", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16788", "abs": "https://arxiv.org/abs/2507.16788", "authors": ["Sebastian Pape", "Anis Bkakria", "Maurice Heymann", "Badreddine Chah", "Abdeljalil Abbas-Turki", "Sarah Syed-Winkler", "Matthias Hiller", "Reda Yaich"], "title": "AUTOPSY: A Framework for Tackling Privacy Challenges in the Automotive Industry", "comment": "19 pages, 4 figures", "summary": "With the General Data Protection Regulation (GDPR) in place, all domains have\nto ensure compliance with privacy legislation. However, compliance does not\nnecessarily result in a privacy-friendly system as for example getting users'\nconsent to process their data does not improve the privacy-friendliness of the\nsystem. Therefore, the goal of the AUTOPSY project was to support the privacy\nengineering process in the automotive domain by providing several building\nblocks which technically improve the privacy-friendliness of modern, i.e.,\nconnected and (partially) automated vehicles. This paper presents the results\nof the AUTOPSY project: a system model to identify relevant entities and\nlocations to apply privacy enhancing technologies (PETs); the privacy manager\naiming at more control of the data flow from the vehicle, a PET selection\napproach based on GDPR principles, and an architectural framework for\nautomotive privacy. Furthermore, we built a demonstrator for location-based\nservices to evaluate the architectural framework."}
{"id": "2507.16622", "categories": ["math.CO", "05C12, 05C69"], "pdf": "https://arxiv.org/pdf/2507.16622", "abs": "https://arxiv.org/abs/2507.16622", "authors": ["Ethan Shallcross", "James Tuite", "Aoise Evans", "Aditi Krishnakumar", "Sumaiyah Boshar"], "title": "Solution to some conjectures on mobile position problems", "comment": null, "summary": "The general position problem for graphs asks for the largest number of\nvertices in a subset $S \\subseteq V(G)$ of a graph $G$ such that for any $u,v\n\\in S$ and any shortest $u,v$-path $P$ we have $S \\cap V(P) = \\{ u,v\\} $,\nwhereas the mutual visibility problem requires only that for any $u,v \\in S$\nthere exists a shortest $u,v$-path with $S \\cap V(P) = \\{ u,v\\} $. In the\nmobile versions of these problems, robots must move through the network in\ngeneral position/mutual visibility such that every vertex is visited by a\nrobot. This paper solves some open problems from the literature. We quantify\nthe effect of adding the restriction that every robot can visit every vertex\n(the so-called \\emph{completely mobile} variants), prove a bound on both mobile\nnumbers in terms of the clique number, and find the mobile mutual visibility\nnumber of line graphs of complete graphs, strong grids and Cartesian grids."}
{"id": "2507.16110", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16110", "abs": "https://arxiv.org/abs/2507.16110", "authors": ["Shengchao Liu", "Hannan Xu", "Yan Ai", "Huanxin Li", "Yoshua Bengio", "Harry Guo"], "title": "Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization", "comment": null, "summary": "Large language models (LLMs) leverage chain-of-thought (CoT) techniques to\ntackle complex problems, representing a transformative breakthrough in\nartificial intelligence (AI). However, their reasoning capabilities have\nprimarily been demonstrated in solving math and coding problems, leaving their\npotential for domain-specific applications-such as battery discovery-largely\nunexplored. Inspired by the idea that reasoning mirrors a form of guided\nsearch, we introduce ChatBattery, a novel agentic framework that integrates\ndomain knowledge to steer LLMs toward more effective reasoning in materials\ndesign. Using ChatBattery, we successfully identify, synthesize, and\ncharacterize three novel lithium-ion battery cathode materials, which achieve\npractical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over\nthe widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this\ndiscovery, ChatBattery paves a new path by showing a successful LLM-driven and\nreasoning-based platform for battery materials invention. This complete\nAI-driven cycle-from design to synthesis to characterization-demonstrates the\ntransformative potential of AI-driven reasoning in revolutionizing materials\ndiscovery."}
{"id": "2507.16226", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16226", "abs": "https://arxiv.org/abs/2507.16226", "authors": ["Dong Ben", "Hui Feng", "Qian Wang"], "title": "Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design", "comment": "7 pages, 4 figures;", "summary": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications."}
{"id": "2507.16625", "categories": ["math.CO", "math.GN", "54E35, 05C63, 05C40"], "pdf": "https://arxiv.org/pdf/2507.16625", "abs": "https://arxiv.org/abs/2507.16625", "authors": ["Max Pitz"], "title": "A metrization theorem for edge-end spaces of infinite graphs", "comment": "12 pages", "summary": "We prove that the edge-end space of an infinite graph is metrizable if and\nonly if it is first-countable. This strengthens a recent result by Aurichi,\nMagalhaes Jr.\\ and Real (2024).\n  Our central graph-theoretic tool is the use of tree-cut decompositions,\nintroduced by Wollan (2015) as a variation of tree decompositions that is based\non edge cuts instead of vertex separations. In particular, we give a new,\nelementary proof for Kurkofka's result (2022) that every infinite graph has a\ntree-cut decomposition of finite adhesion into its $\\omega$-edge blocks. Along\nthe way, we also give a new, short proof for a classic result by Halin (1984)\non $K_{k,\\kappa}$-subdivisions in $k$-connected graphs, making this paper\nself-contained."}
{"id": "2507.16126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16126", "abs": "https://arxiv.org/abs/2507.16126", "authors": ["Michael R. Bock", "Kara Molisee", "Zachary Ozer", "Sumit Shah"], "title": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task", "comment": null, "summary": "Can AI file your taxes? Not yet. Calculating US personal income taxes is a\ntask that requires building an understanding of vast amounts of English text\nand using that knowledge to carefully compute results. We propose TaxCalcBench,\na benchmark for determining models' abilities to calculate personal income tax\nreturns given all of the necessary information. Our experiment shows that\nstate-of-the-art models succeed in calculating less than a third of federal\nincome tax returns even on this simplified sample set. Our analysis concludes\nthat models consistently misuse tax tables, make errors in tax calculation, and\nincorrectly determine eligibility. Our findings point to the need for\nadditional infrastructure to apply LLMs to the personal income tax calculation\ntask."}
{"id": "2507.16694", "categories": ["math.CO", "cs.IT", "math.IT", "51E22, 94B05, 14M15"], "pdf": "https://arxiv.org/pdf/2507.16694", "abs": "https://arxiv.org/abs/2507.16694", "authors": ["Ilaria Cardinali", "Luca Giuzzi"], "title": "Linear codes arising from the point-hyperplane geometry -- Part II: the twisted embedding", "comment": "28 pages", "summary": "Let $\\bar{\\Gamma}$ be the point-hyperplane geometry of a projective space\n$\\mathrm{PG(V)},$ where $V$ is a $(n+1)$-dimensional vector space over a finite\nfield $\\mathbb{F}_q$ of order $q.$ Suppose that $\\sigma$ is an automorphism of\n$\\mathbb{F}_q$ and consider the projective embedding $\\varepsilon_{\\sigma}$ of\n$\\bar{\\Gamma}$ into the projective space $\\mathrm{PG}(V\\otimes V^*)$ mapping\nthe point $([x],[\\xi])\\in \\bar{\\Gamma}$ to the projective point represented by\nthe pure tensor $x^{\\sigma}\\otimes \\xi$, with $\\xi(x)=0.$ In [I. Cardinali, L.\nGiuzzi, Linear codes arising from the point-hyperplane geometry -- part I: the\nSegre embedding (Jun. 2025). arXiv:2506.21309, doi:10.48550/ARXIV.2506.21309]\nwe focused on the case $\\sigma=1$ and we studied the projective code arising\nfrom the projective system $\\Lambda_1=\\varepsilon_{1}(\\bar{\\Gamma}).$ Here we\nfocus on the case $\\sigma\\not=1$ and we investigate the linear code ${\\mathcal\nC}(\\Lambda_{\\sigma})$ arising from the projective system\n$\\Lambda_{\\sigma}=\\varepsilon_{\\sigma}(\\bar{\\Gamma}).$ In particular, after\nhaving verified that $\\mathcal{C}( \\Lambda_{\\sigma})$ is a minimal code, we\ndetermine its parameters, its minimum distance as well as its automorphism\ngroup. We also give a (geometrical) characterization of its minimum and second\nlowest weight codewords and determine its maximum weight when $q$ and $n$ are\nboth odd."}
{"id": "2507.16145", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16145", "abs": "https://arxiv.org/abs/2507.16145", "authors": ["Shuhao Mei", "Yongchao Long", "Shan Cao", "Xiaobo Han", "Shijia Geng", "Jinbo Sun", "Yuxi Zhou", "Shenda Hong"], "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting", "comment": null, "summary": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools."}
{"id": "2507.16730", "categories": ["math.CO", "05C50"], "pdf": "https://arxiv.org/pdf/2507.16730", "abs": "https://arxiv.org/abs/2507.16730", "authors": ["Wei Wang", "Ximei Huang"], "title": "Almost all cographs have a cospectral mate", "comment": "12 pages, 4 figures", "summary": "Complement-reducible graphs (or cographs) are the graphs formed from the\nsingle-vertex graph by the operations of complement and disjoint union. By\ncombining the Johnson-Newman theorem on generalized cospectrality with the\nstandard tools in the asymptotic enumeration of trees, we show that almost all\ncographs have a cospectral mate. This result can be viewed as an analogue to a\nwell-known result by Schwenk, who proved that almost all trees have a\ncospectral mate."}
{"id": "2507.16184", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.16184", "abs": "https://arxiv.org/abs/2507.16184", "authors": ["Myung Ho Kim"], "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)", "comment": "21 pages", "summary": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification."}
{"id": "2507.16759", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.16759", "abs": "https://arxiv.org/abs/2507.16759", "authors": ["Sergey Kurapov", "Maxim Davidovsky"], "title": "Algorithmic methods of finite discrete structures. Topological graph drawing (part IV)", "comment": "67 pages, in Ukrainian language, 83 figures, a preprint of monography", "summary": "The chapter presents mathematical models intended for creating a topological\ndrawing of a non-separable non-planar graph based on the methods of G. Ringel's\nvertex rotation theory. The induced system of cycles generates a topological\ndrawing of a certain thickness. A method for determining the location of\nimaginary vertices by finding the intersection of connections on a plane is\npresented. A topological drawing of a maximum planar subgraph is used as a\nbasis."}
{"id": "2507.16204", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16204", "abs": "https://arxiv.org/abs/2507.16204", "authors": ["Li-Hsiang Shen", "Jyun-Jhe Huang"], "title": "CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted Space-Air-Ground Integrated Networks", "comment": null, "summary": "A space-air-ground integrated network (SAGIN) architecture is proposed,\nempowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)\ncapable of simultaneously reflecting, amplifying, and harvesting wireless\nenergy. The MF-RIS plays a pivotal role in addressing the energy shortages of\nlow-Earth orbit (LEO) satellites operating in shadowed regions, while\nexplicitly accounting for both communication and computing energy consumption\nacross the SAGIN nodes. To maximize the long-term energy efficiency (EE), we\nformulate a joint optimization problem over the MF-RIS parameters, including\nsignal amplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, user association, and computing capability.\nThe formulated problem is highly non-convex and non-linear and contains mixed\ndiscrete-continuous parameters. To tackle this, we conceive a compressed hybrid\nintelligence for twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture achieves superior EE performance due to its\ncomplementary coverage, offering notable advantages over either standalone\nsatellite, aerial, or ground-only deployments."}
{"id": "2507.16765", "categories": ["math.CO", "Primary 05A15, Secondary 11G05, 14H52, 15B36, 11B37, 11B83"], "pdf": "https://arxiv.org/pdf/2507.16765", "abs": "https://arxiv.org/abs/2507.16765", "authors": ["Paul Barry"], "title": "Elliptic Curves, Riordan arrays and Lattice Paths", "comment": "19 pages", "summary": "In this note, we show that to each elliptic curve of the form\n$$y^2-axy-y=x^3-bx^2-cx,$$ we can associate a family of lattice paths whose\nstep set is determined by the parameters of the elliptic curve. The enumeration\nof these lattice paths is by means of an associated Riordan array. The curves\nand the paths have associated Somos $4$ sequences which are essentially the\nsame. For the curves the link to Somos $4$ sequences is a classical result, via\nthe elliptic divisibility sequence. For the paths the link is via a Hankel\ntransform."}
{"id": "2507.16226", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16226", "abs": "https://arxiv.org/abs/2507.16226", "authors": ["Dong Ben", "Hui Feng", "Qian Wang"], "title": "Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design", "comment": "7 pages, 4 figures;", "summary": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications."}
{"id": "2507.16804", "categories": ["math.CO", "05C35"], "pdf": "https://arxiv.org/pdf/2507.16804", "abs": "https://arxiv.org/abs/2507.16804", "authors": ["Zihao Jin", "Sean Longbrake", "Liana Yepremyan"], "title": "Bipartite Turán numbers via edge-gluing", "comment": "20 pages", "summary": "In 1984, Erd\\H{o}s and Simonovits asked the following: given a bipartite\ngraph $H$, do there exist constants $0 \\leq \\alpha < 1$ and $\\beta, C > 0$ such\nthat any graph $G$ on $n$ vertices and $pn^2\\geq C n^{1+ \\alpha}$ edges\ncontains at least $\\beta n^{\\mathrm{v}(H)} p^{\\mathrm{e}(H)}$ copies of $H$?\n  We show that edge-gluing preserves the satisfiability of this conjecture\nunder some mild symmetry conditions. Namely, if two graphs $H_1$ and $H_2$\nsatisfy this conjecture, and if furthermore, gluing them along a fixed edge\nproduces a unique graph then the resulting graph satisfies the conjecture as\nwell. We also show that if $H$ satisfies the conjecture then if we glue several\ncopies of (labeled) $H$ along the same labeled copy of a subforest of $H$ then\nthe resulting graph also satisfies the conjecture.\n  We also show that Zarankiewicz numbers are additive in the order of magnitude\nunder gluing edges. Indeed, for a (signed) bipartite graph $H$ with parts\ncoloured $+$ and $-$, recall $z(m,n, H)$ is the maximum number of edges in a\nsigned bipartite graph $G$ with $+$ side being of size $m$ and $-$ side being\nof size $n$ such that $G$ does not contain a copy of $H$ with $+$ side embedded\nin the $+$ side of $G$. We show that for any two (signed) bipartite graphs\n$H_1$ and $H_2$ if we glue them along an edge preserving the sign of the edge\nthen the resulting graph $H$ satisfies $z(m,n, H) = \\Theta(z(m,n, H_1) + z(m,n,\nH_2))$."}
{"id": "2507.16229", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16229", "abs": "https://arxiv.org/abs/2507.16229", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions."}
{"id": "2507.16648", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.16648", "abs": "https://arxiv.org/abs/2507.16648", "authors": ["Eleon Bach", "Yann Disser", "Sophie Huiberts", "Nils Mosis"], "title": "An unconditional lower bound for the active-set method in convex quadratic maximization", "comment": null, "summary": "We prove that the active-set method needs an exponential number of iterations\nin the worst-case to maximize a convex quadratic function subject to linear\nconstraints, regardless of the pivot rule used. This substantially improves\nover the best previously known lower bound [IPCO 2025], which needs objective\nfunctions of polynomial degrees $\\omega(\\log d)$ in dimension $d$, to a bound\nusing a convex polynomial of degree 2. In particular, our result firmly\nresolves the open question [IPCO 2025] of whether a constant degree suffices,\nand it represents significant progress towards linear objectives, where the\nactive-set method coincides with the simplex method and a lower bound for all\npivot rules would constitute a major breakthrough.\n  Our result is based on a novel extended formulation, recursively constructed\nusing deformed products. Its key feature is that it projects onto a polygonal\napproximation of a parabola while preserving all of its exponentially many\nvertices. We define a quadratic objective that forces the active-set method to\nfollow the parabolic boundary of this projection, without allowing any\nshortcuts along chords corresponding to edges of its full-dimensional preimage."}
{"id": "2507.16280", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16280", "abs": "https://arxiv.org/abs/2507.16280", "authors": ["Tianze Xu", "Pengrui Lu", "Lyumanshan Ye", "Xiangkun Hu", "Pengfei Liu"], "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry", "comment": "22 pages, 3 figures", "summary": "The emergence of deep research systems presents significant capabilities in\nproblem-solving, extending from basic queries to sophisticated research tasks.\nHowever, existing benchmarks primarily evaluate these systems as agents for web\nretrieval and report generation, overlooking their potential to discover novel\ninsights on the frontiers of scientific research. To address this gap, we\nintroduce ResearcherBench, the first benchmark focused on evaluating the\ncapabilities of these advanced, agentic systems - which we refer to as Deep AI\nResearch Systems (DARS) - on frontier AI scientific questions. We compiled a\ndataset of 65 research questions expertly selected from real-world scientific\nscenarios such as laboratory discussions and interviews, spanning 35 different\nAI subjects and categorized into three types: technical details, literature\nreview, and open consulting. Our dual evaluation framework combines rubric\nassessment, which uses expert-designed criteria to evaluate insight quality,\nwith factual assessment, which measures citation accuracy (faithfulness) and\ncoverage (groundedness). We evaluated several leading commercial DARS and\nbaseline systems. Results show that OpenAI Deep Research and Gemini Deep\nResearch significantly outperform other systems, with particular strength in\nopen-ended consulting questions. Such capabilities represent a meaningful step\ntoward AI self-improvement, aligning with the vision of ASI for AI. We\nopen-source ResearcherBench to provide a standardized platform for promoting\nthe development of next-generation AI research assistants, hoping to foster a\nnew perspective in AI research evaluation for a novel pattern of scientific\ncollaboration: https://github.com/GAIR-NLP/ResearcherBench."}
{"id": "2507.16769", "categories": ["math.NT", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.16769", "abs": "https://arxiv.org/abs/2507.16769", "authors": ["Kathrin Bringmann", "Catherine Cossaboom", "William Craig"], "title": "Overpartitions with parts separated by parity", "comment": null, "summary": "In this paper, we generalize Andrews' partitions separated by parity to\noverpartitions in two ways. We investigate the generating functions for 16\noverpartition families whose parts are separated by parity, and we prove\nvarious $q$-series identities for these functions. These identities include\nrelations to modular forms, $q$-hypergeometric series, and mock modular forms."}
{"id": "2507.16296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16296", "abs": "https://arxiv.org/abs/2507.16296", "authors": ["Cairong Zhao", "Yufeng Jin", "Zifan Song", "Haonan Chen", "Duoqian Miao", "Guosheng Hu"], "title": "Cross-Modal Distillation For Widely Differing Modalities", "comment": "14 pages, 9 figures", "summary": "Deep learning achieved great progress recently, however, it is not easy or\nefficient to further improve its performance by increasing the size of the\nmodel. Multi-modal learning can mitigate this challenge by introducing richer\nand more discriminative information as input. To solve the problem of limited\naccess to multi-modal data at the time of use, we conduct multi-modal learning\nby introducing a teacher model to transfer discriminative knowledge to a\nstudent model during training. However, this knowledge transfer via\ndistillation is not trivial because the big domain gap between the widely\ndiffering modalities can easily lead to overfitting. In this work, we introduce\na cross-modal distillation framework. Specifically, we find hard constrained\nloss, e.g. l2 loss forcing the student being exact the same as the teacher, can\neasily lead to overfitting in cross-modality distillation. To address this, we\npropose two soft constrained knowledge distillation strategies at the feature\nlevel and classifier level respectively. In addition, we propose a\nquality-based adaptive weights module to weigh input samples via quantified\ndata quality, leading to robust model training. We conducted experiments on\nspeaker recognition and image classification tasks, and the results show that\nour approach is able to effectively achieve knowledge transfer between the\ncommonly used and widely differing modalities of image, text, and speech."}
{"id": "2507.16322", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16322", "abs": "https://arxiv.org/abs/2507.16322", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical Language Reasoning LLM Benchmarks for African Disease Burdens", "comment": "Preprint. 26 pages, includes appendix and tables", "summary": "Introduction: Existing medical LLM benchmarks largely reflect examination\nsyllabi and disease profiles from high income settings, raising questions about\ntheir validity for African deployment where malaria, HIV, TB, sickle cell\ndisease and other neglected tropical diseases (NTDs) dominate burden and\nnational guidelines drive care. Methodology: We systematically reviewed 31\nquantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English\nmedical QA benchmarks. Alama Health QA was developed using a retrieval\naugmented generation framework anchored on the Kenyan Clinical Practice\nGuidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,\nMedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized\nsemantic profiling (NTD proportion, recency, readability, lexical diversity\nmetrics) and blinded expert rating across five dimensions: clinical relevance,\nguideline alignment, clarity, distractor plausibility, and language/cultural\nfit. Results: Alama Health QA captured >40% of all NTD mentions across corpora\nand the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB\n(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global\nbenchmarks showed minimal representation (e.g., sickle cell disease absent in\nthree sets) despite large scale. Qualitatively, Alama scored highest for\nrelevance and guideline alignment; PubMedQA lowest for clinical utility.\nDiscussion: Quantitative medical LLM benchmarks widely used in the literature\nunderrepresent African disease burdens and regulatory contexts, risking\nmisleading performance claims. Guideline anchored, regionally curated resources\nsuch as Alama Health QA and expanded disease specific derivatives are essential\nfor safe, equitable model evaluation and deployment across African health\nsystems."}
{"id": "2507.16334", "categories": ["cs.AI", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2507.16334", "abs": "https://arxiv.org/abs/2507.16334", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Higher Gauge Flow Models", "comment": null, "summary": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models."}
{"id": "2507.16356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16356", "abs": "https://arxiv.org/abs/2507.16356", "authors": ["Arpan Dasgupta", "Mizhaan Maniyar", "Awadhesh Srivastava", "Sanat Kumar", "Amrita Mahale", "Aparna Hedge", "Arun Suggala", "Karthikeyan Shanmugam", "Aparna Taneja", "Milind Tambe"], "title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for Improved Message Delivery in Mobile Maternal Health", "comment": null, "summary": "Mobile health (mHealth) programs utilize automated voice messages to deliver\nhealth information, particularly targeting underserved communities,\ndemonstrating the effectiveness of using mobile technology to disseminate\ncrucial health information to these populations, improving health outcomes\nthrough increased awareness and behavioral change. India's Kilkari program\ndelivers vital maternal health information via weekly voice calls to millions\nof mothers. However, the current random call scheduling often results in missed\ncalls and reduced message delivery. This study presents a field trial of a\ncollaborative bandit algorithm designed to optimize call timing by learning\nindividual mothers' preferred call times. We deployed the algorithm with around\n$6500$ Kilkari participants as a pilot study, comparing its performance to the\nbaseline random calling approach. Our results demonstrate a statistically\nsignificant improvement in call pick-up rates with the bandit algorithm,\nindicating its potential to enhance message delivery and impact millions of\nmothers across India. This research highlights the efficacy of personalized\nscheduling in mobile health interventions and underscores the potential of\nmachine learning to improve maternal health outreach at scale."}
{"id": "2507.16370", "categories": ["cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.16370", "abs": "https://arxiv.org/abs/2507.16370", "authors": ["Lucas de Lara"], "title": "Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning", "comment": null, "summary": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples."}
{"id": "2507.16395", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16395", "abs": "https://arxiv.org/abs/2507.16395", "authors": ["Bo Hou", "Xin Tan", "Kai Zheng", "Fang Liu", "Yinghao Zhu", "Li Zhang"], "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning", "comment": null, "summary": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks."}
{"id": "2507.16405", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16405", "abs": "https://arxiv.org/abs/2507.16405", "authors": ["Stassa Patsantzis"], "title": "Self-Supervised Inductive Logic Programming", "comment": null, "summary": "Inductive Logic Programming (ILP) approaches like Meta \\-/ Interpretive\nLearning (MIL) can learn, from few examples, recursive logic programs with\ninvented predicates that generalise well to unseen instances. This ability\nrelies on a background theory and negative examples, both carefully selected\nwith expert knowledge of a learning problem and its solutions. But what if such\na problem-specific background theory or negative examples are not available? We\nformalise this question as a new setting for Self-Supervised ILP and present a\nnew MIL algorithm that learns in the new setting from some positive labelled,\nand zero or more unlabelled examples, and automatically generates, and labels,\nnew positive and negative examples during learning. We implement this algorithm\nin Prolog in a new MIL system, called Poker. We compare Poker to\nstate-of-the-art MIL system Louise on experiments learning grammars for\nContext-Free and L-System languages from labelled, positive example strings, no\nnegative examples, and just the terminal vocabulary of a language, seen in\nexamples, as a first-order background theory. We introduce a new approach for\nthe principled selection of a second-order background theory as a Second Order\nDefinite Normal Form (SONF), sufficiently general to learn all programs in a\nclass, thus removing the need for a backgound theory tailored to a learning\ntask. We find that Poker's performance improves with increasing numbers of\nautomatically generated examples while Louise, bereft of negative examples,\nover-generalises."}
{"id": "2507.16414", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16414", "abs": "https://arxiv.org/abs/2507.16414", "authors": ["Hongyi Tang", "Zhihao Zhu", "Yi Yang"], "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework", "comment": null, "summary": "The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs."}
{"id": "2507.16434", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16434", "abs": "https://arxiv.org/abs/2507.16434", "authors": ["Stassa Patsantzis"], "title": "From model-based learning to model-free behaviour with Meta-Interpretive Learning", "comment": null, "summary": "A \"model\" is a theory that describes the state of an environment and the\neffects of an agent's decisions on the environment. A model-based agent can use\nits model to predict the effects of its future actions and so plan ahead, but\nmust know the state of the environment. A model-free agent cannot plan, but can\nact without a model and without completely observing the environment. An\nautonomous agent capable of acting independently in novel environments must\ncombine both sets of capabilities. We show how to create such an agent with\nMeta-Interpretive Learning used to learn a model-based Solver used to train a\nmodel-free Controller that can solve the same planning problems as the Solver.\nWe demonstrate the equivalence in problem-solving ability of the two agents on\ngrid navigation problems in two kinds of environment: randomly generated mazes,\nand lake maps with wide open areas. We find that all navigation problems solved\nby the Solver are also solved by the Controller, indicating the two are\nequivalent."}
{"id": "2507.16454", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.16454", "abs": "https://arxiv.org/abs/2507.16454", "authors": ["Pierangela Bruno", "Carmine Dodaro", "Giuseppe Galatà", "Marco Maratea", "Marco Mochi"], "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions", "comment": "17 pages, International Conference on Logic Programming, Under\n  consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP)."}
{"id": "2507.16473", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.16473", "abs": "https://arxiv.org/abs/2507.16473", "authors": ["Chang Li", "Yaren Zhang", "Haoran Lv", "Qiong Cao", "Chao Xue", "Xiaodong He"], "title": "Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol."}
{"id": "2507.16478", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16478", "abs": "https://arxiv.org/abs/2507.16478", "authors": ["Shreya Saxena", "Siva Prasad", "Zishan Ahmad", "Vishal Vaddina"], "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training", "comment": null, "summary": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration."}
{"id": "2507.16507", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.16507", "abs": "https://arxiv.org/abs/2507.16507", "authors": ["Jean Lelong", "Adnane Errazine", "Annabelle Blangero"], "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications", "comment": "ECAI 2025 demo track, 4 pages", "summary": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large\nLanguage Models (LLMs) but often fall short on complex queries, delivering\nlimited, extractive answers and struggling with multiple targeted retrievals or\nnavigating intricate entity relationships. This is a critical gap in\nknowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system\nfor exploring the scientific data of INRAE (France's National Research\nInstitute for Agriculture, Food and Environment). INRAExplorer employs an\nLLM-based agent with a multi-tool architecture to dynamically engage a rich\nknowledge base, through a comprehensive knowledge graph derived from open\naccess INRAE publications. This design empowers INRAExplorer to conduct\niterative, targeted queries, retrieve exhaustive datasets (e.g., all\npublications by an author), perform multi-hop reasoning, and deliver\nstructured, comprehensive answers. INRAExplorer serves as a concrete\nillustration of enhancing knowledge interaction in specialized fields."}
{"id": "2507.16534", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16534", "abs": "https://arxiv.org/abs/2507.16534", "authors": ["Shanghai AI Lab", ":", "Xiaoyang Chen", "Yunhao Chen", "Zeren Chen", "Zhiyun Chen", "Hanyun Cui", "Yawen Duan", "Jiaxuan Guo", "Qi Guo", "Xuhao Hu", "Hong Huang", "Lige Huang", "Chunxiao Li", "Juncheng Li", "Qihao Lin", "Dongrui Liu", "Xinmin Liu", "Zicheng Liu", "Chaochao Lu", "Xiaoya Lu", "Jingjing Qu", "Qibing Ren", "Jing Shao", "Jingwei Shi", "Jingwei Sun", "Peng Wang", "Weibing Wang", "Jia Xu", "Lewen Yan", "Xiao Yu", "Yi Yu", "Boxuan Zhang", "Jie Zhang", "Weichen Zhang", "Zhijie Zheng", "Tianyi Zhou", "Bowen Zhou"], "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report", "comment": "97 pages, 37 figures", "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges."}
{"id": "2507.16635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16635", "abs": "https://arxiv.org/abs/2507.16635", "authors": ["Ali Mohamed Ali", "Luca Tirel", "Hashim A. Hashim"], "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems", "comment": null, "summary": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach."}
{"id": "2507.16670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16670", "abs": "https://arxiv.org/abs/2507.16670", "authors": ["Amandeep Kaur", "Gyan Prakash"], "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains", "comment": null, "summary": "Agricultural products are often subject to seasonal fluctuations in\nproduction and demand. Predicting and managing inventory levels in response to\nthese variations can be challenging, leading to either excess inventory or\nstockouts. Additionally, the coordination among stakeholders at various level\nof food supply chain is not considered in the existing body of literature. To\nbridge these research gaps, this study focuses on inventory management of\nagri-food products under demand and lead time uncertainties. By implementing\neffective inventory replenishment policy results in maximize the overall profit\nthroughout the supply chain. However, the complexity of the problem increases\ndue to these uncertainties and shelf-life of the product, that makes\nchallenging to implement traditional approaches to generate optimal set of\nsolutions. Thus, the current study propose a novel Deep Reinforcement Learning\n(DRL) algorithm that combines the benefits of both value- and policy-based DRL\napproaches for inventory optimization under uncertainties. The proposed\nalgorithm can incentivize collaboration among stakeholders by aligning their\ninterests and objectives through shared optimization goal of maximizing\nprofitability along the agri-food supply chain while considering perishability,\nand uncertainty simultaneously. By selecting optimal order quantities with\ncontinuous action space, the proposed algorithm effectively addresses the\ninventory optimization challenges. To rigorously evaluate this algorithm, the\nempirical data from fresh agricultural products supply chain inventory is\nconsidered. Experimental results corroborate the improved performance of the\nproposed inventory replenishment policy under stochastic demand patterns and\nlead time scenarios. The research findings hold managerial implications for\npolicymakers to manage the inventory of agricultural products more effectively\nunder uncertainty."}
{"id": "2507.16727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16727", "abs": "https://arxiv.org/abs/2507.16727", "authors": ["Zhenyun Yin", "Shujie Wang", "Xuhong Wang", "Xingjun Ma", "Yinchun Wang"], "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints", "comment": null, "summary": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated."}
{"id": "2507.16768", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16768", "abs": "https://arxiv.org/abs/2507.16768", "authors": ["Ran Wang", "Xiaoxuan Liu", "Hao Ren", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding", "comment": null, "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."}
{"id": "2507.16792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16792", "abs": "https://arxiv.org/abs/2507.16792", "authors": ["Roman Mayr", "Michel Schimpf", "Thomas Bohné"], "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation", "comment": null, "summary": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems."}
{"id": "2507.16796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16796", "abs": "https://arxiv.org/abs/2507.16796", "authors": ["Mian Ibad Ali Shah", "Enda Barrett", "Karl Mason"], "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning", "comment": "7 pages, 4 figures, 1 table, Proceedings of the Main Track of the\n  European Conference on Artificial Intelligence (ECAI 2025), October 25-30,\n  2025", "summary": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities."}
{"id": "2507.15859", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15859", "abs": "https://arxiv.org/abs/2507.15859", "authors": ["Harsha Sammangi", "Aditya Jagatha", "Giridhar Reddy Bojja", "Jun Liu"], "title": "Decentralized AI-driven IoT Architecture for Privacy-Preserving and Latency-Optimized Healthcare in Pandemic and Critical Care Scenarios", "comment": "10 Pages", "summary": "AI Innovations in the IoT for Real-Time Patient Monitoring On one hand, the\ncurrent traditional centralized healthcare architecture poses numerous issues,\nincluding data privacy, delay, and security. Here, we present an AI-enabled\ndecentralized IoT architecture that can address such challenges during a\npandemic and critical care settings. This work presents our architecture to\nenhance the effectiveness of the current available federated learning,\nblockchain, and edge computing approach, maximizing data privacy, minimizing\nlatency, and improving other general system metrics. Experimental results\ndemonstrate transaction latency, energy consumption, and data throughput orders\nof magnitude lower than competitive cloud solutions."}
{"id": "2507.16164", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; D.4.6"], "pdf": "https://arxiv.org/pdf/2507.16164", "abs": "https://arxiv.org/abs/2507.16164", "authors": ["Eldor Abdukhamidov", "Tamer Abuhmed", "Joanna C. S. Santos", "Mohammed Abuhamad"], "title": "Attacking interpretable NLP systems", "comment": null, "summary": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples."}
{"id": "2507.16203", "categories": ["cs.CR", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16203", "abs": "https://arxiv.org/abs/2507.16203", "authors": ["Rui Guo", "Avinash Ayalasomayajula", "Henian Li", "Jingbo Zhou", "Sujan Kumar Saha", "Farimah Farahmandi"], "title": "SVAgent: AI Agent for Hardware Security Verification Assertion", "comment": null, "summary": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment."}
{"id": "2507.16241", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16241", "abs": "https://arxiv.org/abs/2507.16241", "authors": ["Paul R. B. Houssel", "Siamak Layeghy", "Priyanka Singh", "Marius Portmann"], "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection Leveraging Large Language Models", "comment": null, "summary": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer."}
{"id": "2507.16329", "categories": ["cs.CR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16329", "abs": "https://arxiv.org/abs/2507.16329", "authors": ["Boheng Li", "Junjie Wang", "Yiming Li", "Zhiyang Hu", "Leyi Qi", "Jianshuo Dong", "Run Wang", "Han Qiu", "Zhan Qin", "Tianwei Zhang"], "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling", "comment": "Preprint version. Under review", "summary": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity."}
{"id": "2507.16372", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16372", "abs": "https://arxiv.org/abs/2507.16372", "authors": ["Tian Dong", "Yan Meng", "Shaofeng Li", "Guoxing Chen", "Zhen Liu", "Haojin Zhu"], "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion", "comment": "Accepted by USENIX Security 2025. Please cite this paper as \"Tian\n  Dong, Yan Meng, Shaofeng Li, Guoxing Chen, Zhen Liu, Haojin Zhu. Depth Gives\n  a False Sense of Privacy: LLM Internal States Inversion. In the 34th USENIX\n  Security Symposium (USENIX Security '25).\"", "summary": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design."}
