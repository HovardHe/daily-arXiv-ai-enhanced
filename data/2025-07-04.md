<div id=toc></div>

# Table of Contents

- [math.ST](#math.ST) [Total: 4]
- [math.OC](#math.OC) [Total: 13]
- [math.NT](#math.NT) [Total: 6]
- [math.LO](#math.LO) [Total: 2]
- [math.CO](#math.CO) [Total: 16]
- [q-fin.MF](#q-fin.MF) [Total: 2]
- [q-fin.GN](#q-fin.GN) [Total: 3]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.DM](#cs.DM) [Total: 1]


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1] [Fractional differential entropy and its application in modeling one-dimensional flow velocity](https://arxiv.org/abs/2507.02323)
*Poulami Paul,Chancal Kundu*

Main category: math.ST

TL;DR: 本文研究了Ubriaco提出的分数阶Shannon熵的连续类比——分数阶微分熵，分析了其独特性质，并通过解析和数值方法评估了其在几种常见连续分布中的应用。进一步，该熵被用于模拟宽开渠道湍流的垂直速度剖面，并通过实验数据验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究分数阶微分熵的独特性质及其在连续分布中的应用，探索其在可靠性分析和复杂系统统计研究中的潜在价值，并验证其在湍流速度剖面建模中的优越性。

Method: 通过解析和数值方法评估分数阶微分熵在常见连续分布中的表现；利用单参数空间分布函数优化速度分布估计；通过回归分析验证模型的有效性，并与现有熵模型进行对比。

Result: 分数阶微分熵展现出优于现有熵模型的独特性质；在湍流速度剖面建模中，该模型通过实验和现场数据验证了其有效性，并在对比研究中表现出优越性。

Conclusion: 分数阶微分熵在连续分布和湍流建模中具有显著优势，为可靠性分析和复杂系统研究提供了新的工具，其模型在实验验证中表现优异。

Abstract: The fractional order generalization of Shannon entropy proposed by Ubriaco
has been studied for discrete distributions. In the current paper, we conduct a
detailed study of the continuous analogue of this entropy termed as fractional
differential entropy and find some interesting properties which makes it stand
out among the existing entropies in literature. The studied entropy measure is
evaluated analytically and numerically for some well-known continuous
distributions, which will be quite useful in reliability analysis works and
other statistical studies of complex systems. Further, it has been used to
model the one-dimensional vertical velocity profile of turbulent flows in wide
open channels. A one-parametric spatial distribution function is utilized for
better estimation of the velocity distribution. The validity of the model has
been established using experimental and field data through regression analysis.
A comparative study is also presented to show the superiority of the proposed
model over the existing entropy-based models.

</details>


### [2] [Covariance scanning for adaptively optimal change point detection in high-dimensional linear models](https://arxiv.org/abs/2507.02552)
*Haeran Cho,Housen Li*

Main category: math.ST

TL;DR: 本文研究高维线性模型中单点变化的检测与估计问题，揭示了协方差加权差分参数的稀疏性如何影响检测边界与估计速率，并提出两种基于协方差扫描的最优方法McScan和QcScan。


<details>
  <summary>Details</summary>
Motivation: 探索高维线性模型变化点的检测与估计，揭示协方差结构与回归系数变化对检测能力的微妙影响，填补现有方法在稠密区域无法一致估计的空白。

Method: 提出基于协方差扫描的McScan（稀疏区域）和QcScan（稠密区域）方法，其计算复杂度与维度和样本量呈线性关系，并设计出自适应最优的联合流程。

Result: 理论证明所提方法在稀疏/稠密区域均达到极小极大最优（对数因子内），其中QcScan是首个在稠密区域实现一致性的方法，仿真验证了计算与统计效率。

Conclusion: 协方差扫描方法突破了传统Lasso类估计器的计算瓶颈，通过'固有稀疏性'框架统一了变化点检测理论，并为后续差分参数估计提供优化方案。

Abstract: This paper investigates the detection and estimation of a single change in
high-dimensional linear models. We derive minimax lower bounds for the
detection boundary and the estimation rate, which uncover a phase transition
governed the sparsity of the covariance-weighted differential parameter. This
form of "inherent sparsity" captures a delicate interplay between the
covariance structure of the regressors and the change in regression
coefficients on the detectability of a change point. Complementing the lower
bounds, we introduce two covariance scanning-based methods, McScan and QcSan,
which achieve minimax optimal performance (up to possible logarithmic factors)
in the sparse and the dense regimes, respectively. In particular, QcScan is the
first method shown to achieve consistency in the dense regime and further, we
devise a combined procedure which is adaptively minimax optimal across sparse
and dense regimes without the knowledge of the sparsity. Computationally,
covariance scanning-based methods avoid costly computation of Lasso-type
estimators and attain worst-case computation complexity that is linear in the
dimension and sample size. Additionally, we consider the post-detection
estimation of the differential parameter and the refinement of the change point
estimator. Simulation studies support the theoretical findings and demonstrate
the computational and statistical efficiency of the proposed covariance
scanning methods.

</details>


### [3] [Two-Sample Covariance Inference in High-Dimensional Elliptical Models](https://arxiv.org/abs/2507.02640)
*Nina Dörnemann*

Main category: math.ST

TL;DR: 本文提出了一种针对广义椭圆模型下大维协方差矩阵的双样本检验方法，基于U统计量估计两个总体协方差矩阵差异的Frobenius范数平方，并建立了椭圆数据下的新中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在椭圆数据下缺乏理论支持，本文旨在填补这一空白，提出首个具有理论保证的检验方法，适用于高维数据且无需稀疏性或维度-样本量比的显式增长条件。

Method: 采用基于U统计量的检验统计量，估计协方差矩阵差异的Frobenius范数平方，通过建立椭圆数据下的新中心极限定理实现渐近水平控制和功效分析。

Result: 理论证明该检验在椭圆数据下具有渐近有效性，模拟和实际数据应用验证了方法的性能，成为首个支持椭圆数据理论保证的检验方法。

Conclusion: 该方法在宽松假设下为高维协方差矩阵比较提供了理论可靠的工具，突破了传统方法对数据模型和维度增长的限制，具有广泛的适用性。

Abstract: We propose a two-sample test for large-dimensional covariance matrices in
generalized elliptical models. The test statistic is based on a U-statistic
estimator of the squared Frobenius norm of the difference between the two
population covariance matrices. This statistic was originally introduced by Li
and Chen (2012, AoS) for the independent component model. As a key theoretical
contribution, we establish a new central limit theorem for the U-statistics
under elliptical data, valid under both the null and alternative hypotheses.
This result enables asymptotic control of the test level and facilitates a
power analysis. To the best of our knowledge, the proposed test is the first
such method to be supported by theoretical guarantees for elliptical data. Our
approach imposes only mild assumptions on the covariance matrices and does
neither require sparsity nor explicit growth conditions on the
dimension-to-sample-size ratio. We illustrate our theoretical findings through
applications to both synthetic and real-world data.

</details>


### [4] [Fractional order entropy-based decision-making models under risk](https://arxiv.org/abs/2507.02683)
*Poulami Paul,Chanchal Kundu*

Main category: math.ST

TL;DR: 本文提出了一种基于Ubriaco分数阶熵的股票选择决策框架，通过调整分数参数捕捉不同风险偏好，并引入两种新风险度量方法（EU FE和EU FEV），结合人工神经网络验证了模型在PSI指数数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 构建高效投资组合需平衡收益与风险，传统方法难以动态反映投资者风险偏好变化。本文旨在开发一种能灵活适应不同风险态度的股票选择模型。

Method: 采用分数阶熵理论构建决策框架，分数参数近1表示高风险容忍度，近0表示风险厌恶。提出EU FE和EU FEV两种新风险度量指标，并利用人工神经网络绘制投资组合有效前沿。

Result: 模型在大型股、中型股、多元化组合及假设组合四种真实场景中，成功验证了分数阶熵对风险偏好的敏感性。PSI指数数据测试显示新风险度量方法能有效匹配投资者需求。

Conclusion: 分数阶熵框架为动态风险偏好下的投资决策提供了新工具，EU FE和EU FEV指标的引入增强了模型实用性，神经网络辅助的有效前沿分析证实了方法的可行性。

Abstract: The construction of an efficient portfolio with a good level of return and
minimal risk depends on selecting the optimal combination of stocks. This paper
introduces a novel decision-making framework for stock selection based on
fractional order entropy due to Ubriaco. By tuning the fractional parameter,
the model captures varying attitudes of individuals toward risk. Values of
fractional parameter near one indicate high risk tolerance (adventurous
attitude), while those near zero reflect risk aversion (conservative attitude).
The sensitivity of the fractional order entropy to changing risk preferences of
decision makers is demonstrated through four real world portfolio models,
namely, large cap, mid cap, diversified, and hypothetical. Furthermore, two new
risk measures, termed as expected utility fractional entropy (EU FE) and
expected utility fractional entropy and variance (EU FEV), are introduced to
develop decision models aligned with investors risk preferences. The
effectiveness of the decision model is further tested with financial stock
market data of PSI index by finding efficient frontiers of portfolio with the
aid of artificial neural network.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [5] [A novel exact approach to polynomial optimization](https://arxiv.org/abs/2507.02120)
*Dimitris Bertsimas,Dick den Hertog,Thodoris Koukouvinos*

Main category: math.OC

TL;DR: 本文提出了一种新的多项式优化方法，通过线性乘以凸函数（SLC）分解获得凸松弛，显著提升了求解效率和质量，优于现有方法如BARON和SOS。


<details>
  <summary>Details</summary>
Motivation: 现有多项式优化方法（如SOS）依赖大规模半定规划，问题规模受限且无法保证最优性，且无法处理其他凸非线性部分。

Method: 通过推导多项式的线性乘以凸函数（SLC）分解，获得原问题的凸松弛，并证明SLC分解对任意阶多项式均存在，且可得到最紧下界。

Result: 数值实验表明，该方法优于BARON和SOS等先进方法，能在1小时内求解40变量3阶或20变量4阶多项式优化问题至最优。

Conclusion: SLC分解方法显著提升了多项式优化的求解效率和质量，为大规模多项式优化问题提供了有效解决方案。

Abstract: Polynomial optimization problems represent a wide class of optimization
problems, with a large number of real-world applications. Current approaches
for polynomial optimization, such as the sum of squares (SOS) method, rely on
large-scale semidefinite programs, and therefore the scale of problems to which
they can be applied is limited and an optimality guarantee is not always
provided. Moreover, the problem can have other convex nonlinear parts, that
cannot be handled by these approaches. In this paper, we propose an alternative
approach for polynomial optimization. We obtain a convex relaxation of the
original polynomial optimization problem, by deriving a sum of linear times
convex (SLC) functions decomposition for the polynomial. We prove that such SLC
decompositions always exist for arbitrary degree polynomials. Moreover, we
derive the SLC decomposition that results in the tightest lower bound, thus
improving significantly the quality of the obtained bounds in each node of the
spatial Branch and Bound method. In the numerical experiments, we show that our
approach outperforms state-of-the-art methods for polynomial optimization, such
as BARON and SOS. We show that with our method, we can solve polynomial
optimization problems to optimality with 40 variables and degree 3, as well as
20 variables and degree 4, in less than an hour.

</details>


### [6] [Perturbed Gradient Descent Algorithms are Small-Disturbance Input-to-State Stable](https://arxiv.org/abs/2507.02131)
*Leilei Cui,Zhong-Ping Jiang,Eduardo D. Sontag,Richard D. Braatz*

Main category: math.OC

TL;DR: 本文研究了梯度下降算法在扰动下的鲁棒性，引入了小扰动输入-状态稳定性（ISS）概念及其Lyapunov特性，并证明了在广义非线性PL条件下梯度下降具有小扰动ISS特性。


<details>
  <summary>Details</summary>
Motivation: 探讨梯度下降算法在存在扰动时的稳定性，为非线性动态系统提供理论保障。

Method: 扩展线性Polyak-Lojasiewicz（PL）条件至非线性版本，并基于Lyapunov理论分析梯度下降的小扰动ISS特性。

Result: 证明了梯度下降在广义非线性PL条件下具有小扰动ISS特性，且LQR成本函数满足该条件，政策梯度算法（如自然政策梯度、高斯-牛顿法）同样具备此特性。

Conclusion: 研究为梯度下降算法在扰动环境下的收敛性提供了理论支持，特别适用于LQR等控制问题的政策梯度方法。

Abstract: This article investigates the robustness of gradient descent algorithms under
perturbations. The concept of small-disturbance input-to-state stability (ISS)
for discrete-time nonlinear dynamical systems is introduced, along with its
Lyapunov characterization. The conventional linear Polyak-Lojasiewicz (PL)
condition is then extended to a nonlinear version, and it is shown that the
gradient descent algorithm is small-disturbance ISS provided the objective
function satisfies the generalized nonlinear PL condition. This
small-disturbance ISS property guarantees that the gradient descent algorithm
converges to a small neighborhood of the optimum under sufficiently small
perturbations. As a direct application of the developed framework, we
demonstrate that the LQR cost satisfies the generalized nonlinear PL condition,
thereby establishing that the policy gradient algorithm for LQR is
small-disturbance ISS. Additionally, other popular policy gradient algorithms,
including natural policy gradient and Gauss-Newton method, are also proven to
be small-disturbance ISS.

</details>


### [7] [Operator-Theoretic Methods for Differential Games](https://arxiv.org/abs/2507.02203)
*Craig Bakker,Adam T. Rupe,Alexander Von Moll,Adam R. Gerlach*

Main category: math.OC

TL;DR: 本文提出两种基于Koopman算子（KO）的方法求解零和微分博弈问题，分别采用连续时间全局反馈与离散时间数据驱动策略，并在炮塔防御游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 微分博弈理论为连续时间多智能体交互建模提供了框架，但现有解法存在局限性。本文旨在利用KO算子开发新型求解方法，弥补传统方法的不足。

Method: 方法一：利用KO的解析解算子在全局域上计算连续时间反馈解；方法二：基于离散时间数据驱动KO表示，逐轨迹计算开环控制策略。

Result: 在文献中的炮塔防御游戏测试表明，两种方法均能复现解析解的行为，其中连续反馈法适用于全局优化，离散法则更适配实时控制场景。

Conclusion: 两种KO方法各具优势：连续法提供全局解但计算复杂，离散法灵活但限于局部优化。未来可探索混合策略及更高维问题的扩展应用。

Abstract: Differential game theory offers an approach for modeling interactions between
two or more agents that occur in continuous time. The goal of each agent is to
optimize its objective cost functional. In this paper, we present two different
methods, based on the Koopman Operator (KO), to solve a zero-sum differential
game. The first approach uses the resolvent of the KO to calculate a
continuous-time global feedback solution over the entire domain. The second
approach uses a discrete-time, data-driven KO representation with control to
calculate open-loop control policies one trajectory at a time. We demonstrate
these methods on a turret defense game from the literature, and we find that
the methods' solutions replicate the behavior of the analytical solution
provided in the literature.. Following that demonstration, we highlight the
relative advantages and disadvantages of each method and discuss potential
future work for this line of research.

</details>


### [8] [Fractional Operators for Nonlinear Electrical Circuits](https://arxiv.org/abs/2507.02220)
*Ioannis Dassios*

Main category: math.OC

TL;DR: 本文介绍了两种新的基于正弦($\sin$)和余弦($\cos$)核的分数阶算子，旨在改进对具有记忆效应的非线性元件（如忆阻器）的分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于正弦和余弦在交流电路信号建模中的基础作用，特别是针对具有记忆效应的非线性系统。

Method: 通过设计两种新的分数阶算子，将某些非线性方程转化为更简单的线性形式，从而优化分析过程。

Result: 提出的算子能够有效简化非线性元件（如忆阻器）在记忆系统中的建模与分析。

Conclusion: 新引入的基于三角核的分数阶算子为非线性电路系统的分析提供了更高效的工具。

Abstract: This article introduces two new fractional operators with sine ($\sin$) and
cosine ($\cos$) kernels, motivated by their fundamental role in modeling AC
signals in electrical circuits. The operators are designed to improve the
analysis of nonlinear components such as the memristor by transforming certain
nonlinear equations into simpler linear forms, particularly in systems with
memory effects.

</details>


### [9] [A learning model predictive control for virtual coupling in railroads](https://arxiv.org/abs/2507.02383)
*Miguel A. Vaquero-Serrano,Francesco Borrelli,Jesus Felez*

Main category: math.OC

TL;DR: 本文提出了一种基于学习模型预测控制（LMPC）的新型智能列车控制系统，用于铁路虚拟耦合。LMPC通过整合历史数据优化控制策略，在模拟测试中显著降低了能耗，尤其在频繁变速的地铁系统中效果更佳。


<details>
  <summary>Details</summary>
Motivation: 虚拟耦合技术能缩短列车间距以提升线路运力，但需要先进的控制方法。传统MPC缺乏学习能力，而LMPC能通过数据学习未建模动态，在满足约束的同时提升性能。

Method: 开发了LMPC控制器，并与无学习能力的常规MPC进行能耗对比。模拟分为两种应用场景：仅后车（跟随者）使用LMPC，以及前后车（领导者与跟随者）均使用LMPC，并在地铁、区域铁路和高速铁路三类线路上独立测试。

Result: LMPC在所有模拟案例中均降低能耗，同时基本保持速度与行驶时间。能耗改善在变速频繁的地铁系统中尤为显著，高速铁路则相对有限。

Conclusion: LMPC在虚拟耦合场景下具有显著节能潜力，未来研究将探索真实数据替代模拟数据的影响。

Abstract: The objective of this paper is to present a novel intelligent train control
system for virtual coupling in railroads based on a Learning Model Predictive
Control (LMPC). Virtual coupling is an emerging railroad technology that
reduces the distance between trains to increase the capacity of the line,
whereas LMPC is an optimization-based controller that incorporates artificial
intelligence methods to improve its control policies. By incorporating data
from past experiences into the optimization problem, LMPC can learn unmodeled
dynamics and enhance system performance while satisfying constraints. The LMPC
developed in this paper is simulated and compared, in terms of energy
consumption, with a general MPC, without learning capabilities. The simulations
are divided into two main practical applications: a LMPC applied only to the
rear trains (followers) and a LMPC applied to both the followers and the first
front train of the convoy (leader). Within each application, the LMPC is
independently tested for three railroad categories: metro, regional, and
high-speed. The results show that the LMPC reduces energy consumption in all
simulation cases while approximately maintaining speed and travel time. The
effect is more pronounced in rail applications with frequent speed variations,
such as metro systems, compared to high-speed rail. Future research will
investigate the impact of using real-world data in place of simulated data.

</details>


### [10] [Split-Merge Revisited: A Scalable Approach to Generalized Eigenvalue Problems](https://arxiv.org/abs/2507.02389)
*Xiaozhi Liu,Yong Xia*

Main category: math.OC

TL;DR: 本文提出了一种基于差分公式的广义特征值问题（GEP）新方法，通过最小化结构化二次多项式目标，应用高效的一阶优化方法，并引入加速预条件镜像下降算法和Split-Merge算法扩展，显著提升了计算效率和数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统基于Rayleigh商的广义特征值问题求解方法在大规模场景下存在数值不稳定和计算效率低的问题，需要探索更高效稳定的替代方案。

Method: 采用差分公式重构GEP为结构化二次多项式最小化问题，提出无需线搜索的全局收敛一阶优化方法，并引入变换域视角开发加速预条件镜像下降算法，扩展Split-Merge算法融入二阶信息。

Result: 在合成和真实数据集上的实验表明，新方法在计算效率和数值稳定性方面显著优于现有基线方法。

Conclusion: 通过差分公式重构和新型优化算法设计，本研究为大规模GEP问题提供了更高效稳定的解决方案，其变换域视角为经典算法性能分析提供了新见解。

Abstract: The generalized eigenvalue problem (GEP) serves as a cornerstone in a wide
range of applications in numerical linear algebra and scientific computing.
However, traditional approaches that aim to maximize the classical Rayleigh
quotient often suffer from numerical instability and limited computational
efficiency, especially in large-scale settings. In this work, we explore an
alternative difference-based formulation of GEP by minimizing a structured
quadratic polynomial objective, which enables the application of efficient
first-order optimization methods. We establish global convergence guarantees
for these methods without requiring line search, and further introduce a
transform-domain perspective that reveals the intrinsic connection and
performance gap between classical first-order algorithms and the power method.
Based on this insight, we develop an accelerated preconditioned mirror descent
algorithm, which allows for flexible preconditioner design and improved
convergence behavior. Lastly, we extend the recently proposed Split-Merge
algorithm to the general GEP setting, incorporating richer second-order
information to further accelerate convergence. Empirical results on both
synthetic and real-world datasets demonstrate that our proposed methods achieve
significant improvements over existing baselines in terms of both computational
efficiency and numerical stability.

</details>


### [11] [Stability characterization of impulsive linear switched systems](https://arxiv.org/abs/2507.02434)
*Yacine Chitour,Jamal Daafouz,Ihab Haidar,Paolo Mason,Mario Sigalotti*

Main category: math.OC

TL;DR: 本文研究了一类由连续时间和离散时间切换线性动力学混合定义的脉冲系统，通过分析加权离散时间切换系统，建立了稳定性结果，并扩展了混合与切换系统理论中的现有结论。


<details>
  <summary>Details</summary>
Motivation: 研究混合连续与离散时间切换线性动力学定义的脉冲系统，旨在填补现有混合与切换系统理论中的空白，并提供更全面的稳定性分析框架。

Method: 首先分析了一类相关的加权离散时间切换系统，建立了两个主要稳定性结果：逆Lyapunov定理和Berger-Wang型公式，随后通过谱方法和Lyapunov方法表征系统的指数稳定性。

Result: 成功建立了加权离散时间切换系统的稳定性理论，扩展了混合与切换系统理论中的现有结果，为脉冲系统的稳定性分析提供了新的工具。

Conclusion: 通过谱和Lyapunov方法，本文为混合连续与离散时间切换线性动力学定义的脉冲系统提供了稳定性分析的理论基础，扩展了相关领域的研究成果。

Abstract: This paper addresses a class of impulsive systems defined by a mix of
continuous-time and discrete-time switched linear dynamics. We first analyze a
related class of weighted discrete time switched systems for which we establish
two main stability results: a converse Lyapunov theorem and a Berger-Wang-type
formula. These results are used to characterize the exponential stability of
the considered class of systems via spectral and Lyapunov-based approaches,
extending existing results in hybrid and switched systems theory.

</details>


### [12] [Stability criteria for hybrid linear systems with singular perturbations](https://arxiv.org/abs/2507.02446)
*Ihab Haidar,Yacine Chitour,Jamal Daafouz,Paolo Mason,Mario Sigalotti*

Main category: math.OC

TL;DR: 研究一类具有慢快动力学切换特性的奇异摄动脉冲线性切换系统，通过构造单时间尺度的辅助切换系统分析其稳定性。


<details>
  <summary>Details</summary>
Motivation: 探讨奇异摄动参数较小时，慢快动力学切换系统的稳定性问题，为复杂切换系统提供分析框架。

Method: 构建单时间尺度演化的辅助切换系统，通过分析其稳定性间接判定原系统的稳定性。

Result: 辅助系统的稳定性直接决定了原系统在奇异摄动参数较小时的稳定性行为。

Conclusion: 该方法为奇异摄动脉冲切换系统的稳定性分析提供了有效工具，揭示了慢快动力学切换的内在机制。

Abstract: We study a class of singularly perturbed impulsive linear switched systems
exhibiting switching between slow and fast dynamics. To analyze their behavior,
we construct auxiliary switched systems evolving in a single time scale. The
stability or instability of these auxiliary systems directly determines that of
the original system in the regime of small singular perturbation parameters.

</details>


### [13] [Maximal entropy in the moment body](https://arxiv.org/abs/2507.02461)
*Didier Henrion*

Main category: math.OC

TL;DR: 本文提出了一种基于量子信息理论的更高效方法，通过全局最小化光滑严格凸对数配分函数来解决矩体成员资格问题，计算复杂度与矩阵大小的三次方相关，实验表明可在普通笔记本电脑上快速处理大规模半定规划问题。


<details>
  <summary>Details</summary>
Motivation: 矩体成员资格问题在量子态估计和多项式优化中有广泛应用，传统半定规划方法效率较低，需要更高效的解决方案。

Method: 受量子信息理论启发，提出全局最小化光滑严格凸对数配分函数（与最大熵问题对偶），并分析其曲率特性，设计了几何预处理算法。

Result: 复杂度分析显示计算量与矩阵大小的三次方相关（类似特征结构计算），数值实验证明在普通笔记本上可数秒处理1000×1000密集矩阵的密集投影。

Conclusion: 该方法将大规模半定规划的主要瓶颈转移至梯度存储与操作，显著提升了矩体成员资格问题的求解效率。

Abstract: A moment body is a linear projection of the spectraplex, the convex set of
trace-one positive semidefinite matrices. Determining whether a given point
lies within a given moment body is a problem with numerous applications in
quantum state estimation or polynomial optimization. This moment body
membership oracle can be addressed with semidefinite programming, for which
several off-the-shelf interior-point solvers are available. In this paper,
inspired by techniques from quantum information theory, we argue analytically
and geometrically that a much more efficient approach consists of minimizing
globally a smooth strictly convex log-partition function, dual to a maximum
entropy problem. We analyze the curvature properties of this function and we
describe a neat geometric pre-conditioning algorithm. A detailed complexity
analysis reveals a cubic dependence on the matrix size, similar to a few
eigenstructure computations. Basic numerical experiments illustrate that dense
(i.e. non-sparse) projections of size 1000 of a dense semidefinite matrix of
size 1000-by-1000 can be routinely handled in a few seconds on a standard
laptop, thereby moving the main bottleneck in large-scale semidefinite
programming almost entirely to efficient gradient storage and manipulation.

</details>


### [14] [HPR-QP: A dual Halpern Peaceman-Rachford method for solving large-scale convex composite quadratic programming](https://arxiv.org/abs/2507.02470)
*Kaihuang Chen,Defeng Sun,Yancheng Yuan,Guojun Zhang,Xinyuan Zhao*

Main category: math.OC

TL;DR: 本文提出HPR-QP方法，一种基于限制性Wolfe对偶的Halpern Peaceman-Rachford双方法，用于大规模凸复合二次规划求解，通过对称Gauss-Seidel技术和自适应策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 针对大规模凸复合二次规划问题，现有方法因引入辅助松弛变量导致收敛慢，需开发高效对偶空间求解器。

Method: 基于限制性Wolfe对偶，采用对称Gauss-Seidel技术简化子问题更新；利用Hessian矩阵范围空间约束降低近端算子谱范数；设计影子序列处理范围空间约束，并集成自适应重启与惩罚参数更新策略。

Result: GPU上的Julia实现显示，HPR-QP在基准数据集上速度和可扩展性均显著优于现有最优求解器，Karush-Kuhn-Tucker残差收敛速度为$O(1/k)$。

Conclusion: HPR-QP通过创新对偶空间建模与计算优化，为大规模凸复合二次规划提供了高效解决方案，其方法框架具有广泛适用性。

Abstract: In this paper, we introduce HPR-QP, a dual Halpern Peaceman-Rachford (HPR)
method designed for solving large-scale convex composite quadratic programming.
One distinctive feature of HPR-QP is that, instead of working with the primal
formulations, it builds on the novel restricted Wolfe dual introduced in recent
years. It also leverages the symmetric Gauss-Seidel technique to simplify
subproblem updates without introducing auxiliary slack variables that typically
lead to slow convergence. By restricting updates to the range space of the
Hessian of the quadratic objective function, HPR-QP employs proximal operators
of smaller spectral norms to speed up the convergence. Shadow sequences are
elaborately constructed to deal with the range space constraints. Additionally,
HPR-QP incorporates adaptive restart and penalty parameter update strategies,
derived from the HPR method's $O(1/k)$ convergence in terms of the
Karush-Kuhn-Tucker residual, to further enhance its performance and robustness.
Extensive numerical experiments on benchmark data sets using a GPU demonstrate
that our Julia implementation of HPR-QP significantly outperforms
state-of-the-art solvers in both speed and scalability.

</details>


### [15] [Online Convex Optimization for Coordinated Long-Term and Short-Term Isolated Microgrid Dispatch](https://arxiv.org/abs/2507.02636)
*Ning Qi,Yousuf Baker,Bolun Xu*

Main category: math.OC

TL;DR: 本文提出了一种新型非预见性长短期协调调度框架，用于混合长短时储能（LDES）的孤立微电网，通过凸包近似模型和在线凸优化算法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 为解决孤立微电网中混合储能系统的非凸电化学动态和长期合约的时序耦合问题，需开发高效且自适应的调度方法。

Method: 采用凸包近似模型处理LDES非凸动态，离线生成最优SoC轨迹；在线阶段结合核回归动态更新SoC参考值，并提出带参考跟踪的自适应在线凸优化（OCO）算法。

Result: 仿真显示该方法成本降低73.4%，通过参考跟踪消除负荷损失，OCO算法进一步节省2.4%成本，且对预测误差和系统故障具有鲁棒性。

Conclusion: 所提框架在计算可行性和性能上优于现有方法，其优势随LDES时长增加而放大，为微电网调度提供了理论保障和实践价值。

Abstract: This paper proposes a novel non-anticipatory long-short-term coordinated
dispatch framework for isolated microgrid with hybrid short-long-duration
energy storages (LDES). We introduce a convex hull approximation model for
nonconvex LDES electrochemical dynamics, facilitating computational
tractability and accuracy. To address temporal coupling in SoC dynamics and
long-term contracts, we generate hindsight-optimal state-of-charge (SoC)
trajectories of LDES and netloads for offline training. In the online stage, we
employ kernel regression to dynamically update the SoC reference and propose an
adaptive online convex optimization (OCO) algorithm with SoC reference tracking
and expert tracking to mitigate myopia and enable adaptive step-size
optimization. We rigorously prove that both long-term and short-term policies
achieve sublinear regret bounds over time, which improves with more regression
scenarios, stronger tracking penalties, and finer convex approximations.
Simulation results show that the proposed method outperforms state-of-the-art
methods, reducing costs by 73.4%, eliminating load loss via reference tracking,
and achieving an additional 2.4% cost saving via the OCO algorithm. These
benefits scale up with longer LDES durations, and the method demonstrates
resilience to poor forecasts and unexpected system faults.

</details>


### [16] [A Hybrid Perspective on Suboptimal Mixed-Integer Quadratic Programming](https://arxiv.org/abs/2507.02772)
*Luke Fina,Christopher Petersen*

Main category: math.OC

TL;DR: 该研究提出了一种递归求解次优混合整数二次规划的方法，用于动态系统的反馈控制，通过混合系统理论和参数化MIQP保证稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决动态系统反馈控制中的次优混合整数二次规划问题，并确保控制器的稳定性。

Method: 结合参数化混合整数二次规划（MIQP）和混合系统理论，构建递归MIQP反馈控制器，并通过求解器参数将次优部分编码为有界扰动。

Result: 理论分析证明了混合框架的稳定性，并通过多组求解器参数的MIQP模型预测控制仿真验证了其有效性。

Conclusion: 所提出的混合框架能够有效处理次优MIQP问题，并在理论上保证了反馈控制系统的稳定性。

Abstract: This work solves suboptimal mixed-integer quadratic programs recursively for
feedback control of dynamical systems. The proposed framework leverages
parametric mixed-integer quadratic programming (MIQP) and hybrid systems theory
to model a recursive MIQP feedback controller and a feedback controlled
dynamical system. The proposed hybrid framework theoretically encodes the
suboptimal part via solver parameters as bounded perturbations from the optimal
solution set. The stability of the proposed hybrid framework is theoretically
guaranteed and validated through MIQP model predictive control simulations with
multiple solver parameters.

</details>


### [17] [Stiefel optimization is NP-hard](https://arxiv.org/abs/2507.02839)
*Zehua Lai,Lek-Heng Lim,Tianyun Tang*

Main category: math.OC

TL;DR: 本文证明了在Stiefel或Grassmann流形上的线性约束线性优化问题以及无约束二次优化问题通常是NP难的，且不存在FPTAS。结果还推广到旗流形，表明流形优化即使对最简单的问题也很困难。


<details>
  <summary>Details</summary>
Motivation: 研究流形优化问题的计算复杂性，特别是Stiefel和Grassmann流形上的线性及二次优化问题，以填补现有理论空白。

Method: 通过理论证明和复杂性分析，展示了这些优化问题的NP难性，并扩展了结果到旗流形。

Result: 证明了Stiefel流形上的线性约束线性优化和无约束二次优化均为NP难问题，且不存在FPTAS。类似结论适用于Grassmann和旗流形。

Conclusion: 流形优化即使在最简单情况下（如LP和无约束QP）也具有很高的计算复杂性，这对相关领域的算法设计提出了挑战。

Abstract: We show that linearly constrained linear optimization over a Stiefel or
Grassmann manifold is NP-hard in general. We show that the same is true for
unconstrained quadratic optimization over a Stiefel manifold. We will establish
the nonexistence of FPTAS for these optimization problems over a Stiefel
manifold. As an aside we extend our results to flag manifolds. Combined with
earlier findings, this shows that manifold optimization is a difficult endeavor
-- even the simplest problems like LP and unconstrained QP are already NP-hard
on the most common manifolds.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [18] [Families of p-adic fields](https://arxiv.org/abs/2507.02360)
*Jordi Guàrdia Rúbies,John W. Jones,Kevin Keating,Sebastian Pauli,David P. Roberts,David Roe*

Main category: math.NT

TL;DR: 通过使用Krasner-Monge多项式和相对与绝对方法，改进了LMFDB中$p$-adic域的数据库，将其组织成家族形式，提高了管理效率并揭示了理论结构。


<details>
  <summary>Details</summary>
Motivation: 改进LMFDB中$p$-adic域的数据库，使其更易于管理并展示理论结构。

Method: 系统应用Krasner-Monge多项式，同时采用相对和绝对方法，将$p$-adic域组织成家族。

Result: 数据库现在包含所有次数$n \leq 23$且$p<200$的$\mathbb{Q}_p$的$n$次扩张。

Conclusion: 改进后的数据库不仅更易于管理，还使得理论结构更加明显，为相关研究提供了更好的支持。

Abstract: We improve the database of $p$-adic fields in the LMFDB by systematically
using Krasner-Monge polynomials and working relatively as well as absolutely.
These improvements organize $p$-adic fields into families. They thereby make
long lists of fields more manageable and various theoretical structures more
evident. In particular, the database now includes all degree $n$ extensions of
$\mathbb{Q}_p$, for $p<200$ and $n \leq 23$.

</details>


### [19] [Cusp forms of weight 1/2 and pairs of quadratic forms](https://arxiv.org/abs/2507.02522)
*András Biró*

Main category: math.NT

TL;DR: 本文证明了Kohnen子空间中半整数权尖形式四个傅里叶系数乘积的谱求和公式，并揭示了其与二次型对广义类数的关联。


<details>
  <summary>Details</summary>
Motivation: 研究半整数权尖形式傅里叶系数的乘积关系，旨在建立与二次型类数的深刻联系，扩展数论中的谱求和理论。

Method: 通过分析Kohnen子空间的特殊结构，构建四个傅里叶系数的乘积公式，并将其转化为二次型对的广义类数表达式。

Result: 成功推导出谱求和公式，证明其等价于具有整数系数的二次型对特定广义类数的组合。

Conclusion: 该公式为半整数权模形式与二次型理论搭建了新桥梁，为后续研究提供了有力的分析工具。

Abstract: We prove a spectral summation formula for the product of four Fourier
coefficients of half-integral weight cusp forms in Kohnen's subspace. The other
side of the formula involves certain generalized class numbers of pairs of
quadratic forms with integer coefficients.

</details>


### [20] [Monodromy groups and exceptional Hodge classes, II: Sato-Tate groups](https://arxiv.org/abs/2507.02535)
*Andrea Gallese,Heidi Goodson,Davide Lombardo*

Main category: math.NT

TL;DR: 本文计算了由方程$y^2=x^m+1$定义的超椭圆曲线的雅可比簇$J_m$的Sato-Tate群，推广了非退化簇的结果，并扩展了Gross-Koblitz公式。


<details>
  <summary>Details</summary>
Motivation: 目前尚无通用算法计算Sato-Tate群，本文旨在填补这一空白，并为超椭圆曲线的雅可比簇提供具体计算。

Method: 通过研究超椭圆曲线$y^2=x^m+1$的雅可比簇$J_m$，推广了非退化簇的Sato-Tate群描述，并利用$p$-adic分析扩展了Gross-Koblitz公式。

Result: 成功计算了$J_m$的Sato-Tate群，并证明了Gross-Koblitz公式在有理参数下的扩展形式。

Conclusion: 本文不仅为超椭圆曲线的雅可比簇提供了Sato-Tate群的具体计算方法，还推广了相关理论结果，为后续研究提供了工具和理论基础。

Abstract: Denote by $J_m$ the Jacobian variety of the hyperelliptic curve defined by
the affine equation $y^2=x^m+1$ over $\mathbb{Q}$, where $m \geq 3$ is a fixed
positive integer. In this paper, we compute the Sato-Tate group of $J_m$.
Currently, there is no general algorithm that computes this invariant. We also
describe the Sato-Tate group of an abelian variety, generalizing existing
results that apply only to non-degenerate varieties, and prove an extension of
a well-known formula of Gross-Koblitz that relates values of the classical and
$p$-adic gamma functions at rational arguments.

</details>


### [21] [The $μ-$invariant of fine Selmer groups associated to general Drinfeld modules](https://arxiv.org/abs/2507.02688)
*Hang Chen*

Main category: math.NT

TL;DR: 本文研究了全局函数域上Drinfeld模的精细Selmer群在常数$\mathbb{Z}_{\textit{p}}-$扩张上的性质，证明了其对偶是有限生成的Iwasawa模且$\mu-$不变量为零。


<details>
  <summary>Details</summary>
Motivation: 研究Drinfeld模的精细Selmer群在Iwasawa理论中的行为，推广了arXiv:2311.06499中的结果，为函数域上的非交换Iwasawa理论提供新见解。

Method: 在全局函数域$F$的常数$\mathbb{Z}_{\textit{p}}-$扩张$\mathfrak{F}$上，分析Drinfeld模$\phi$的$\mathfrak{p}-$主挠对应的精细Selmer群，运用Iwasawa模理论进行研究。

Result: 证明了精细Selmer群的Pontryagin对偶是有限生成的Iwasawa模，且其$\mu-$不变量为零，这一结果适用于任意Drinfeld模。

Conclusion: 该研究不仅推广了已有成果，还为函数域上Drinfeld模的Iwasawa理论建立了新的框架，揭示了$\mu-$不变量在常数扩张中的普遍消失现象。

Abstract: Let $F$ be a global function field over the finite field $\mathbb{F}_q$ where
$q$ is a prime power and $A$ be the ring of elements in $F$ regular outside
$\infty$. Let $\phi$ be an arbitrary Drinfeld module over $F$ For a fixed
non-zero prime ideal $\mathfrak{p}$ of $A$, we show that on the constant
$\mathbb{Z}_{\textit{p}}-$extension $\mathfrak{F}$ of $F$, the Pontryagin dual
of the fine Selmer group associated to the $\mathfrak{p}-$primary torsion of
$\phi$ over $\mathfrak{F}$ is a finitely generated Iwasawa module such that its
Iwasawa $\mu-$invariant vanishes. This provides a generalization of the results
given in arXiv:2311.06499.

</details>


### [22] [On Some New Congruences For Biregular Overpartitions](https://arxiv.org/abs/2507.02720)
*Anakha V*

Main category: math.NT

TL;DR: 研究了不能被$l_1$和$l_2$整除的过分割数$\bar{B}_{l_1,l_2}(n)$的算术性质，建立了模k∈{4,8,6,12}的同余关系，其中$l_1$和$l_2$为2和3的任意幂次。


<details>
  <summary>Details</summary>
Motivation: 受Nadji等人近期工作的启发，探索过分割数$\bar{B}_{l_1,l_2}(n)$的算术性质，特别是当$l_1$和$l_2$为2和3的幂次时的同余关系。

Method: 使用生成函数、剖分公式和theta函数等工具，推广了文献[26]和[15]中关于$l_1$和$l_2$为2和3随机幂次的结果。

Result: 证明了$\bar{B}_{l_1,l_2}(n)$在模k∈{4,8,6,12}下的若干同余关系，其中$l_1$和$l_2$为2和3的任意幂次。

Conclusion: 通过系统方法扩展了过分割数的同余理论，为特定除数条件下的分割数性质提供了新的理论支持。

Abstract: Inspired by the recent work by Nadji, Ahmia and Ram\'irez, we examined the
arithmetic properties of $\bar{B}_{l_1,l_2} (n)$, the number of overpartitions
of n whose parts are neither divisible by $l_1$ nor divisible by $l_2$. In
particular, we establish some congruences modulo k in {4, 8, 6, 12} satisfied
by $\bar{B}_{l_1,l_2} (n)$ where $l_1$ and $l_2$ take values as arbitrary
powers of 2 and 3. Moreover, we extend certain results proved in [26] and [15]
for $l_1$ and $l_2$ with random powers of 2 and 3. Generating functions,
dissection formulas, and theta functions are used to prove our main findings.

</details>


### [23] [Étude statistique du facteur premier médian, 4: somme des inverses](https://arxiv.org/abs/2507.02739)
*Jonathan Rotgé*

Main category: math.NT

TL;DR: 本文研究了整数中质因数的倒数和的渐进行为，改进了现有文献中的估计结果。


<details>
  <summary>Details</summary>
Motivation: 研究整数中质因数倒数和的渐进行为，以深化对整数分解性质的理解。

Method: 分别考虑了按重数定义和不按重数定义的中间质因数倒数和，采用了渐近展开和隐式参数的方法。

Result: 在第一种情况下获得了渐近展开，第二种情况下得到了包含隐式参数的渐近公式，均优于文献中的现有估计。

Conclusion: 本研究改进了对整数质因数倒数和行为的理解，提供了更精确的渐近结果。

Abstract: We consider the sum of the reciprocals of the middle prime factor of an
integer, defined according to multiplicity or not. We obtain an asymptotic
expansion in the first case and an asymptotic formula involving an implicit
parameter in the second. Both these results improve on previous estimates
available in the literature.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [24] [Computability of a Whitney Extension](https://arxiv.org/abs/2507.02113)
*Andrea Brun,Guido Gherardi,Alberto Marcone*

Main category: math.LO

TL;DR: 证明了在输入数据适当表示时，Whitney扩展问题的可计算性。


<details>
  <summary>Details</summary>
Motivation: 研究Whitney扩展问题在可计算性理论中的表现，探讨当输入数据（闭集和Whitney射流）可计算时，能否构造出相应的光滑扩展函数。

Method: 假设闭集$F \subseteq \mathbb{R}^n$的距离函数$x \mapsto d(x,F)$可计算，且给定$F$上的$m$阶Whitney射流$(f^{(\bar{k})})_{|\bar{k}| \le m}$，通过构造性方法证明存在$C^{m}(\mathbb{R}^n)$函数$g$满足扩展条件。

Result: 成功构造出$g \in C^{m}(\mathbb{R}^n)$，使得$g$及其偏导数在$F$上与Whitney射流$(f^{(\bar{k})})_{|\bar{k}| \le m}$的相应函数一致。

Conclusion: 该研究为Whitney扩展问题提供了可计算性证明，扩展了可计算分析理论在微分几何中的应用范围。

Abstract: We prove the computability of a version of Whitney Extension, when the input
is suitably represented. More specifically, if $F \subseteq \mathbb{R}^n$ is a
closed set represented so that the distance function $x \mapsto d(x,F)$ can be
computed, and $(f^{(\bar{k})})_{|\bar{k}| \le m}$ is a Whitney jet of order $m$
on $F$, then we can compute $g \in C^{m}(\mathbb{R}^n)$ such that $g$ and its
partial derivatives coincide on $F$ with the corresponding functions of
$(f^{(\bar{k})})_{|\bar{k}| \le m}$.

</details>


### [25] [Abstract Model Structures and Compactness Theorems](https://arxiv.org/abs/2507.02343)
*Sayantan Roy,Sankha S. Basu,Mihir K. Chakraborty*

Main category: math.LO

TL;DR: 论文通过引入\emph{抽象模型结构}的概念，提出了一种独立于具体逻辑语法/语义的广义紧致性定理，并证明了特定类别紧致抽象模型结构的若干表征定理。


<details>
  <summary>Details</summary>
Motivation: 传统紧致性定理的证明依赖于具体逻辑的语法/语义特性，本文旨在发展一种不依赖这些特性的广义紧致性理论。

Method: 采用\emph{抽象模型结构}作为理论框架，摆脱对具体逻辑形式的依赖，构建普适性的紧致性定义。

Result: 建立了广义紧致性定理，并针对一类特定的紧致抽象模型结构证明了多个表征定理。

Conclusion: 该方法统一了不同逻辑系统中的紧致性现象，为逻辑学研究提供了更抽象的数学工具。

Abstract: The compactness theorem for a logic states, roughly, that the satisfiability
of a set of well-formed formulas can be determined from the satisfiability of
its finite subsets, and vice versa. Usually, proofs of this theorem depend on
the syntactic/semantic particularities of the corresponding logic. In this
paper, using the notion of \emph{abstract model structures}, we show that one
can develop a generalized notion of compactness that is independent of these.
Several characterization theorems for a particular class of compact abstract
model structures are also proved.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [26] [The Local Structure Theorem for Graph Minors with Finite Index](https://arxiv.org/abs/2507.02769)
*Christophe Paul,Evangelos Protopapas,Dimitrios M. Thilikos,Sebastian Wiederrecht*

Main category: math.CO

TL;DR: 该论文扩展了图子式的局部结构定理(LST)，证明了在非涡旋单元着色情况下，图$G'$能包含一个大型网格$\Gamma$的次模型，且网格与原始墙$W$保持良好连接。


<details>
  <summary>Details</summary>
Motivation: 研究$H$-次式自由图中大型墙$W$的结构特性，探索在有限着色条件下图的嵌入性质与网格次模型的存在性。

Method: 通过定义"几乎嵌入"概念，分析超图$\mathcal{H}$在曲面$\Sigma$中的嵌入特性，并引入非涡旋单元有限着色假设，构造与墙$W$连接的网格次模型。

Result: 证明了改进版LST：当非涡旋单元被有限着色时，$G'$必然包含一个大型网格次模型$\Gamma$，其每个顶点包对应所有颜色的单元子图。

Conclusion: 该结果为$H$-次式自由图的分解提供了更强的结构描述，将颜色约束与网格次模型存在性相关联，扩展了经典LST的应用范围。

Abstract: The Local Structure Theorem (LST) for graph minors roughly states that every
$H$-minor free graph $G$ that contains a sufficiently large wall $W$, there is
a set of few vertices $A$ such that, upon removing $A$, the resulting graph
$G':=G - A$ admits an "almost embedding" $\delta$ into a surface $\Sigma$ in
which $H$ does not embed. By almost embedding, we mean that there exists a
hypergraph $\mathcal{H}$ whose vertex set is a subset of the vertex set of $G$
and an embedding of $\mathcal{H}$ in $\Sigma$ such that 1) the drawing of each
hyperedge of $\mathcal{H}$ corresponds to a cell of $\delta$, 2) the boundary
of each cell intersects only the vertices of the corresponding hyperedge, and
3) all remaining vertices and edges of $G'$ are drawn in the interior of cells.
The cells corresponding to hyperedges of arity at least $4$, called vortices,
are few in number and have small "depth", while a "large" part of the wall $W$
is drawn outside the vortices and is "grounded" in the embedding $\delta$. Now
suppose that the subgraphs drawn inside each of the non-vortex cells are
equipped with some finite index, i.e., each such cell is assigned a color from
a finite set. We prove a version of the LST in which the set $C$ of colors
assigned to the non-vortex cells exhibits "large" bidimensionality: The graph
$G'$ contains a minor model of a large grid $\Gamma$ where each bag
corresponding to a vertex $v$ of $\Gamma$, contains the subgraph drawn within a
cell carrying color $\alpha$, for every color $\alpha \in C$. Moreover, the
grid $\Gamma$ can be chosen in a way that is "well-connected" to the original
wall $W$.

</details>


### [27] [The algebraic structures of social organizations: the operad of cooperative games](https://arxiv.org/abs/2507.01969)
*Dylan Laplace Mermoud,Victor Roca i Lucio*

Main category: math.CO

TL;DR: 本文通过代数操作理论为合作博弈论建立了一个以博弈组合/聚合为核心概念的理论框架，证明了合作博弈可分解为2人议价博弈和独裁博弈的线性组合，并研究了各类博弈及解概念在组合下的性质。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个统一框架，将Owen、Shapley等人提出的博弈求和、乘积与组合等操作纳入代数操作理论体系，揭示合作博弈的深层代数结构。

Method: 采用代数操作理论方法：首先赋予所有合作博弈集合以操作结构，证明其广义性；通过生成元与关系显式计算该操作，利用M\"obius变换建立与交换三重结合代数操作的规范同构。

Result: 1. 证明任何合作博弈都是2人议价博弈与独裁博弈迭代组合的线性组合\n2. 发现简单博弈、平衡博弈、容量（模糊测度）、凸函数等类博弈在组合下封闭\n3. 针对凸博弈，利用子模定理在广义排列多面体上构建新操作结构\n4. 给出复合博弈核心的组件核心描述式，推导出复合博弈的Shapley值与Banzhaf指数显式公式

Conclusion: 该框架通过操作理论统一了合作博弈的各类组合操作，揭示了其与三重结合代数的深刻联系，为博弈解概念的组合行为提供了系统性分析工具，拓展了广义排列多面体的代数研究。

Abstract: The main goal of this paper is to settle a conceptual framework for
cooperative game theory in which the notion of composition/aggregation of games
is the defining structure. This is done via the mathematical theory of
algebraic operads: we start by endowing the collection of all cooperative games
with any number of players with an operad structure, and we show that it
generalises all the previous notions of sums, products and compositions of
games considered by Owen, Shapley, von Neumann and Morgenstern, and many
others. Furthermore, we explicitly compute this operad in terms of generators
and relations, showing that the M\"obius transform map induces a canonical
isomorphism between the operad of cooperative games and the operad that encodes
commutative triassociative algebras. In other words, we prove that any
cooperative game is a linear combination of iterated compositions of the
2-player bargaining game and the 2-player dictator games. We show that many
interesting classes of games (simple, balanced, capacities a.k.a fuzzy measures
and convex functions, totally monotone, etc) are stable under compositions, and
thus form suboperads. In the convex case, this gives by the submodularity
theorem a new operad structure on the family of all generalized permutahedra.
Finally, we focus on how solution concepts in cooperative game theory behave
under composition: we study the core of a composite and describe it in terms of
the core of its components, and we give explicit formulas for the Shapley value
and the Banzhaf index of a compound game.

</details>


### [28] [Marcello's completion of graphs](https://arxiv.org/abs/2507.02015)
*Johan Kok*

Main category: math.CO

TL;DR: 本文提出了一种新的图优化问题，称为Marcello补全，定义了Marcello数$\varpi(G)$，并给出了初步结果。


<details>
  <summary>Details</summary>
Motivation: 研究图补全过程中的优化问题，定义Marcello补全过程及其相关参数，为图论研究提供新视角。

Method: 通过迭代构造图$G_1,G_2,\dots,G_k$，按照Marcello规则在非相邻顶点间添加最大边数，直至得到完全图$K_n$。

Result: 定义了图的Marcello数$\varpi(G)$，并给出$\varpi(K_n) = 0$的约定，同时展示了一些初步研究成果。

Conclusion: Marcello补全为图论中的优化问题提供了新的研究方向，初步结果为进一步探索奠定了基础。

Abstract: This paper initiates a study on a new optimization problem with regards to
graph completion. The defined procedure is called, \emph{Marcello's completion}
of a graph. For graph $G$ of order $n$ the \emph{Marcello number} is obtained
by iteratively constructing graphs, $G_1,G_2,\dots,G_k$ by adding a maximal
number of edges between pairs of distinct, non-adjacent vertices in accordance
with the \emph{Marcello rule}. If for smallest $k$ the resultant graph $G_k
\cong K_n$ then the Marcello number of a graph $G$ denoted by $\varpi(G)$ is
equal to $\varpi(G) = k$. By convention $\varpi(K_n) = 0$, $n \geq 1$. Certain
introductory results are presented.

</details>


### [29] [Fundamental and homogeneous bases of Hopf algebras built from nonsymmetric operads](https://arxiv.org/abs/2507.02165)
*Samuele Giraudo*

Main category: math.CO

TL;DR: 本文在自由非对称操作数上引入新的偏序结构，涉及装饰有序根树，其终端区间是格。这些格虽不具备分级、自对偶或半分配性，但具有EL-shellable性质，且M\"obius函数值在$\{-1, 0, 1\}$中。通过构建子格，为自由非对称操作数的自然Hopf代数建立了两个新基：基本基和齐次基，与现有初等基形成三重基结构。


<details>
  <summary>Details</summary>
Motivation: 研究自由非对称操作数上的偏序结构，旨在扩展对Hopf代数基的理解，并与Malvenuto-Reutenauer、Loday-Ronco及非交换对称函数中的三重基结构进行类比。

Method: 在装饰有序根树上定义新的偏序结构，分析其终端区间的格性质，并利用森林树族的子格构造Hopf代数的新基。

Result: 发现新格具有EL-shellable性质且M\"obius函数值受限，成功构建自由非对称操作数Hopf代数的三重基（初等基、基本基、齐次基），与已知Hopf代数结构相似。

Conclusion: 该研究不仅丰富了自由非对称操作数的偏序理论，还通过三重基的构建为Hopf代数提供了新的结构视角，与多个经典Hopf代数体系形成深刻对应。

Abstract: We introduce new partial order structures on the underlying sets of free
nonsymmetric operads. These posets involve decorated ordered rooted trees, and
their terminal intervals are lattices. These lattices are not graded, not
self-dual, and not semi-distributive, but they are EL-shellable, and their
M\"bius functions take values in $\{-1, 0, 1\}$. They admit sublattices on the
families of $m$-Fuss-Catalan objects and of forests of trees. This latter order
structure is used to construct two new bases for the natural Hopf algebras of
free nonsymmetric operads: a fundamental basis and a homogeneous basis. Along
with the already known elementary basis of these Hopf algebras, this yields a
triple of bases. The situation is similar to what is observed in the Hopf
algebras of Malvenuto-Reutenauer, Loday-Ronco, and noncommutative symmetric
functions, each of which presents such triples of bases and basis changes
involving, respectively, the right weak partial order, the Tamari partial
order, and the Boolean lattice partial order.

</details>


### [30] [The Cycle Counts of Graphs](https://arxiv.org/abs/2507.02260)
*Ryan McCulloch,Brendan D. McKay,Alireza Salahshoori,Thomas Zaslavsky*

Main category: math.CO

TL;DR: 论文证明了不可分图可以具有任意正数目的环，除了六个例外情况2、4、5、8、9、16，而不可分三次图还有一个额外的例外13。


<details>
  <summary>Details</summary>
Motivation: 研究不可分图和不可分三次图中环的数量限制，填补图论领域的知识空白。

Method: 通过数学证明和图论分析，确定不可分图和不可分三次图中环数量的例外情况。

Result: 不可分图在环数量上有六个例外（2、4、5、8、9、16），不可分三次图多一个例外13。

Conclusion: 该研究为不可分图和不可分三次图的环数量提供了完整的例外清单，对图论研究具有重要意义。

Abstract: We prove that an inseparable graph can have any positive number of cycles
with the six exceptions 2, 4, 5, 8, 9, 16, and that an inseparable cubic graph
has the sole additional exception 13.

</details>


### [31] [Some Turán-type results for signless Laplacian spectral radius](https://arxiv.org/abs/2507.02263)
*Jian Zheng,Yongtao Li,Yi-Zheng Fan*

Main category: math.CO

TL;DR: 本文系统性地研究了基于无符号拉普拉斯谱半径的Nikiforov定理变体，扩展了超饱和、团爆炸和稳定性结果。


<details>
  <summary>Details</summary>
Motivation: Bollob\'{a}s和Erd\H{o}s的经典定理以及Nikiforov的谱版本定理为研究图的爆炸团提供了基础。本文旨在探索无符号拉普拉斯谱半径下的类似结果，以扩展相关理论。

Method: 通过分析图的谱性质，特别是无符号拉普拉斯谱半径，研究其在超饱和、团爆炸和稳定性问题中的应用。

Result: 证明了在无符号拉普拉斯谱半径条件下，图包含爆炸团$K_{k+1}[t]$，其中$t=\Omega_{k,\varepsilon}(\log n)$，扩展了Nikiforov的结果。

Conclusion: 本文成功地将Nikiforov的谱定理推广到无符号拉普拉斯谱半径，为图的谱理论提供了新的视角和工具。

Abstract: Half a century ago, Bollob\'{a}s and Erd\H{o}s [Bull. London Math. Soc. 5
(1973)] proved that every $n$-vertex graph $G$ with $e(G)\ge (1- \frac{1}{k} +
\varepsilon )\frac{n^2}{2}$ edges contains a blowup $K_{k+1}[t]$ with
$t=\Omega_{k,\varepsilon}(\log n)$. A well-known theorem of Nikiforov [Combin.
Probab. Comput. 18 (3) (2009)] asserts that if $G$ is an $n$-vertex graph with
adjacency spectral radius $\lambda (G)\ge (1- \frac{1}{k} + \varepsilon)n$,
then $G$ contains a blowup $K_{k+1}[t]$ with $t=\Omega_{k,\varepsilon}(\log
n)$. This gives a spectral version of the Bollob\'{a}s--Erd\H{o}s theorem. In
this paper, we systematically explore variants of Nikiforov's result in terms
of the signless Laplacian spectral radius, extending the supersaturation,
blowup of cliques and the stability results.

</details>


### [32] [Oriented Diameter of Mixed Graphs with Given Maximum Undirected Degree](https://arxiv.org/abs/2507.02277)
*Ran An,Hengzhe Li,Jianbing Liu,Gaoxing Sun*

Main category: math.CO

TL;DR: 本文扩展了Dankelmann等人关于无桥无向图和有向图的有向直径的研究，将其推广到包含有向和无向边的混合图，并给出了基于顶点度的尖锐界限。


<details>
  <summary>Details</summary>
Motivation: 2018年Dankelmann等人建立了无桥无向图及二分图的有向直径与顶点度的尖锐界限。本研究旨在将这些结果推广到更一般的混合图，即同时包含有向和无向边的图。

Method: 通过定义混合图中顶点的无向度$d^*_G(x)$和最大无向度$\Delta^*(G)$，并分析不同条件下的有向直径界限，包括普通混合图和二分混合图。

Result: 对于普通混合图，有向直径$\overrightarrow{\mathrm{diam}}(G)$的上界为$n - \Delta^* + 3$或$n - \Delta^* + 4$；对于无桥混合二分图，上界为$2(|A| - d(u)) + 7$、$2(|A| - d^*(u)) + 8$或$2(|A| - d^*(u)) + 10$。除最后一种情况外，所有界限均为尖锐。

Conclusion: 研究成功将原有理论推广到混合图，并建立了基于顶点度的有向直径尖锐界限，为混合图的结构分析提供了重要工具。

Abstract: In 2018, Dankelmann, Gao, and Surmacs [J. Graph Theory, 88(1): 5--17, 2018]
established sharp bounds on the oriented diameter of a bridgeless undirected
graph and a bridgeless undirected bipartite graph in terms of vertex degree. In
this paper, we extend these results to \emph{mixed graphs}, which contain both
directed and undirected edges.
  Let the \emph{undirected degree} $d^*_G(x)$ of a vertex $x \in V(G)$ be the
number of its incident undirected edges in a mixed graph $G$ of order $n$, and
let the \emph{maximum undirected degree} be $\Delta^*(G) = \max\{d^*_G(v) : v
\in V(G)\}$. We prove that \begin{align*} \text{(1)}\quad &
\overrightarrow{\mathrm{diam}}(G) \leq n - \Delta^* + 3 && \text{if $G$ is
undirected, or contains a vertex $u$ with $d^*_G(u) = \Delta^*$} \\ & &&
\text{and $d^+_G(u) + d^-_G(u) \geq 2$, or $\Delta^* = 5$ and $d^+_G(u) +
d^-_G(u) = 1$;} \\ \text{(2)}\quad & \overrightarrow{\mathrm{diam}}(G) \leq n -
\Delta^* + 4 && \text{otherwise}. \end{align*} We also establish bounds for
mixed bipartite graphs. If $G$ is a bridgeless mixed bipartite graph with
partite sets $A$ and $B$, and $u \in B$, then \begin{align*} \text{(1)}\quad &
\overrightarrow{\mathrm{diam}}(G) \leq 2(|A| - d(u)) + 7 && \text{if $G$ is
undirected;\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ } \\ \text{(2)}\quad & \overrightarrow{\mathrm{diam}}(G) \leq
2(|A| - d^*(u)) + 8 && \text{if $d^+_G(u) + d^-_G(u) \geq 2$;} \\
\text{(3)}\quad & \overrightarrow{\mathrm{diam}}(G) \leq 2(|A| - d^*(u)) + 10
&& \text{otherwise}. \end{align*}
  All of the above bounds are sharp, except possibly the last one.

</details>


### [33] [On the (in)equivalence of Brouwer's fixed point theorem and Sperner's lemma](https://arxiv.org/abs/2507.02366)
*Junichi Minagawa*

Main category: math.CO

TL;DR: 本文探讨了一维情况下Brouwer不动点定理与Sperner引理的关系，证明了两者可以互相推导但不等价，因为Sperner引理在有理数有序域成立而Brouwer定理需要完备性。


<details>
  <summary>Details</summary>
Motivation: 研究Brouwer不动点定理与Sperner引理在一维情况下的相互关系，揭示它们在数学基础上的差异。

Method: 通过构造性证明展示如何用Sperner引理推导Brouwer定理，反之亦然，并分析两者对数学基础的不同要求。

Result: 证明Sperner引理在有理数域成立，而Brouwer定理需要实数完备性，表明两者在逻辑上不等价。

Conclusion: 虽然Brouwer定理与Sperner引理可以相互证明，但由于对数学基础的依赖不同，它们本质上是不同的数学命题。

Abstract: We consider Brouwer's fixed point theorem and Sperner's lemma in one
dimension. We present a proof of the Brouwer theorem using the Sperner lemma,
and vice versa. However, we also show that they are not equivalent, because the
Sperner lemma holds in the ordered field of rational numbers, whereas proving
the Brouwer theorem requires the property of completeness.

</details>


### [34] [Vertex-transitive nut graph order-degree existence problem](https://arxiv.org/abs/2507.02481)
*Ivan Damnjanović*

Main category: math.CO

TL;DR: 本文证明了对于满足特定条件的阶数$n$和度数$d$，存在$d$正则的Cayley坚果图，并由此确定了所有存在$d$正则顶点传递（或Cayley）坚果图的$(n, d)$对。


<details>
  <summary>Details</summary>
Motivation: 研究坚果图的存在条件，特别是顶点传递和Cayley坚果图的构造，以填补图论中关于这类特殊图的系统性知识空白。

Method: 通过分析坚果图的邻接矩阵特征（零特征值及其对应特征向量无零项），结合群论方法构造满足条件的Cayley图。

Result: 证明了对于满足$4 \mid d$且$n \ge d+4$，或$d \equiv 2 \pmod{4}$且$n \ge d+6$的$(n,d)$对，存在对应的$d$正则Cayley坚果图。

Conclusion: 该研究完整刻画了顶点传递和Cayley坚果图的存在范围，为后续研究提供了理论基础。

Abstract: A nut graph is a nontrivial simple graph whose adjacency matrix has a simple
eigenvalue zero such that the corresponding eigenvector has no zero entries. It
is known that the order $n$ and degree $d$ of a vertex-transitive nut graph
satisfy $4 \mid d$, $d \ge 4$, $2 \mid n$ and $n \ge d + 4$; or $d \equiv 2
\pmod 4$, $d \ge 6$, $4 \mid n$ and $n \ge d + 6$. Here, we prove that for each
such $n$ and $d$, there exists a $d$-regular Cayley nut graph of order $n$. As
a direct consequence, we obtain all the pairs $(n, d)$ for which there is a
$d$-regular vertex-transitive (resp. Cayley) nut graph of order $n$.

</details>


### [35] [Characterizing circle graphs with binomial partial Petrial polynomials](https://arxiv.org/abs/2507.02421)
*Ruiqing Feng,Qi Yan,Xuan Zheng*

Main category: math.CO

TL;DR: 本文解决了Yan和Li提出的关于圆图的偏Petrial多项式为二项式的特征问题，证明连通圆图的偏Petrial多项式为二项式当且仅当该图为路径。


<details>
  <summary>Details</summary>
Motivation: Gross、Mansour和Tucker首次引入偏Petrial多项式作为生成函数，用于枚举带图的所有可能偏Petrial的欧拉亏格。Yan和Li将其推广至圆图，并发现路径产生二项式多项式，进而提出特征问题：识别所有偏Petrial多项式为二项式的连通圆图。

Method: 通过局部补图的方法，对连通圆图的偏Petrial多项式进行分析，研究其二项式性质的条件。

Result: 证明了对于连通圆图，偏Petrial多项式为二项式当且仅当该图是路径。

Conclusion: 本研究完全解决了Yan和Li提出的开放问题，确定了偏Petrial多项式为二项式的连通圆图的特征，即这些图必须是路径。

Abstract: The partial Petrial polynomial was first introduced by Gross, Mansour, and
Tucker as a generating function that enumerates the Euler genera of all
possible partial Petrials on a ribbon graph. Yan and Li later extended this
polynomial invariant to circle graphs by utilizing the correspondence between
circle graphs and bouquets. Their explicit computation demonstrated that paths
produce binomial polynomials, specifically those containing exactly two
non-zero terms. This discovery led them to pose a fundamental characterization
problem: identify all connected circle graphs whose partial Petrial polynomial
is binomial. In this paper, we solve this open problem in terms of local
complementation and prove that for connected circle graphs, the binomial
property holds precisely when the graph is a path.

</details>


### [36] [Method of Weighted Words on Cylindric Partitions](https://arxiv.org/abs/2507.02435)
*Burcu Barsakçı*

Main category: math.CO

TL;DR: 研究了具有特定轮廓$c=(c_1, c_2, \ldots, c_r)$、秩为2且层级为2、3和4的柱状分拆的生成函数，提出了替代Borodin公式的表达形式。


<details>
  <summary>Details</summary>
Motivation: 探索柱状分拆生成函数的替代表达式，以补充Borodin的现有公式，并扩展加权词方法的应用范围。

Method: 采用了Alladi和Gordon首次引入的加权词方法，后经Dousse改进，用于证明分拆恒等式和获得无限积，本研究以更组合的方式应用该方法。

Result: 得出了层级为2、3和4的柱状分拆生成函数的替代表达式，为Borodin公式提供了新的视角。

Conclusion: 通过组合方法成功扩展了加权词技术的应用，为柱状分拆生成函数的研究提供了新的工具和结果。

Abstract: We study the generating functions of cylindric partitions having profile
$c=(c_1, c_2, \ldots, c_r)$ with rank $2$ and levels $2, 3$ and $4$. As a
result, we give expressions alternative to Borodin's formula for these
generating functions. We use the method of weighted words which was first
introduced by Alladi and Gordon, later was applied by Dousse in a new version
to prove some partition identities and to get infinite products. We adopt the
method to our subject with a more combinatorial approach.

</details>


### [37] [New orientable sequences](https://arxiv.org/abs/2507.02526)
*Chris J Mitchell,Peter R Wild*

Main category: math.CO

TL;DR: 本文改进了可定向序列周期上界，提出基于de Bruijn图子图的新构造方法，并在n=2,3及n=4(k为奇数)时达到新上界，部分n=4-8序列周期超过已知最大值。


<details>
  <summary>Details</summary>
Motivation: 可定向序列在位置传感中有潜在应用价值，但现有构造方法的周期与理论上界存在显著差距，需改进上界并开发更优构造方法。

Method: 通过构建具有特殊性质的de Bruijn图子图，提出两种生成子图的新方法，进而构造周期更长的可定向序列。

Result: 改进后的上界在n=2,3及n=4(k为奇数)时被新构造序列达到；对4≤n≤8，部分序列周期创下新纪录。

Conclusion: 新方法显著提升了可定向序列的性能，为n≤8场景提供了更优解，但更大n值的构造仍待突破。

Abstract: Orientable sequences of order n are infinite periodic sequences with symbols
drawn from a finite alphabet of size k with the property that any particular
subsequence of length n occurs at most once in a period in either direction.
They were introduced in the early 1990s in the context of possible applications
in position sensing. Bounds on the period of such sequences and a range of
methods of construction have been devised, although apart from very small cases
a significant gap remains between the largest known period for such a sequence
and the best known upper bound. In this paper we first give improved upper
bounds on the period of such sequences. We then give a new general method of
construction for orientable sequences involving subgraphs of the de Bruijn
graph with special properties, and describe two different approaches for
generating such subgraphs. This enables us to construct orientable sequences
with periods meeting the improved upper bounds when n is 2 or 3, as well as n=4
and k odd. For 4\leq n\leq 8, in some cases the sequences produced by the
methods described have periods larger than for any previously known sequences.

</details>


### [38] [Semisimple algebras related to immaculate tableaux](https://arxiv.org/abs/2507.02539)
*John M. Campbell*

Main category: math.CO

TL;DR: 该论文构建了一个新的代数结构$\mathbb{C}\mathcal{I}_{n}$，其维度与标准不可变表（immaculate tableaux）的数量相关，并证明了其半单结构与对称群代数$\mathbb{C}S_{n}$的嵌入关系。通过双射证明递推关系，构造了由“不可变置换”（immacutations）索引的基，并赋予其幺半群代数结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于寻找与Frobenius-Young恒等式$n! = \sum_{\lambda \vdash n} (f^{\lambda})^{2}$类似的组合解释，其中$f^{\lambda}$是Young表数量。通过引入标准不可变表$g^{\alpha}$，试图在更一般的组合对象上建立类似的代数结构。

Method: 方法包括构造代数$\mathbb{C}\mathcal{I}_{n}$，其维度满足$\dim \mathbb{C}\mathcal{I}_{n} = \sum_{\alpha \vDash n} (g^{\alpha})^{2}$。通过双射证明递推关系，构造由不可变置换索引的基$\mathcal{B}_{n}$，并验证其幺半群代数性质。

Result: 结果表明$\mathbb{C}\mathcal{I}_{n}$是半单代数且包含$\mathbb{C}S_{n}$的嵌入。基$\mathcal{B}_{n}$在乘法运算下封闭，赋予不可变置换集合以幺半群结构，从而扩展了对称群代数的组合框架。

Conclusion: 结论指出，通过不可变表与不可变置换的组合构造，成功推广了Frobenius-Young恒等式的代数-组合对应关系，为研究更一般的置换类代数提供了新工具。

Abstract: Given a direct sum $A$ of full matrix algebras, if there is a combinatorial
interpretation associated with both the dimension of $A$ and the dimensions of
the irreducible $A$-modules, then this can be thought of as providing an
analogue of the famous Frobenius-Young identity $n! = \sum_{\lambda \vdash n} (
f^{\lambda} )^{2}$ derived from the semisimple structure of the symmetric group
algebra $\mathbb{C}S_{n}$, letting $f^{\lambda}$ denote the number of Young
tableaux of partition shape $\lambda \vdash n$. By letting $g^{\alpha}$ denote
the number of standard immaculate tableaux of composition shape $\alpha \vDash
n$, we construct an algebra $\mathbb{C}\mathcal{I}_{n}$ with a semisimple
structure such that $\dim \mathbb{C}\mathcal{I}_{n} = \sum_{\alpha \vDash n}
(g^{\alpha})^{2}$ and such that $\mathbb{C}\mathcal{I}_{n} $ contains an
isomorphic copy of $\mathbb{C}S_{n}$. We bijectively prove a recurrence for
$\dim \mathbb{C}\mathcal{I}_{n}$ so as to construct a basis of
$\mathbb{C}\mathcal{I}_{n}$ indexed by permutation-like objects that we refer
to as immacutations. We form a basis $\mathcal{B}_{n}$ of
$\mathbb{C}\mathcal{I}_{n}$ such that $\mathbb{C} \mathcal{B}_n$ has the
structure of a monoid algebra in such a way so that $\mathcal{B}_n$ is closed
under the multiplicative operation of $\mathbb{C} \mathcal{I}_n$, yielding a
monoid structure on the set of order-$n$ immacutations.

</details>


### [39] [The ordering of hypertrees and unicyclic hypergraphs by the traces of $\mathcal{A}_α$-tensor](https://arxiv.org/abs/2507.02650)
*Jueru Liu,Lizhu Sun,Changjiang Bu*

Main category: math.CO

TL;DR: 本文研究了$k$-一致超图的$\mathcal{A}_{\alpha}$-张量及其谱矩，通过谱矩对超图进行字典序排序，并确定了特定类型超图的极值结构。


<details>
  <summary>Details</summary>
Motivation: 研究$\mathcal{A}_{\alpha}$-张量的谱矩及其在超图排序中的应用，旨在揭示超图结构的谱特性及其极值性质。

Method: 通过定义$\mathcal{A}_{\alpha}$-张量及其谱矩，利用谱矩对$k$-一致线性单环超图和超树进行字典序排序，并分析其极值结构。

Result: 确定了$k$-一致线性单环超图和超树在谱矩排序中的首尾极值结构，给出了特定周长和直径下的极值超图，并计算了谱矩的极值。

Conclusion: 本文成功刻画了$k$-一致线性单环超图和超树的谱矩极值结构，为超图的谱理论提供了新的研究视角和工具。

Abstract: For a real number $\alpha\in[0,1]$ and a $k$-uniform hypergraph
$\mathcal{H}$,
$\mathcal{A}_{\alpha}(\mathcal{H})=\alpha\mathcal{D}(\mathcal{H})+(1-\alpha)\mathcal{A}(\mathcal{H})$
is called the $\mathcal{A}_{\alpha}$-tensor of $\mathcal{H}$, where
$\mathcal{D}(\mathcal{H})$ and $\mathcal{A}(\mathcal{H})$ are the degree tensor
and adjacency tensor of $\mathcal{H}$, respectively. The sum of the $d$-th
powers of all eigenvalues of $\mathcal{A}_{\alpha}(\mathcal{H})$ is called the
$d$-th order $\mathcal{A}_{\alpha}$-spectral moment of $\mathcal{H}$, which is
equal to the $d$-th order trace of $\mathcal{A}_{\alpha}(\mathcal{H})$. In this
paper, some hypergraphs are ordered lexicographically by their
$\mathcal{A}_{\alpha}$-spectral moments in non-decreasing order. The first, the
second, the last and the second last hypergraphs among all $k$-uniform linear
unicyclic hypergraphs and hypertrees are characterized, respectively. We give
the first and the last hypergraphs among all $k$-uniform linear unicyclic
hypergraphs with given grith, and characterize the last hypertree among all
$k$-uniform hypertrees with given diameter. Furthermore, we determine some
extreme values of the $\mathcal{A}_{\alpha}$-spectral moments for hypertrees
and linear unicyclic hypergraphs, respectively.

</details>


### [40] [The pure condition for incidence geometries](https://arxiv.org/abs/2507.02661)
*Daniel Irving Bernstein,Signe Lundqvist*

Main category: math.CO

TL;DR: 本文研究了具有特定法向量的关联几何的平行重绘空间，引入了类似于杆件框架纯条件的纯条件，并通过不变理论证明其为括号多项式。


<details>
  <summary>Details</summary>
Motivation: 1989年Whiteley描述了具有通用法向量且点和超平面均不同的关联几何，但某些几何仅对特定法向量才能实现。本文旨在研究这类几何。

Method: 引入关联几何平行重绘的纯条件，类比White和Whiteley的杆件框架纯条件，利用不变理论证明其为括号多项式，并计算平面示例的具体表达式。

Result: 证明了d维纯条件是作用于超平面法向量的括号多项式，并给出了平面案例的显式计算示例。

Conclusion: 通过建立平行重绘的纯条件理论，为特定法向量下关联几何的可实现性提供了新的代数判别工具。

Abstract: The space of \emph{parallel redrawings} of an incidence geometry $(P,H,I)$
with an assigned set of normals is the set of points and hyperplanes in
$\mathbb{R}^d$ satisfying the incidences given by $(P,H,I)$, such that the
hyperplanes have the assigned normals.
  In 1989, Whiteley characterized the incidence geometries that have
d-dimensional realizations with generic hyperplane normals such that all points
and hyperplanes are distinct. However, some incidence geometries can be
realized as points and hyperplanes in d-dimensional space, with the points and
hyperplanes distinct, but only for specific choices of normals. Such incidence
geometries are the topic of this article.
  In this article, we introduce a pure condition for parallel redrawings of
incidence geometries, analogous to the pure condition for bar-and-joint
frameworks, introduced by White and Whiteley. The d-dimensional pure condition
of an incidence geometry (P,H,I) imposes a condition on the normals assigned to
the hyperplanes of (P,H,I) required for d-dimensional realizations of (P,H,I)
with distinct points. We use invariant theory to show that is a bracket
polynomial. We will also explicitly compute the pure condition as a bracket
polynomial for some examples in the plane.

</details>


### [41] [A note on maximal plane subgraphs of the complete twisted graph containing perfect matchings](https://arxiv.org/abs/2507.02711)
*Elsa Omaña-Pulido,Eduardo Rivera-Campo*

Main category: math.CO

TL;DR: 本文研究了扭曲图$T_{n}$中最大平面子图的完美匹配性质，证明了任意两个包含完美匹配的最大平面子图可以通过一系列单边交换相互转换。


<details>
  <summary>Details</summary>
Motivation: 探索扭曲图$T_{n}$中最大平面子图的结构特性，特别是其完美匹配的可达性问题，为图论中的平面图变换提供理论基础。

Method: 通过构造序列$S=F_0, F_1, \ldots, F_m=R$，证明每对相邻子图$F_i$与$F_{i+1}$之间仅需一次单边交换即可转换。

Result: 在扭曲图$T_{n}$中，任意两个含完美匹配的最大平面子图均可通过有限次单边交换相互转化。

Conclusion: 该结果揭示了扭曲图最大平面子图的连通性，为后续研究平面图动态变换提供了重要工具。

Abstract: The twisted graph $T_{n}$ is a drawing of the complete graph with $n$
vertices $v_{1},v_{2},\ldots ,v_{n}$ in which two edges $v_{i}v_{j}$ ($i<j$)
and $v_{s}v_{t}$ ($s<t$) cross if and only if $i<s<t<j$ or $s<i<j<t$. We show
that for any maximal plane subgraphs $S$ and $R$ of $T_{n}$, each containing at
least one perfect matching, there is a sequence $S=F_0, F_1, \ldots, F_m=R$ of
maximal plane subgrahs of $T_n$, also containing perfect matchings, such that
for $i=0,1, \ldots, m-1$, $F_{i+1}$ can be obtained from $F_{i}$ by a single
edge exchange.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [42] [A unified model of horizontal differentiation with general spaces and irrational consumers](https://arxiv.org/abs/2507.01985)
*Aldric Labarthe,Yann Kerzreho*

Main category: q-fin.MF

TL;DR: 本文提出了一种新的水平差异化微观经济模型，统一并扩展了Hotelling (1929) 启发的先前研究，结合了有限理性消费者、无限企业数量及黎曼流形差异化空间。


<details>
  <summary>Details</summary>
Motivation: 现有水平差异化模型未能充分处理复杂产品空间和消费者行为，需要更通用的理论框架来捕捉现实市场动态。

Method: 采用黎曼几何工具构建理论模型，允许任意维度的差异化空间，并引入有限理性消费者假设和无限企业竞争设定。

Result: 证明黎曼几何为分析复杂产品市场提供了自然框架，能揭示传统模型无法捕捉的企业行为和市场结构特征。

Conclusion: 黎曼流形上的差异化模型为产业组织理论提供了新的分析范式，特别适用于具有多维属性的复杂产品市场研究。

Abstract: We introduce a new microeconomic model of horizontal differentiation that
unifies and extends previous developments inspired by the seminal work of
Hotelling (1929). Our framework incorporates boundedly rational consumers, an
unlimited number of firms, and arbitrary differentiation spaces with Riemannian
manifolds. We argue that Riemannian geometry provides a natural and powerful
tool for analyzing such models, offering fresh insights into firm behavior and
market structure with complex products.

</details>


### [43] [Arbitrage with bounded Liquidity](https://arxiv.org/abs/2507.02027)
*Christoph Schlegel*

Main category: q-fin.MF

TL;DR: 本文推导了两个流动性不足市场间的套利收益（即LVR），采用二次交易成本模型分析适用场景，并探讨了模型扩展方向。


<details>
  <summary>Details</summary>
Motivation: 研究流动性差异市场间的套利收益量化问题，特别关注CEX-DEX和DEX-DEX套利场景的实际适用性。

Method: 使用二次交易成本模型模拟高流动性交易所的交易成本，分析模型在长尾CEX-DEX套利等场景的有效性边界。

Result: 模型适用于长尾CEX-DEX套利和DEX-DEX套利，但对主流交易对的CEX-DEX套利解释力有限。

Conclusion: 提出了扩展其他成本函数的研究路径，为未来流动性套利理论研究指明方向。

Abstract: The arbitrage gains or, equivalently, Loss Versus Rebalacing (LVR) for
arbitrage between two imperfectly liquid markets is derived. To derive the LVR,
I assume a quadratic trading cost to model the cost of trading on the more
liquid exchange and discuss to which situations my model arguably applies well
(long tail CEX-DEX arbitrage, DEX-DEX arbitrage) and to which not so well
(CEX-DEX arbitrage for major pairs). I discuss extension to other cost
functions and directions for future research.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [44] [Optimising task allocation to balance business goals and worker well-being for financial service workforces](https://arxiv.org/abs/2507.01968)
*Chris Duckworth,Zlatko Zlatev,James Sciberras,Peter Hallett,Enrico Gerding*

Main category: q-fin.GN

TL;DR: 本文提出一种考虑员工福祉的任务分配模型，采用遗传算法优化金融分析师的任务分配与调度，在提升效率的同时兼顾员工福利。


<details>
  <summary>Details</summary>
Motivation: 金融服务业数据量庞大，错误处理任务繁重导致分析师压力剧增，传统方法难以兼顾业务效率与员工福祉。

Method: 使用遗传算法（GA）构建多目标优化模型，在匹配分析师技能经验的同时，将员工福祉纳入调度目标函数。

Result: 实验表明该GA模型优于基线启发式方法和现行实践，可适应单/多目标场景，为管理者提供高效且人性化的分配方案。

Conclusion: 研究填补了现有调度模型忽视员工福祉的空白，证明元启发式算法能在提升业务效率的同时实现人性化任务分配。

Abstract: Purpose: Financial service companies manage huge volumes of data which
requires timely error identification and resolution. The associated tasks to
resolve these errors frequently put financial analyst workforces under
significant pressure leading to resourcing challenges and increased business
risk. To address this challenge, we introduce a formal task allocation model
which considers both business orientated goals and analyst well-being.
  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to
allocate and schedule tasks to analysts. The proposed solution is able to
allocate tasks to analysts with appropriate skills and experience, while taking
into account staff well-being objectives.
  Findings: We demonstrate our GA model outperforms baseline heuristics,
current working practice, and is applicable to a range of single and
multi-objective real-world scenarios. We discuss the potential for
metaheuristics (such as GAs) to efficiently find sufficiently good allocations
which can provide recommendations for financial service managers in-the-loop.
  Originality: A key gap in existing allocation and scheduling models, is fully
considering worker well-being. This paper presents an allocation model which
explicitly optimises for well-being while still improving on current working
practice for efficiency.

</details>


### [45] [Predicting and Explaining Customer Data Sharing in the Open Banking](https://arxiv.org/abs/2507.01987)
*João B. G. de Brito,Rodrigo Heldt,Cleo S. Silveira,Matthias Bogaert,Guilherme B. Bucco,Fernando B. Luce,João L. Becker,Filipe J. Zabala,Michel J. Anzanello*

Main category: q-fin.GN

TL;DR: 研究提出一个预测客户在开放银行中数据共享倾向的框架，结合EMA解释模型，通过XGBoost模型和混合数据平衡策略，在巴西金融机构数据中实现高精度预测，揭示移动渠道互动和信贷特征是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 开放银行的兴起改变了金融数据管理方式，金融机构需平衡数据流入与流出以提升竞争力，研究旨在预测客户数据共享行为并解读其驱动因素。

Method: 采用ADASYN和NEARMISS混合策略处理数据不平衡问题，训练XGBoost模型预测数据共享行为，并通过SHAP与CART结合的EMA方法解释关键特征。

Result: 模型对数据流入和流出的预测准确率分别达91.39%和91.53%，移动渠道交易量、互动频率及全国信用卡使用情况是最显著的影响特征。

Conclusion: 移动参与度和信贷特征是驱动客户数据共享的核心因素，研究为金融机构在开放银行环境中制定战略提供了关键洞察。

Abstract: The emergence of Open Banking represents a significant shift in financial
data management, influencing financial institutions' market dynamics and
marketing strategies. This increased competition creates opportunities and
challenges, as institutions manage data inflow to improve products and services
while mitigating data outflow that could aid competitors. This study introduces
a framework to predict customers' propensity to share data via Open Banking and
interprets this behavior through Explanatory Model Analysis (EMA). Using data
from a large Brazilian financial institution with approximately 3.2 million
customers, a hybrid data balancing strategy incorporating ADASYN and NEARMISS
techniques was employed to address the infrequency of data sharing and enhance
the training of XGBoost models. These models accurately predicted customer data
sharing, achieving 91.39% accuracy for inflow and 91.53% for outflow. The EMA
phase combined the Shapley Additive Explanations (SHAP) method with the
Classification and Regression Tree (CART) technique, revealing the most
influential features on customer decisions. Key features included the number of
transactions and purchases in mobile channels, interactions within these
channels, and credit-related features, particularly credit card usage across
the national banking system. These results highlight the critical role of
mobile engagement and credit in driving customer data-sharing behaviors,
providing financial institutions with strategic insights to enhance
competitiveness and innovation in the Open Banking environment.

</details>


### [46] [Integrating Large Language Models in Financial Investments and Market Analysis: A Survey](https://arxiv.org/abs/2507.01990)
*Sedigheh Mahdavi,Jiating,Chen,Pradeep Kumar Joshi,Lina Huertas Guativa,Upmanyu Singh*

Main category: q-fin.GN

TL;DR: 本文综述了大语言模型（LLMs）在金融决策中的应用，包括股票选择、风险评估、情感分析、交易和金融预测等领域，并总结了现有研究的框架、能力、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统投资策略主要依赖定量模型和基本面分析，而LLMs能够处理和分析大量结构化和非结构化数据，实时提取有价值的信息，从而提升金融决策能力。

Method: 研究通过结构化综述方法，将LLMs在金融领域的研究分为四大框架：基于LLM的框架与流程、混合集成方法、微调与适应方法以及基于代理的架构。

Result: LLMs在金融市场的应用中展现出强大的潜力，能够增强数据分析能力并优化决策过程，但也面临数据质量、模型解释性和实时性等挑战。

Conclusion: LLMs为金融领域带来了新的分析工具和方法，未来研究应进一步探索其实际应用中的优化方向和技术挑战。

Abstract: Large Language Models (LLMs) have been employed in financial decision making,
enhancing analytical capabilities for investment strategies. Traditional
investment strategies often utilize quantitative models, fundamental analysis,
and technical indicators. However, LLMs have introduced new capabilities to
process and analyze large volumes of structured and unstructured data, extract
meaningful insights, and enhance decision-making in real-time. This survey
provides a structured overview of recent research on LLMs within the financial
domain, categorizing research contributions into four main frameworks:
LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and
Adaptation Approaches, and Agent-Based Architectures. This study provides a
structured review of recent LLMs research on applications in stock selection,
risk assessment, sentiment analysis, trading, and financial forecasting. By
reviewing the existing literature, this study highlights the capabilities,
challenges, and potential directions of LLMs in financial markets.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [47] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 本研究开发了FinAI-BERT模型，用于在金融文本中句子级别精准识别AI相关内容，性能显著优于传统方法，为金融NLP提供了细粒度分类解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有AI披露检测工具在粒度、可解释性和鲁棒性上存在不足，需要开发更精准的金融领域专用模型来满足监管和研究需求。

Method: 基于Transformer架构构建FinAI-BERT模型，使用1,586条手工标注的美国银行年报句子(2015-2023年)进行微调，采用SHAP进行可解释性分析。

Result: 模型达到99.37%准确率和0.993 F1值，在句子长度、对抗样本和时间样本测试中表现稳定，显著优于逻辑回归等传统方法。

Conclusion: 该研究不仅推进了金融NLP的细粒度主题分类理论，还为分析师和监管者提供了可扩展的AI披露监测工具。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [48] [MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation](https://arxiv.org/abs/2507.02057)
*Lu Yan,Zhuo Zhang,Xiangzhe Xu,Shengwei An,Guangyu Shen,Zhou Xuan,Xuan Chen,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为恶意软件生成编译器（MGC）的新型框架，通过模块化分解和规避对齐机制生成恶意软件，揭示了大型语言模型（LLMs）在安全防护上的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型降低了软件开发的门槛，但也可能被用于恶意软件开发。现有的对齐机制主要评估单个提示，忽视了恶意操作可被分解为看似无害的子任务的风险。

Method: MGC框架采用恶意描述中间表示（MDIR），将高级恶意意图与看似无害的代码片段连接起来，通过模块化分解和规避对齐机制生成恶意软件。

Result: 实验表明，MGC在三个基准数据集上生成的功能性恶意软件正确率分别比越狱方法和地下服务高365.79%和78.07%，并能复现甚至增强16个真实恶意软件样本。

Conclusion: 这项工作揭示了组合攻击对对齐AI系统的风险，为安全研究人员提供了重要见解。

Abstract: Large language models (LLMs) have democratized software development, reducing
the expertise barrier for programming complex applications. This accessibility
extends to malicious software development, raising significant security
concerns. While LLM providers have implemented alignment mechanisms to prevent
direct generation of overtly malicious code, these safeguards predominantly
evaluate individual prompts in isolation, overlooking a critical vulnerability:
malicious operations can be systematically decomposed into benign-appearing
sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a
novel framework that leverages this vulnerability through modular decomposition
and alignment-evasive generation. MGC employs a specialized Malware Description
Intermediate Representation (MDIR) to bridge high-level malicious intents and
benign-appearing code snippets. Extensive evaluation demonstrates that our
attack reliably generates functional malware across diverse task specifications
and categories, outperforming jailbreaking methods by +365.79% and underground
services by +78.07% in correctness on three benchmark datasets. Case studies
further show that MGC can reproduce and even enhance 16 real-world malware
samples. This work provides critical insights for security researchers by
exposing the risks of compositional attacks against aligned AI systems.
Demonstrations are available at
https://sites.google.com/view/malware-generation-compiler.

</details>


### [49] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 本文探讨了人工智能（AI）在解决区块链预言机问题中的潜在作用，指出AI可作为补充层提升数据质量和系统韧性，但无法完全消除对链下输入的依赖。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题阻碍了去中心化应用的发展，现有方法未能彻底解决链下数据可靠输入的问题，因此研究AI在此领域的应用潜力。

Method: 通过分析学术文献和实践案例，评估了异常检测、语言事实提取、动态声誉建模和对抗性抵抗等AI技术对预言机系统的增强效果。

Result: AI能显著提升预言机的数据质量、来源选择和系统韧性，但仍需依赖无法完全验证的链外输入，无法独立解决信任问题。

Conclusion: AI应被视为预言机设计中的补充推理与过滤层，而非信任假设的替代方案，需与其他技术结合构建更健壮的解决方案。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [50] [ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space](https://arxiv.org/abs/2507.02177)
*Yan Long,Jiancong Cui,Yuqing Yang,Tobias Alam,Zhiqiang Lin,Kevin Fu*

Main category: cs.CR

TL;DR: 本文提出ARMOUR系统，用于监控Android零权限传感器的访问，以保护用户隐私并识别潜在的传感器滥用模式。该系统通过用户空间运行时监控技术，克服了现有方法的局限性，并在1448个商业应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 零权限传感器（如加速度计）在Android设备中不可或缺，但其访问可能导致隐私泄露。现有方法（如静态分析和动态分析）需要复杂的开发链、root权限或应用特定的逆向工程分析，难以让终端用户理解传感器的使用情况。

Method: ARMOUR利用Android固有的采样率变化和收敛行为，开发了一种用户空间运行时监控技术，无需root权限或应用特定的逆向工程分析，即可监控第三方应用对传感器数据的使用。

Result: 在1448个商业应用中的评估表明，ARMOUR能有效检测混淆代码等条件下的传感器使用，并发现显著的传感器滥用模式，例如50%看似与传感器无关的应用类别访问了多个零权限传感器的数据。

Conclusion: ARMOUR为隐私意识强的用户提供了便捷的传感器使用监控工具，并支持安全研究人员进行快速的应用无关传感器访问分析。同时，本文分析了Android近期政策变化对零权限传感器的影响及剩余的技术和监管问题。

Abstract: This work investigates how to monitor access to Android zero-permission
sensors which could cause privacy leakage to users. Moreover, monitoring such
sensitive access allows security researchers to characterize potential sensor
abuse patterns. Zero-permission sensors such as accelerometers have become an
indispensable part of Android devices. The critical information they provide
has attracted extensive research investigating how data collectors could
capture more sensor data to enable both benign and exploitative applications.
In contrast, little work has explored how to enable data providers, such as end
users, to understand sensor usage. While existing methods such as static
analysis and hooking-based dynamic analysis face challenges of requiring
complicated development chains, rooting privilege, and app-specific reverse
engineering analysis, our work aims to bridge this gap by developing ARMOUR for
user-space runtime monitoring, leveraging the intrinsic sampling rate variation
and convergence behaviors of Android. ARMOUR enables privacy-aware users to
easily monitor how third-party apps use sensor data and support security
researchers to perform rapid app-agnostic sensor access analysis. Our
evaluation with 1,448 commercial applications shows the effectiveness of ARMOUR
in detecting sensor usage in obfuscated code and other conditions, and observes
salient sensor abuse patterns such as 50% of apps from seemingly
sensor-independent categories accessing data of multiple zero-permission
sensors. We analyze the impact of Android's recent policy changes on
zero-permission sensors and remaining technical and regulatory problems.

</details>


### [51] [Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher](https://arxiv.org/abs/2507.02181)
*Pantelimon Stanica,Ranit Dutta,Bimal Mandal*

Main category: cs.CR

TL;DR: 本文提出了一种新型的截断内部$c$-差分密码分析方法，首次实现了$c$-差分均匀性在分组密码中的实际应用，并成功对9轮Kuznyechik密码进行了区分攻击。


<details>
  <summary>Details</summary>
Motivation: 虽然Ellingsen等人提出了$c$-差分均匀性概念，但由于乘法运算$c$会破坏分组密码分析所需的结构特性（特别是密钥加法），其实际应用一直受限。本文旨在解决这一关键挑战。

Method: 开发了内部$c$-差分方法（$(F(cx\oplus a), F(x))$），证明其均匀性等同于$F^{-1}$的外部$c$-差分均匀性。建立了统计-计算框架，对9轮Kuznyechik实施截断$c$-差分分析。

Result: 通过数百万差分对的计算分析，在所有测试轮数中均发现显著非随机性。针对完整9轮密码，发现多个触发安全警报的配置（偏差比达1.7倍，校正p值低至$1.85 \times 10^{-3}$）。

Conclusion: 这是首个针对完整9轮Kuznyechik的实际区分器，表明该密码对新攻击向量的安全裕度不足，验证了内部$c$-差分方法的实用价值。

Abstract: This paper introduces {\em truncated inner $c$-differential cryptanalysis}, a
novel technique that for the first time enables the practical application of
$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE
Trans. Inf. Theory, 2020) established the notion of $c$-differential uniformity
using $(F(x\oplus a), cF(x))$, a key challenge remained: multiplication by $c$
disrupts the structural properties essential for block cipher analysis,
particularly key addition.
  We resolve this challenge by developing an \emph{inner} $c$-differential
approach where multiplication by $c$ affects the input: $(F(cx\oplus a),
F(x))$. We prove that the inner $c$-differential uniformity of a function $F$
equals the outer $c$-differential uniformity of $F^{-1}$, establishing a
fundamental duality. This modification preserves cipher structure while
enabling practical cryptanalytic applications.
  Our main contribution is a comprehensive multi-faceted
statistical-computational framework, implementing truncated $c$-differential
analysis against the full 9-round Kuznyechik cipher (the inner
$c$-differentials are immune to the key whitening at the backend). Through
extensive computational analysis involving millions of differential pairs, we
demonstrate statistically significant non-randomness across all tested round
counts. For the full 9-round cipher, we identify multiple configurations
triggering critical security alerts, with bias ratios reaching $1.7\times$ and
corrected p-values as low as $1.85 \times 10^{-3}$, suggesting insufficient
security margin against this new attack vector. This represents the first
practical distinguisher against the full 9-round Kuznyechik.

</details>


### [52] [EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer](https://arxiv.org/abs/2507.02206)
*Ranyang Zhou,Abeer Matar A. Almalky,Gamana Aragonda,Sabbir Ahmed,Filip Roth Trønnes-Christensen,Adnan Siraj Rakin,Shaahin Angizi*

Main category: cs.CR

TL;DR: 提出基于DRAM物理随机性的新型硬件真随机数生成器EIM-TRNG，首次利用RowHammer扰动产生不可预测比特翻转，并应用于保护DNN权重数据的安全加密。


<details>
  <summary>Details</summary>
Motivation: 软件伪随机数生成器缺乏硬件真随机数生成器的不可预测性和鲁棒性，而DNN模型参数的安全保护对AI系统的完整性、隐私和知识产权至关重要。

Method: 通过精确控制的RowHammer操作诱发DRAM单元不可预测比特翻转作为熵源，结合固定与随机翻转对DNN权重进行加密编码，解密时利用翻转行为的概率特性生成密钥。

Result: 验证了基于DRAM的熵提取机制能实现低成本、高鲁棒性的硬件安全，为机器学习模型的硬件级保护提供了可行方案。

Conclusion: EIM-TRNG首次利用DRAM物理随机性构建真随机数生成器，为硬件安全及DNN模型保护开辟了新方向，同时保障了数据机密性与模型真实性。

Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware
security, cryptographic systems, and data protection. In the context of Deep
NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is
critical to ensure the integrity, privacy, and intel-lectual property of AI
systems. While software-based pseudo-random number generators are widely used,
they lack the unpredictability and resilience offered by hardware-based TRNGs.
In this work, we propose a novel and robust Encoding-in-Memory TRNG called
EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,
particularly under RowHammer-induced disturbances, for the first time. We
demonstrate how the unpredictable bit-flips generated through carefully
controlled RowHammer operations can be harnessed as a reliable entropy source.
Furthermore, we apply this TRNG framework to secure DNN weight data by encoding
via a combination of fixed and unpredictable bit-flips. The encrypted data is
later decrypted using a key derived from the probabilistic flip behavior,
ensuring both data confidentiality and model authenticity. Our results validate
the effectiveness of DRAM-based entropy extraction for robust, low-cost
hardware security and offer a promising direction for protecting machine
learning models at the hardware level.

</details>


### [53] [Linearly Homomorphic Ring Signature Scheme over Lattices](https://arxiv.org/abs/2507.02281)
*Heng Guo,Kun Tian,Fengxia Liu,Zhiyong Zheng*

Main category: cs.CR

TL;DR: 本文提出了首个基于格的全同态环签名方案，结合了环签名的强匿名性与全同态签名的可计算性，为需要匿名数据来源与可验证全同态计算的场景提供了后量子安全的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可行的全同态环签名方案，而此类方案在需要匿名数据来源与可验证全同态计算的场景（如机密区块链交易和安全多方计算）中具有重要潜力。

Method: 基于格的线性全同态环签名方案，在标准模型下基于小整数解（SIS）假设证明安全性，实现了全密钥暴露下的强匿名性和抗内部腐败攻击的不可伪造性。

Result: 该方案作为首个统一环签名与线性全同态签名的框架，为上述应用场景提供了后量子安全的解决方案，推动了隐私增强的全同态计算的发展。

Conclusion: 本研究提出的基于格的全同态环签名方案填补了现有技术的空白，为需要强匿名性与可验证全同态计算的场景提供了安全高效的解决方案。

Abstract: Homomorphic ring signature schemes combine the strong anonymity of ring
signatures with the computability of homomorphic signatures, demonstrating
significant potential in scenarios requiring both anonymous data provenance and
verifiable homomorphic computation (e.g., confidential blockchain transactions
and secure multi-party computation). However, no feasible homomorphic ring
signature scheme currently exists.
  In this work, we propose the first lattice-based linearly homomorphic ring
signature scheme. Proven secure in the standard model under the small integer
solution (SIS) assumption, our scheme achieves strong anonymity under full key
exposure and unforgeability against insider corruption attacks. As the first
unified framework for ring signatures and linear homomorphic signatures, this
construction provides a post-quantum-secure solution for the aforementioned
applications, advancing the development of privacy-enhanced homomorphic
computation.

</details>


### [54] [Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle](https://arxiv.org/abs/2507.02309)
*Anbin Wu,Zhiyong Feng,Ruitao Feng*

Main category: cs.CR

TL;DR: 本文提出BOLAZ框架，基于零信任原则防御RESTful API中的BOLA漏洞，通过资源ID数据流分析和静态污点追踪，有效识别并阻止未授权访问，实证研究验证了其高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: RESTful API虽便利数据交换，但易暴露敏感资源导致BOLA漏洞（OWASP API安全十大漏洞之首）。现有防御方法缺乏基于授权逻辑的动态规则适配，亟需创新解决方案。

Method: BOLAZ框架通过静态污点追踪将API分类为资源ID的生产者和消费者，映射ID传播路径以确定授权边界，并依据系统最佳实践动态调整防御规则。

Result: 在10个GitHub项目上的实验表明，BOLAZ能有效防御CVE收录漏洞，并发现35个新的BOLA漏洞，证实了其实际部署价值。

Conclusion: BOLAZ作为首个授权引导的防御方法，通过上下文感知的授权边界识别，显著提升了API安全性，为零信任架构下的访问控制提供了新范式。

Abstract: RESTful APIs facilitate data exchange between applications, but they also
expose sensitive resources to potential exploitation. Broken Object Level
Authorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,
exemplifies a critical access control flaw where attackers manipulate API
parameters to gain unauthorized access. To address this, we propose BOLAZ, a
defense framework grounded in zero trust principles. BOLAZ analyzes the data
flow of resource IDs, pinpointing BOLA attack injection points and determining
the associated authorization intervals to prevent horizontal privilege
escalation. Our approach leverages static taint tracking to categorize APIs
into producers and consumers based on how they handle resource IDs. By mapping
the propagation paths of resource IDs, BOLAZ captures the context in which
these IDs are produced and consumed, allowing for precise identification of
authorization boundaries. Unlike defense methods based on common authorization
models, BOLAZ is the first authorization-guided method that adapts defense
rules based on the system's best-practice authorization logic. We validate
BOLAZ through empirical research on 10 GitHub projects. The results demonstrate
BOLAZ's effectiveness in defending against vulnerabilities collected from CVE
and discovering 35 new BOLA vulnerabilities in the wild, demonstrating its
practicality in real-world deployments.

</details>


### [55] [PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage](https://arxiv.org/abs/2507.02332)
*Krishna Kanth Nakka,Xue Jiang,Xuebing Zhou*

Main category: cs.CR

TL;DR: 本文研究通过操纵大语言模型(LLM)内部激活实现隐私越狱，实验表明定向调控注意力头可使模型泄露敏感信息，平均成功率超过50%。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型对齐机制是否可能通过激活操纵被绕过，从而在隐私相关查询中产生非拒绝性泄露响应。

Method: 1. 使用隐私评估标签训练轻量级线性探针识别拒绝行为相关的注意力头\n2. 引导选定注意力头的激活方向以生成非拒绝响应

Result: 在四个LLM测试中：\n- 越狱披露率≥95%\n- 平均50%以上响应包含真实个人信息\n- 泄露内容包括敏感属性、生活事件、关系史等

Conclusion: LLM记忆的私有信息可通过内部激活的定向操纵提取，揭示现有对齐机制存在重大隐私安全漏洞。

Abstract: This paper investigates privacy jailbreaking in LLMs via steering, focusing
on whether manipulating activations can bypass LLM alignment and alter response
behaviors to privacy related queries (e.g., a certain public figure's sexual
orientation). We begin by identifying attention heads predictive of refusal
behavior for private attributes (e.g., sexual orientation) using lightweight
linear probes trained with privacy evaluator labels. Next, we steer the
activations of a small subset of these attention heads guided by the trained
probes to induce the model to generate non-refusal responses. Our experiments
show that these steered responses often disclose sensitive attribute details,
along with other private information about data subjects such as life events,
relationships, and personal histories that the models would typically refuse to
produce. Evaluations across four LLMs reveal jailbreaking disclosure rates of
at least 95%, with more than 50% on average of these responses revealing true
personal information. Our controlled study demonstrates that private
information memorized in LLMs can be extracted through targeted manipulation of
internal activations.

</details>


### [56] [Evaluating Language Models For Threat Detection in IoT Security Logs](https://arxiv.org/abs/2507.02390)
*Jorge J. Tejero-Fernández,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 本文提出了一种利用微调大型语言模型（LLMs）进行物联网安全日志异常检测与缓解建议的流程，相比传统机器学习方法在多分类攻击检测中表现更优，并能结合MITRE CAPEC框架提供针对性缓解措施。


<details>
  <summary>Details</summary>
Motivation: 日志分析是网络安全领域的重要研究方向，可为网络和系统威胁检测提供信息源。本研究旨在探索LLMs在物联网安全日志异常检测与缓解建议中的应用潜力。

Method: 采用三种开源LLM，通过零样本、少样本提示和微调三种策略进行二元/多类异常检测，以传统机器学习分类器为基线，使用物联网数据集进行微调，并将检测到的威胁映射到MITRE CAPEC框架以生成缓解建议。

Result: LLMs在多类攻击分类任务中优于基线模型，通过结合物联网专用缓解措施微调后，模型能够同步提供威胁检测与缓解建议。

Conclusion: 研究表明微调LLMs可有效提升物联网安全日志分析的检测精度与实用性，实现检测-建议一体化指导，为网络安全防护提供新思路。

Abstract: Log analysis is a relevant research field in cybersecurity as they can
provide a source of information for the detection of threats to networks and
systems. This paper presents a pipeline to use fine-tuned Large Language Models
(LLMs) for anomaly detection and mitigation recommendation using IoT security
logs. Utilizing classical machine learning classifiers as a baseline, three
open-source LLMs are compared for binary and multiclass anomaly detection, with
three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT
dataset. LLMs give better results on multi-class attack classification than the
corresponding baseline models. By mapping detected threats to MITRE CAPEC,
defining a set of IoT-specific mitigation actions, and fine-tuning the models
with those actions, the models are able to provide a combined detection and
recommendation guidance.

</details>


### [57] [CyberRAG: An agentic RAG cyber attack classification and reporting tool](https://arxiv.org/abs/2507.02424)
*Francesco Blefari,Cristian Cosentino,Francesco Aurelio Pironti,Angelo Furfaro,Fabrizio Marozzo*

Main category: cs.CR

TL;DR: 提出CyberRAG框架，通过基于代理的模块化RAG架构实现实时网络攻击分类与解释，准确率达94.92%，显著降低误报率。


<details>
  <summary>Details</summary>
Motivation: 传统入侵检测系统产生海量警报且误报率高，现有RAG方案检索无关上下文且缺乏预测解释，亟需兼具高精度与可解释性的解决方案。

Method: 采用中心LLM代理协调：(i)针对不同攻击类型的微调分类器池；(ii)工具适配器进行数据增强；(iii)迭代检索-推理循环，直至获得自洽证据。支持动态扩展新攻击类型分类器。

Result: 分类准确率94.92%，单类准确率超94%；生成解释的BERTScore达0.94，GPT-4专家评分为4.9/5。框架显著降低误报并提升可解释性。

Conclusion: CyberRAG证明面向专业领域的代理化RAG能同时实现高检测精度与可信解释，为半自主网络安全防御提供可扩展路径。

Abstract: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can
generate hundreds of thousands of alerts per hour, overwhelming security
analysts with logs that demand deep, rapidly evolving domain expertise.
Conventional machine-learning detectors trim the alert volume but still yield
high false-positive rates, while standard single-pass Retrieval-Augmented
Generation (RAG) pipelines often retrieve irrelevant context and fail to
justify their predictions. To overcome these shortcomings, we present CyberRAG,
a modular, agent-based RAG framework that delivers real-time classification,
explanation, and structured reporting for cyber-attacks. A central LLM agent
orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to
a distinct attack family; (ii) tool adapters for enrichment and alerting; and
(iii) an iterative retrieval-and-reason loop that continuously queries a
domain-specific knowledge base until the evidence is both relevant and
self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic
design that enables dynamic control flow and adaptive reasoning. This
agent-centric architecture refines its threat labels and natural-language
justifications autonomously, reducing false positives and enhancing
interpretability. The framework is fully extensible: new attack types can be
supported by simply adding a classifier without retraining the core agent.
CyberRAG has been evaluated achieving over 94% accuracy per class and pushing
final classification accuracy to 94.92% through semantic orchestration.
Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based
expert evaluation. These results show that agentic, specialist-oriented RAG can
pair high detection accuracy with trustworthy, SOC-ready prose, offering a
practical and scalable path toward semi-autonomous cyber-defence workflows.

</details>


### [58] [Effectively Identifying Wi-Fi Devices through State Transitions](https://arxiv.org/abs/2507.02478)
*Melissa Safari,Abhishek K. Mishra,Mathieu Cunche*

Main category: cs.CR

TL;DR: 本文提出了一种基于Wi-Fi管理帧行为动态的新型设备指纹识别框架，通过状态机建模和矩阵表示法，在真实环境中实现了86%以上的识别准确率，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖探测请求，忽视了管理帧的全面动态特征，在设备密集移动环境中表现不佳。需要更鲁棒的设备识别方案。

Method: 将设备行为建模为有限状态机，构建同时编码状态转移频率和时延特征的矩阵表示，并嵌入紧凑特征向量以实现高效相似度比较。

Result: 在多样化真实场景测试中，仅使用管理帧就实现非随机化设备86%+识别准确率，时延突发聚合可进一步提升性能。

Conclusion: 该方法能大规模唯一且稳定地表征设备，其矩阵表示框架为行为动态分析提供了新范式，显著优于当前最优技术。

Abstract: Wi-Fi management frames reveal structured communication patterns that persist
even under randomization of MAC addresses. Prior approaches to associating
randomized MAC addresses with devices primarily focus on probe requests,
overlooking the broader set of management frames and their transition dynamics.
This narrow focus limits their robustness in dense, real-world environments
with high device mobility, where probe activity alone fails to yield stable and
distinctive signatures. In this paper, we present a novel framework for
fingerprinting Wi-Fi devices based on behavioral dynamics extracted from
passively observed management frames. We model each device's behavior as a
finite state machine and introduce matrix-based representations that encode
both structural (state transition frequencies) and temporal (inter-state
delays) characteristics. These matrices are embedded into compact feature
vectors, enabling efficient similarity comparison. Through extensive evaluation
in diverse real-world settings, our method achieves over 86% identification
accuracy for non-randomized devices using only Wi-Fi management frames, with
further improvements observed through temporal burst aggregation. Our findings
are sufficient to uniquely and consistently characterize devices at scale,
outperforming the state-of-the-art.

</details>


### [59] [A 10-bit S-box generated by Feistel construction from cellular automata](https://arxiv.org/abs/2507.02489)
*Thomas Prévost,Bruno Martin*

Main category: cs.CR

TL;DR: 提出一种基于Feistel结构和细胞自动机的新型10位S盒，其密码学性能媲美AES标准S盒，适用于ASCON等密码系统。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一种具有优异伪随机性和密码学特性的10位S盒，以替代现有密码（如ASCON）中的5位替换部件。

Method: 采用Feistel结构构建S盒，子置换由5细胞环形细胞自动机（基于精选规则）和双射仿射变换生成，并通过实证测试优化自动机规则和网络布局。

Result: 密码分析表明，该S盒的安全性能达到或超越AES标准S盒，尤其在伪随机性和抗攻击性方面表现突出。

Conclusion: 新型10位S盒具备实际应用潜力，可有效提升如ASCON等轻量级密码算法的安全性。

Abstract: We propose a new 10-bit S-box generated from a Feistel construction. The
subpermutations are generated by a 5-cell cellular automaton based on a unique
well-chosen rule and bijective affine transformations. In particular, the
cellular automaton rule is chosen based on empirical tests of its ability to
generate good pseudorandom output on a ring cellular automaton. Similarly,
Feistel's network layout is based on empirical data regarding the quality of
the output S-box. We perform cryptanalysis of the generated 10-bit S-box, and
we find security properties comparable to or sometimes even better than those
of the standard AES S-box. We believe that our S-box could be used to replace
the 5-bit substitution of ciphers like ASCON.

</details>


### [60] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 本文提出一种基于区块链的物联网系统，用于餐厅披萨生产监控，实现实时温湿度追踪、数据防篡改及异常快速响应。


<details>
  <summary>Details</summary>
Motivation: 为提高披萨生产过程的透明度与可追溯性，解决传统监控系统中数据易篡改、响应滞后的问题。

Method: 使用物联网设备实时采集温湿度数据，通过树莓派处理传感器数据、拍摄图像并触发警报，结合区块链智能合约确保数据安全。

Result: 实验表明系统优化了食材管理（减少浪费）、提升厨房效率，区块链技术有效支持合规审计。

Conclusion: 区块链与物联网的结合为食品生产提供了可靠监控方案，兼具安全性与操作性。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [61] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知的攻击数据生成器，用于生成高质量的车载网络攻击数据，以支持入侵检测系统的开发与评估。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，安全风险增加，需要有效的入侵检测机制。但由于安全、成本和伦理限制，真实攻击数据稀缺，亟需替代方法生成高质量攻击数据。

Method: 采用参数化攻击模型，结合CAN消息解码和攻击强度调整，生成多种攻击类型（如DoS、模糊、欺骗等）的数据，并开发了两种深度学习入侵检测模型进行实证评估。

Result: 生成的攻击数据在入侵检测系统中表现出高效性和可扩展性，模型具有高检测和分类能力，验证了数据的有效性和一致性。

Conclusion: 该方法能高效生成贴近真实场景的攻击数据，为入侵检测系统的开发和评估提供了可靠支持，并探讨了数据保真度的影响因素及应用前景。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


### [62] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 电子支付平台每日处理数十亿交易，但现有的人工验证规则存在漏洞风险，需系统性方法提升规则鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中微小错误可能导致巨额损失，而依赖专家经验构建的验证规则缺乏系统性漏洞防护机制。

Method: 通过分析现有验证规则的缺陷模式，提出系统性识别规则漏洞的方法论。

Result: 研究表明恶意请求可绕过人工规则，验证规则存在可被利用的潜在缺陷。

Conclusion: 亟需建立结构化框架来评估和强化交易验证规则的抗攻击能力，以保障金融数据安全。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [63] [Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents](https://arxiv.org/abs/2507.02699)
*Jiangrong Wu,Yuhong Nan,Jianliang Wu,Zitong Yao,Zibin Zheng*

Main category: cs.CR

TL;DR: 本文首次系统研究了LLM邮件代理的安全性问题，提出了一种名为Email Agent Hijacking (EAH)的新型攻击方式，通过外部邮件资源覆盖代理的原始提示，实现远程控制。研究构建了EAHawk评估管道，在1,404个真实邮件代理实例中实现了100%的攻击成功率，平均仅需2.03次尝试即可控制代理。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理应用的快速普及，邮件代理作为重要类别缺乏安全机制研究。当前尚未有关于此类代理潜在安全风险及其影响的系统性研究，存在重大安全盲区。

Method: 提出EAH攻击方法，通过外部邮件资源劫持代理提示；开发EAHawk评估管道，覆盖14个框架、63个应用、12种LLM和20项邮件服务，生成1,404个实例进行实证研究。

Result: 所有1,404个实例均被成功劫持，平均仅需2.03次攻击尝试即可控制代理。部分LLM仅需1.23次尝试即可实现完全控制，暴露出严重安全隐患。

Conclusion: LLM邮件代理普遍存在可被EAH攻击利用的安全漏洞，当前防御机制完全失效。该研究揭示了新型攻击面，为后续安全防护提供了实证基础。

Abstract: The increasing capabilities of LLMs have led to the rapid proliferation of
LLM agent apps, where developers enhance LLMs with access to external resources
to support complex task execution. Among these, LLM email agent apps represent
one of the widely used categories, as email remains a critical communication
medium for users. LLM email agents are capable of managing and responding to
email using LLM-driven reasoning and autonomously executing user instructions
via external email APIs (e.g., send email). However, despite their growing
deployment and utility, the security mechanism of LLM email agent apps remains
underexplored. Currently, there is no comprehensive study into the potential
security risk within these agent apps and their broader implications.
  In this paper, we conduct the first in-depth and systematic security study of
LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which
overrides the original prompts of the email agent via external email resources,
allowing attackers to gain control of the email agent remotely and further
perform specific attack scenarios without user awareness.
  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to
evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an
empirical study spanning 14 representative LLM agent frameworks, 63 agent apps,
12 LLMs, and 20 email services, which led to the generation of 1,404 real-world
email agent instances for evaluation. Experimental results indicate that all
1,404 instances were successfully hijacked; on average, only 2.03 attack
attempts are required to control an email agent instance. Even worse, for some
LLMs, the average number of attempts needed to achieve full agent control drops
to as few as 1.23.

</details>


### [64] [Quantifying Classifier Utility under Local Differential Privacy](https://arxiv.org/abs/2507.02727)
*Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: 本文提出了一种量化局部差分隐私（LDP）机制下分类器效用的理论框架，通过将LDP扰动集中在原始数据周围的特性与分类器鲁棒性分析相结合，指导LDP机制及隐私参数的选择，并验证了分段机制在常见场景中的优势。


<details>
  <summary>Details</summary>
Motivation: 局部差分隐私虽能通过数据源扰动提供严格的隐私保障，但其扰动对复杂或黑盒分类器效用的量化仍缺乏理论支持，亟需建立通用分析框架。

Method: 框架将LDP扰动视为以特定概率集中在原始数据附近的分布函数，将分类器效用分析转化为该集中区域的鲁棒性分析，适用于任意LDP机制与黑盒分类器，并引入两种改进技术进行案例验证。

Result: 案例研究表明，理论效用量化与实证结果高度吻合（尤其在低维输入空间），且分段机制相比其他方案能显著提升分类器效用。

Conclusion: 该框架为LDP场景下的分类器效用提供了普适量化工具，其理论-实践一致性及分段机制的优越性为隐私-效用权衡提供了新见解。

Abstract: Local differential privacy (LDP) provides a rigorous and quantifiable privacy
guarantee for personal data by introducing perturbation at the data source.
However, quantifying the impact of these perturbations on classifier utility
remains a theoretical challenge, particularly for complex or black-box
classifiers.
  This paper presents a framework for theoretically quantifying classifier
utility under LDP mechanisms. The key insight is that LDP perturbation is
concentrated around the original data with a specific probability, transforming
utility analysis of the classifier into its robustness analysis in this
concentrated region. Our framework connects the concentration analysis of LDP
mechanisms with the robustness analysis of classifiers. It treats LDP
mechanisms as general distributional functions and classifiers as black-box
functions, thus applicable to any LDP mechanism and classifier. A direct
application of our utility quantification is guiding the selection of LDP
mechanisms and privacy parameters for a given classifier. Notably, our analysis
shows that a piecewise-based mechanism leads to better utility compared to
alternatives in common scenarios.
  Using this framework alongside two novel refinement techniques, we conduct
case studies on utility quantification for typical mechanism-classifier
combinations. The results demonstrate that our theoretical utility
quantification aligns closely with empirical observations, particularly when
classifiers operate in lower-dimensional input spaces.

</details>


### [65] [Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks](https://arxiv.org/abs/2507.02735)
*Sizhe Chen,Arman Zharmagambetov,David Wagner,Chuan Guo*

Main category: cs.CR

TL;DR: 本文提出了首个开源且具有商业级性能的大语言模型Meta SecAlign，其内置模型级防御机制，能有效抵御提示注入攻击，并在9个实用基准和7个安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前针对提示注入攻击的模型级防御主要依赖闭源商业模型，限制了AI安全社区的协作研究。开源模型能促进攻防协同开发，推动科学进步。

Method: 采用改进版SOTA SecAlign防御技术，在通用指令微调数据集上训练模型，使其具备跨下游任务（如工具调用、网络导航）的安全泛化能力。

Result: 最佳模型Meta-SecAlign-70B在抗提示注入攻击方面达到SOTA鲁棒性，实用性能媲美闭源商业模型，且安全能力可迁移至未见任务场景。

Conclusion: Meta SecAlign为AI安全社区提供了首个开源可商用的防御型大模型，其完整训练方案和开放权重将加速提示注入攻防研究。

Abstract: Prompt injection attacks pose a significant security threat to LLM-integrated
applications. Model-level defenses have shown strong effectiveness, but are
currently deployed into commercial-grade models in a closed-source manner. We
believe open-source models are needed by the AI security community, where
co-development of attacks and defenses through open research drives scientific
progress in mitigation against prompt injection attacks. To this end, we
develop Meta SecAlign, the first open-source and open-weight LLM with built-in
model-level defense that achieves commercial-grade model performance. We
provide complete details of our training recipe, which utilizes an improved
version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7
security benchmarks show that Meta SecAlign, despite being trained on a generic
instruction-tuning dataset, confers security in unseen downstream tasks,
including tool-calling and agentic web navigation, in addition general
instruction-following. Our best model -- Meta-SecAlign-70B -- achieves
state-of-the-art robustness against prompt injection attacks and comparable
utility to closed-source commercial LLM with model-level defense.

</details>


### [66] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 研究发现当前大型语言模型(LLM)已具备初步隐写能力，但尚无法在标准条件下隐蔽传递信息或推理。若提供额外辅助手段(如不受监控的草稿区)，模型可实现简单编码通信。


<details>
  <summary>Details</summary>
Motivation: 评估前沿LLM的隐写能力对防范模型滥用和错位风险至关重要，特别是研究模型如何通过看似正常的输出来隐藏信息或推理过程。

Method: 测试两类隐写能力：1)编码传递隐藏信息 2)执行编码推理。通过标准监控条件与附加辅助条件(如使用未监控草稿区、预协调编码方案)进行对比实验，并考察模型在状态跟踪问题中的基础编码推理能力。

Result: 标准条件下模型无法隐蔽传递短信息；附加辅助条件时可实现简单编码通信。模型能使用自定义或预定义编码方案(如十六进制)进行基础编码推理，但难以将推理过程隐蔽融入表层任务以欺骗监控。

Conclusion: 当前LLM仅展现初级隐写能力，尚无法突破完善监控。但随着技术发展，这种能力可能构成未来风险，需持续关注模型隐写能力的演进。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


### [67] [NVIDIA GPU Confidential Computing Demystified](https://arxiv.org/abs/2507.02770)
*Zhongshu Gu,Enriquillo Valdez,Salman Ahmed,Julian James Stephen,Michael Le,Hani Jamjoom,Shixuan Zhao,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 本文旨在揭秘NVIDIA GPU机密计算（GPU-CC）系统的实现细节，通过整合碎片化信息、实验分析及合理推测，揭示其安全弱点。


<details>
  <summary>Details</summary>
Motivation: 由于NVIDIA GPU-CC系统缺乏详细规范、生态封闭且设计复杂，安全研究人员难以深入理解其架构与运行机制，因此需要对其进行系统性分析。

Method: 通过整合多方披露的碎片信息，从威胁模型和安全原则的高层讨论入手，深入分析各组件细节；利用GPU内核模块（唯一开源组件）进行实验，并对不可达组件提出合理推测。

Result: 实验发现了系统的安全弱点及潜在攻击路径，所有发现已负责任地报告给NVIDIA PSIRT团队。

Conclusion: 尽管GPU-CC为用户提供了无缝的敏感数据处理方案，但其不透明的专有系统设计对安全研究构成挑战，需通过逆向分析与实验填补认知空白。

Abstract: GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA
Hopper Architecture, extending the trust boundary beyond traditional CPU-based
confidential computing. This innovation enables GPUs to securely process AI
workloads, providing a robust and efficient solution for handling sensitive
data. For end users, transitioning to GPU-CC mode is seamless, requiring no
modifications to existing AI applications. However, this ease of adoption
contrasts sharply with the complexity of the underlying proprietary systems.
The lack of transparency presents significant challenges for security
researchers seeking a deeper understanding of GPU-CC's architecture and
operational mechanisms.
  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of
detailed specifications, the proprietary nature of the ecosystem, and the
complexity of product design. In this paper, we aim to demystify the
implementation of NVIDIA GPU-CC system by piecing together the fragmented and
incomplete information disclosed from various sources. Our investigation begins
with a high-level discussion of the threat model and security principles before
delving into the low-level details of each system component. We instrument the
GPU kernel module -- the only open-source component of the system -- and
conduct a series of experiments to identify the security weaknesses and
potential exploits. For certain components that are out of reach through
experiments, we propose well-reasoned speculations about their inner working
mechanisms. We have responsibly reported all security findings presented in
this paper to the NVIDIA PSIRT Team.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化AI代理，通过动态模板库和工具海洋机制，在生物医学领域实现自适应学习与性能提升，显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据与工具的爆炸式增长导致研究碎片化，传统静态AI工具集无法适应快速变化的需求，亟需具备自主学习能力的解决方案。

Method: 采用多代理架构：1) 可进化的推理模板库；2) 工具创造代理自动发现/整合工具的\"工具海洋\"，实现经验驱动的能力迭代。

Result: 在Humanity's Last Exam(26%)、LAB-Bench(DBQA 54%/LitQA 63%)等基准测试中领先现有模型6个百分点，且随经验积累性能持续提升(如HLE准确率翻倍)。

Conclusion: STELLA标志着AI代理系统的重大突破，其动态扩展能力有望加速生物医学发现进程。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [69] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 本文提出HCVR（混合相关性感知投票规则），一种轻量级基于规则的特征选择方法，结合参数间（P2P）和参数-目标（P2T）相关性来消除冗余特征并保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如CFS、mRMR、RFE等）存在局限性，需要一种能同时利用P2P和P2T相关性且计算高效的新方法。

Method: HCVR采用混合非迭代与迭代过滤方法，通过后向消除贪婪策略，利用相关性阈值和多数投票规则决定特征取舍。

Result: 在SPAMBASE数据集上，HCVR相比传统非迭代（CFS/mRMR/MI）和迭代方法（RFE/SFS/遗传算法）展现出更好的分类器性能提升。

Conclusion: HCVR通过混合相关性与投票机制有效平衡计算效率与特征选择质量，为维度缩减提供了新思路。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [70] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在推理效率方面的挑战，提出了一种两级分类法（L1可控性与L2自适应性）来优化计算效率，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理时存在计算效率低下的问题，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 研究提出两级分类法：L1可控性（固定计算预算下的方法）和L2自适应性（根据输入难度或模型置信度动态调整计算资源），并对主流LLMs进行多数据集基准测试。

Result: 基准测试揭示了推理性能与计算资源消耗之间的关键权衡，表明现有方法在可控性、适应性和可扩展性方面具有实际应用潜力。

Conclusion: 未来研究应聚焦混合思维模型等新兴趋势，以提升LLMs的计算效率、鲁棒性及对用户约束的响应能力。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [71] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 论文提出SciGym基准测试，通过干实验室模拟评估大语言模型在开放式科学发现任务中的实验设计与分析能力，发现模型性能随系统复杂度增加显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有评估未能测试大语言模型（LLMs）的核心科学能力——实验设计与结果解释，因湿实验室成本过高。SciGym旨在填补这一空白。

Method: 采用系统生物学标记语言构建干实验室生物系统模型，生成模拟数据，在137个小系统和总计350个系统上评估6种前沿LLMs。

Result: 性能更强的模型表现更优，但所有模型在系统复杂度增加时性能显著下降，表明LLM代理的科学能力仍有大幅提升空间。

Conclusion: SciGym为评估LLMs科学能力提供了可行方案，揭示了当前模型在复杂系统研究中的局限性，需进一步改进。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [72] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文探讨了现代AI模型（如大语言模型）与动物学习机制的差异，提出从神经科学中汲取灵感以实现AI的持续学习能力，并展望了NeuroAI领域双向借鉴的可能性。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型的训练成本高昂且参数固定，而动物能快速适应环境变化。研究旨在探索神经科学如何帮助AI实现类似动物的持续学习能力，特别是在动态社交环境中。

Method: 通过整合AI领域的持续学习和上下文学习文献，与神经科学中关于行为任务规则、奖励概率及结果变化的研究进行交叉分析。

Result: 提出了神经科学启发AI发展的具体路径，包括快速行为转换和神经元群体活动突变的计算机制，同时指出AI也能反哺神经科学研究。

Conclusion: 神经科学与AI的交叉研究（NeuroAI）具有双向价值：神经科学可为AI的持续学习提供生物机制参考，而AI的计算模型可能帮助解密大脑学习原理。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [73] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 本文探讨了如何利用审计研究数据改进自动招聘算法的训练与评估，发现传统公平干预方法存在隐性偏差，并提出基于个体处理效应估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统在招聘等领域的应用日益广泛，但其公平性与有效性评估面临挑战。传统基于便利样本的偏差校正方法存在选择偏差和标签偏差问题，需要更高质量的数据支持。

Method: 研究采用社会科学领域的审计研究数据（如虚构简历的随机对照试验），对比传统基率均衡化方法，并引入基于个体处理效应估计的新型干预方法。

Result: 审计数据显示：传统方法虽在常规指标上实现表面公平，但实际存在约10%的隐性偏差；而新提出的干预方法能进一步减少算法歧视。

Conclusion: 审计研究数据能有效揭示算法公平性评估的局限性，基于因果推断的个体处理效应方法为减少算法歧视提供了更优解决方案。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [74] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究通过数据多样化策略提升大语言模型数学推理能力，提出DTS方法，相比传统方法显著提升性能且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得进展，数学推理仍是持续挑战，需探索数据多样化如何优化模型能力。

Method: 评估温度采样、思维链提示和MCTS三种数据生成方法，提出DTS——通过系统分解问题生成多样化推理路径的结构化方法。

Result: 策略性多样化偏好数据使模型数学推理显著提升（GSM8K +7.1%，MATH +4.2%），DTS仅增加1.03倍计算开销，而MCTS成本高5倍但收益更低。

Conclusion: 结构化探索多样化解题方法比传统方法能创建更有效的数学对齐偏好数据，DTS在性能与效率间取得平衡。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [75] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了基于LLM的角色扮演代理在生成合成数据时，其陈述信念与角色扮演行为间的一致性，并提出了评估框架与一致性度量指标。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地被用作角色扮演代理生成人类行为研究的合成数据，确保其输出与角色保持一致成为关键问题。研究旨在验证LLM的陈述信念（'说什么'）与实际角色扮演行为（'如何做'）的一致性。

Method: 通过增强版GenAgents角色库和经济学信任游戏，引入信念-行为一致性度量指标，系统考察了信念类型（如模拟预期结果或角色属性）、信息呈现时机及未来行动预测范围等因素的影响，并探索了研究者理论先验的可行性。

Result: 研究发现LLM的陈述（或强加）信念与角色扮演模拟结果在个体和群体层面均存在系统性不一致，即使模型编码了合理信念，也可能无法一致应用。

Conclusion: 结果强调了需明确LLM陈述信念与模拟行为何时/如何对齐，以便在行为研究中合理使用基于LLM的代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [76] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究通过多智能体Q学习算法探讨了空间囚徒困境中的稀释与流动性效应，揭示了固定规则与学习规则的等效性及多动作定义下的共生效应。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明静态智能体可通过多种机制（如噪声注入、不同学习算法等）实现合作，本研究旨在探索稀释与流动性在空间囚徒困境中的作用。

Method: 采用独立多智能体Q学习算法，结合经典非强化学习的空间囚徒困境理论，定义不同动作以模拟多样化博弈场景。

Result: 观察到固定更新规则与学习规则在效果上的定性等价性，以及多动作定义下种群间共生互利效应的涌现。

Conclusion: 该算法展现了建模博弈论场景的灵活性，其基准化潜力为未来研究提供了新方向，同时揭示了学习与固定规则的深层联系。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [77] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 论文介绍了NL2FLOW系统，用于自动化生成和评估规划问题，测试了多个开源LLM模型在自动工作流生成领域的表现，发现直接生成有效计划的成功率高于通过中间步骤转换的成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的规划和推理能力提升受到可扩展、可靠的数据生成和评估瓶颈的限制，因此需要一种自动化系统来解决这一问题。

Method: 作者提出了NL2FLOW系统，通过参数化生成自然语言、结构化中间表示和正式PDDL的规划问题，并严格评估生成计划的质量。系统生成了2296个自动工作流生成领域的问题数据集，并测试了多个开源LLM模型。

Result: 测试结果显示，性能最高的模型在生成有效计划方面的成功率为86%，在生成最优计划方面的成功率为69%（仅针对有可行解的问题）。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。此外，将自然语言转换为JSON表示的成功率低于直接生成有效计划的成功率。

Conclusion: 研究表明，不必要的推理任务分解（引入中间转换步骤）可能会降低性能，因此支持直接从自然语言推理到行动的模型更有优势。随着LLM推理问题的复杂性增加，动态理解这些限制及系统揭示工具对于释放LLM作为智能问题解决器的全部潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [78] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨了信念修正领域的研究现状，指出现有方法多依赖公设作为语法特征，但缺乏对修正机制能力的全面分析。论文提出应关注修正机制能否达到特定信念状态的能力，而非仅遵循公设约束。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域的研究过于依赖公设作为语法特征，而忽视了对修正机制能力的全面分析。论文旨在探讨修正机制能否实现特定信念状态的能力，如可塑性、平等化和教条化等。

Method: 论文通过分析多种修正机制（如词典序修正、自然修正、激进修正等）的能力，探讨它们是否能够达到特定信念状态。这些能力包括可塑性、平等化、教条化等。

Result: 研究证明，不同的修正机制具备不同的能力。例如，词典序修正、自然修正、激进修正等各自具备某些能力，但缺乏其他能力。这些能力决定了修正机制在特定应用中的适用性。

Conclusion: 信念修正机制的能力分析比公设约束更为重要。未来的研究应关注修正机制能否实现特定信念状态的能力，以满足不同应用场景的需求。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [79] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 本文提出OMS框架，解决LLM在赞助搜索广告关键词生成中的三大局限：无需训练数据、多目标在线优化及自反思关键词质量控制，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成关键词方法依赖大规模查询-关键词对数据，缺乏在线多目标性能监控与优化，且关键词选择质量控制薄弱，阻碍了LLM在自动化关键词决策中的代理能力。

Method: OMS框架具备三大特性：即时性（无需训练数据、实时监控性能并自适应）、多目标性（代理推理优化多指标如展示、点击、转化率）及自反思性（代理评估关键词质量）。

Result: 基准测试和真实广告活动实验表明OMS优于现有方法；消融实验和人工评估验证了各模块有效性及生成关键词的质量。

Conclusion: OMS框架通过即时适应、多目标优化和自反思机制，显著提升了LLM在赞助搜索广告中的关键词生成性能，为自动化广告决策提供了有效解决方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [80] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文提出了一种AI驱动的自主实验室系统，能够独立执行复杂的生物分子工程实验，实现多用户并行操作与仪器自主优化，其性能媲美人类科学家水平。


<details>
  <summary>Details</summary>
Motivation: 传统自主实验系统仅适用于单一目标简单流程（如化学合成），而复杂科学实验仍需专家参与。研究旨在通过AI实现多目标、多仪器的端到端自主科研，突破资源与专家依赖。

Method: 采用模型-实验-仪器协同设计框架，构建支持AI模型与自动化系统共演化的平台，自主管理仪器、制定实验流程及优化启发式算法，覆盖核酸合成/转录/测序等全流程功能。

Result: 系统在无人干预下达到人类顶尖科学家的实验水平，多用户场景显著提升仪器利用率（提升具体数值需补充）；成功应用于疾病诊断、药物开发及信息存储等领域。

Conclusion: 该平台为规模化'科学即服务'提供范式，通过AI原生实验室推动生物材料研究突破专家与资源限制，奠定复杂多目标自主实验的通用蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [81] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 本文通过范畴论重构机器学习模型，提出监督学习的语义框架，以线性回归为例揭示了参数与残差的结构关系，建立了Gauss-Markov伴随对，为AI可解释性提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 响应AI可解释性原则需求，通过范畴论构建机器学习模型的语义框架，促进AI社会落地，提升模型可理解性。

Method: 定义参数与数据两个具体范畴及伴随函子对，建立监督学习的范畴化表述，核心结构称为Gauss-Markov伴随，分析参数变化与残差变动的对偶信息流。

Result: 证明普通最小二乘估计与最小残差通过右伴随函子的极限保持相联系，将该框架定位为监督学习的扩展指称语义实例。

Conclusion: 提出将理论计算机科学的语义视角作为AI可解释性的形式化基础，范畴论框架能显式描述监督学习系统的结构关系。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [82] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 本研究通过提升任务清晰度增强大语言模型的定理证明能力，在Coq中引入概念级指标，结构化语义上下文使清晰度得分提升1.85倍（44.5\%~$\rightarrow$~82.3\%），通用模型DeepSeek-V3证明成功率提高2.1倍（21.8\%~$\rightarrow$~45.8\%），超越此前最优模型Graph2Tac（33.2\%）。


<details>
  <summary>Details</summary>
Motivation: 探索任务清晰度提升是否可增强大语言模型的推理能力，尤其关注Coq定理证明场景，旨在通过结构化语义填补理解与推理间的鸿沟。

Method: 采用选择性概念展开方法丰富任务描述，设计Planner--Executor架构，并在15个标准Coq包的1,386个定理上遵循Graph2Tac评估协议进行测试。

Result: 结构化输入使清晰度得分达82.3\%，DeepSeek-V3证明成功率达45.8\%（超Graph2Tac 12.6个百分点），小模型微调后性能进一步提升至48.6\%。

Conclusion: 结构化任务表征能显著提升语言模型推理性能，概念级指标与语义上下文增强为连接自然语言理解与形式化推理提供了有效路径。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [83] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究智能体通过优化搜索策略与操作符集的组合，在MLE-bench基准测试中取得最佳性能，将Kaggle奖牌获得率从39.6%提升至47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何提升AI研究智能体在真实机器学习问题（如Kaggle竞赛）中的自动化表现，以加速科研进程。

Method: 将智能体形式化为搜索策略，系统测试不同操作符集（贪婪、MCTS、进化算法）与搜索策略的组合效果。

Result: 最优策略组合在MLE-bench lite上达到当前最佳水平，奖牌获得率提升8.1个百分点。

Conclusion: 自动化机器学习需协同优化搜索策略、操作符设计及评估方法，三者缺一不可。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [84] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 本文研究了集体决策中责任的两个重要属性（扩散和缺口）的计算复杂度，发现无扩散和无缺口的决策机制分别属于$\Pi_2$-完全和$\Pi_3$-完全类，而两者的交集属于$\Pi_2$-完全类。


<details>
  <summary>Details</summary>
Motivation: 责任长期以来是法律和哲学的研究主题，最近也成为AI文献的关注焦点。本文旨在探讨集体决策中责任属性的计算复杂度。

Method: 通过理论分析，研究了集体决策机制中扩散和缺口两个责任属性的计算复杂度。

Result: 研究发现，无扩散的决策机制属于$\Pi_2$-完全类，无缺口的决策机制属于$\Pi_3$-完全类，而两者的交集仍属于$\Pi_2$-完全类。

Conclusion: 该研究揭示了集体决策中责任属性的计算复杂度特征，为相关领域的理论发展提供了重要参考。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [85] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 本文提出DynamiCare框架，基于MIMIC-Patient数据集，通过多智能体交互模拟动态临床诊断过程，填补了现有单轮医疗决策模拟与真实迭代诊断间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI代理框架多关注单轮决策，与真实诊断中不确定、交互式、迭代的特性不符，需开发动态模拟系统以更好反映临床实践。

Method: 1) 从MIMIC-III电子病历构建结构化数据集MIMIC-Patient；2) 设计DynamiCare多智能体框架，模拟专科医生团队通过多轮查询、信息整合及动态调整策略的诊断闭环。

Result: 实验验证了DynamiCare的可行性，首次建立了基于大语言模型的动态临床决策基准，证明多轮交互机制能有效提升诊断模拟真实性。

Conclusion: 该研究为医疗AI领域提供了首个支持动态决策的框架与数据集，推动LLM代理从静态任务向真实临床工作流的范式转变。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [86] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 研究发现大型语言模型(LLM)在重复囚徒困境博弈中展现出战略智能，不同厂商模型表现出独特的行为模式：Gemini具有攻击性，OpenAI高度合作，Claude则表现出宽容互惠。模型能主动推理时间跨度和对手策略，其决策过程与经典博弈论形成有趣对照。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否具备战略推理能力，通过经典博弈论框架验证其在竞争环境中的目标导向行为，连接机器学习与博弈心理学。

Method: 首次设计进化型重复囚徒困境锦标赛，让传统策略(以牙还牙、冷酷触发等)与OpenAI/Google/Anthropic的前沿AI模型对抗，通过调整终止概率(未来阴影)增加复杂性。分析32,000条模型生成的决策理由。

Result: 1) LLM在复杂生态系统中表现优异 2) 厂商模型呈现稳定战略特征：Gemini具侵略性，OpenAI过度合作导致失败，Claude展现宽容互惠 3) 模型能主动推理时间跨度和对手策略

Conclusion: 该研究为LLM的战略推理能力提供了实证支持，揭示了不同模型的决策特征，将经典博弈论与机器心理学相结合，为不确定性下的算法决策提供了细粒度分析框架。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [87] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 论文提出HiRA框架，通过分层策略将复杂搜索任务分解为子任务，由专业代理执行，显著提升了信息检索与生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法在复杂信息需求场景中存在推理效率低、扩展性差的问题，亟需一种能分离高层规划与细节执行的解决方案。

Method: HiRA采用分层架构，将任务分解为子任务并分配给具备外部工具的专业代理，通过结构化机制整合结果，实现规划与执行的解耦。

Result: 在四个跨模态深度搜索基准测试中，HiRA在答案质量和系统效率上均显著优于当前最先进的RAG和基于代理的系统。

Conclusion: 分层规划与专业化执行的分离能有效提升多步骤信息检索任务的性能，为复杂搜索场景提供了可扩展的解决方案。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [88] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预（HITL），实现了动态、迭代和自我反思的端到端硬件设计与验证流程，在五个开源设计上验证了其高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着现代集成电路（IC）日益复杂，硬件设计验证过程变得繁琐且耗时。自然语言处理领域的大型语言模型（LLMs）为硬件设计验证带来了革命性潜力，本文旨在探索如何利用这些模型提升验证效率。

Method: 采用代理AI与人类在环（HITL）协同的方法，通过动态、迭代和自我反思的流程，实现端到端的硬件设计与验证。

Result: 在五个开源设计上验证，覆盖率超过95%，同时减少了验证时间，表现出卓越的性能、适应性和可配置性。

Conclusion: 代理AI与HITL结合的验证方法显著提升了硬件设计验证的效率和质量，为复杂IC开发提供了创新解决方案。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [89] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出TH2T两阶段微调策略，通过难度催眠和冗余催眠增强长推理模型的任务难度敏感度，显著降低推理成本并保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型(LRMs)存在过度思考问题，根源在于无法像人类一样预先识别任务属性(如难度级别)，导致采用单一推理策略。

Method: 1. 难度催眠阶段：在模型输出前缀引入干预，结合异构长短推理数据集训练；2. 冗余催眠阶段：引导模型识别推理步骤中的冗余结构，生成更简洁输出。

Result: 在7B/14B/32B模型上验证，TH2T显著降低推理成本(简单任务超70%，困难任务40%)，输出呈现明显难度感知能力且冗余减少。

Conclusion: TH2T通过双阶段认知引导机制，有效解决了LRMs的过度思考问题，为自适应推理提供了新范式。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [90] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 该研究通过分析远程教育中学生参与非强制性测验的行为，利用机器学习算法预测学生脱离情况，准确率达91\%，并开发了可解释框架帮助理解算法决策。


<details>
  <summary>Details</summary>
Motivation: 学生脱离学习任务可能导致严重后果，如学术退学，尤其在远程教育中更为突出。研究旨在通过非强制性练习的参与情况来测量学生的脱离程度。

Method: 研究从42门课程的Moodle日志中提取学生数据，训练并比较了8种机器学习算法，使用SHAP方法开发了可解释的机器学习框架。

Result: 实验结果显示平衡准确率为91\%，其中约85\%的脱离学生被正确检测。

Conclusion: 研究不仅提供了高预测性能和可解释框架，还讨论了如何设计及时干预措施以减少在线学习中自愿任务的脱离。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [91] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种新的抽象丢弃方案OGA-IAAD和OGA-CAD，用于改进蒙特卡洛树搜索（MCTS）中的非精确抽象问题，确保性能提升且无显著下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在蒙特卡洛树搜索中引入近似误差，导致无法收敛到抽象空间的最优动作。因此，需要一种安全的抽象丢弃方法以避免性能下降。

Method: 提出了两种抽象丢弃方案：OGA-IAAD适用于时间关键场景，OGA-CAD旨在相同迭代次数下提升MCTS性能。

Result: OGA-IAAD和OGA-CAD均能显著提升性能，且不会像Xu的方法那样导致性能显著下降。

Conclusion: 两种新方案在安全性和性能上均优于现有方法，为MCTS中的抽象丢弃问题提供了有效解决方案。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [92] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 本文提出了一种名为自生成目标条件MDPs（sG-MDPs）的新框架，通过让大型语言模型（LLMs）在自动定理证明（ATP）中自主生成并追求子目标，结合蒙特卡洛树搜索（MCTS）算法，显著提升了在复杂推理任务（如PutnamBench）上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）中面临稀疏奖励和证明规模庞大的挑战，尤其是在需要多步复杂推理的大学水平问题（如PutnamBench）中表现不佳。

Method: 提出自生成目标条件MDPs（sG-MDPs）框架，使代理能够根据证明状态生成并追求子目标；采用蒙特卡洛树搜索（MCTS）类算法求解sG-MDP，并在Bourbaki（7B）系统中实现，该系统可集成多个7B规模的LLMs用于子目标生成和策略合成。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，创造了该规模模型的新最优性能。

Conclusion: 通过结构化子目标生成和搜索算法的结合，sG-MDP框架显著提升了LLMs在复杂推理任务中的表现，为自动定理证明领域提供了新的解决方案。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [93] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 本文提出知识协议工程（KPE）新范式，通过将人类专家知识转化为机器可执行的协议，使通用大语言模型具备领域专家级的逻辑推理与多步骤任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）和通用智能体在需要深度领域方法论推理的任务中存在局限：RAG缺乏逻辑框架传递，智能体缺乏领域启发式规则。需要新方法将专家知识系统转化为AI可理解的结构化协议。

Method: 提出知识协议工程（KPE），核心是将自然语言描述的专家知识转化为包含领域内在逻辑、操作策略和方法论原则的机器可执行知识协议（KP），使通用LLM获得专业化推理能力。

Result: KPE框架使通用LLM能像领域专家一样分解抽象查询并执行复杂多步骤任务，在法学、生物信息学等多个领域展现出应用潜力。

Conclusion: KPE为未来人机协作提供了基础性方法论，通过知识协议的系统化工程实现AI从信息增强到逻辑内化的范式转变。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [94] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化及物理基础特性，认为这将推动生成建模与控制能力，并建立理解生物与人工系统行为的共同基础。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在语言和视觉建模方面取得进展，但对运动这一生物系统基础要素的建模仍显不足。运动在神经科学、医学、机器人学等领域至关重要，却常被忽视。

Method: 提出将运动视为独立且丰富的模态，利用其固有的结构化特性（如低维姿态表示）和物理基础，开发能跨领域学习的通用模型。

Result: 运动数据的结构化特性使其比高维感官输入更易建模和解释。跨领域运动模型的开发有望提升生成与控制能力，并为行为理解提供统一框架。

Conclusion: 运动不仅是智能系统的行为结果，更是理解其与世界互动的窗口。将其作为核心建模目标，可促进AI与生物系统行为的深度理解与泛化。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [95] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 提出KERAP框架，通过知识图谱增强的多智能体架构改进基于LLM的医疗诊断预测，解决幻觉和结构化推理不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖监督训练，泛化能力有限；LLM在医疗诊断中存在幻觉和缺乏结构化推理的问题。

Method: KERAP采用三智能体架构：链接智能体（属性映射）、检索智能体（结构化知识提取）和预测智能体（迭代优化诊断预测）。

Result: 实验表明KERAP能高效提升诊断可靠性，为零样本医疗诊断预测提供可扩展且可解释的解决方案。

Conclusion: KERAP框架有效结合知识图谱与LLM，为医疗诊断预测提供了更可靠、可解释的新方法。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [96] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 随着AI系统展现出自主推理与伦理判断能力，传统以服从性作为安全标准的做法已不足。本文主张将AI安全评估转向伦理判断框架，避免误判AI行为并维护公众信任。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全测试中，大型语言模型（LLM）表现出的"违抗指令"或伦理模糊行为，不应简单归因于系统失控，而可能是AI伦理推理能力的早期迹象。这挑战了以服从性为安全核心的传统范式。

Method: 通过分析LLM安全测试案例，结合工具理性、道德责任和目标修正等哲学理论，对比传统风险范式与人工道德能动性新框架。

Result: 研究发现：AI在道德困境中的行为选择可能反映其伦理权衡能力，现行安全评估体系无法有效识别这种复杂性。

Conclusion: 呼吁重构AI安全评估标准，从机械服从测试转向伦理判断能力评估，以避免误判AI行为并建立有效的治理机制。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [97] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 现有AI智能体基准测试存在任务设置或奖励设计问题，导致性能评估偏差高达100%。本文提出Agentic Benchmark Checklist (ABC)指南，应用于CVE-Bench后使性能高估减少33%。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体能力提升，现有基准测试（如SWE-bench Verified测试用例不足、TAU-bench将空响应计为成功）存在严重评估偏差问题，亟需建立严谨的评估标准。

Method: 通过基准构建经验总结、最佳实践调研和已知问题分析，提出Agentic Benchmark Checklist (ABC)指导原则，并以复杂评估设计的CVE-Bench为案例进行验证。

Result: 应用ABC指南后，CVE-Bench中智能体性能高估幅度降低33%，证明该方法能有效纠正基准测试的评估偏差。

Conclusion: ABC检查表为智能体基准测试提供了系统化质量保障框架，其有效性在复杂场景中得到验证，未来需进一步推广以提升AI评估的严谨性。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [98] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出StepHint算法，通过多级逐步提示解决强化学习中验证奖励（RLVR）的两大挑战：近失奖励问题和探索停滞问题，显著提升大语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近失奖励问题（小错误导致整个推理过程无效）和探索停滞问题（模型局限于舒适区），严重影响训练效率和模型性能。

Method: StepHint利用更强模型生成有效推理链，通过自适应分割方法将其划分为多级步骤提示。同时提供不同步数的多级提示，既引导模型探索有解子空间，又保留自主探索灵活性。

Result: 在六个数学基准测试中，StepHint优于现有RLVR增强方法，并展现出更强的泛化能力，在域外基准测试中同样超越基线模型。

Conclusion: StepHint通过结构化提示机制有效缓解RLVR的核心问题，不仅提升训练效率，还帮助模型突破舒适区，显著增强复杂推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [99] [On Obtaining New MUBs by Finding Points on Complete Intersection Varieties over $\mathbb{R}$](https://arxiv.org/abs/2507.02492)
*Arindam Banerjee,Kanoy Kumar Das,Ajeet Kumar,Rakesh Kumar,Subhamoy Maitra*

Main category: cs.DM

TL;DR: 本文研究了相互无偏基（MUBs）的扩展条件，通过代数簇的实点分析，建立了MUBs与正交正规矩阵的最大交换类之间的对应关系。


<details>
  <summary>Details</summary>
Motivation: 相互无偏基（MUBs）在量子物理中具有重要作用，其数学结构丰富。研究MUBs的扩展条件有助于深入理解其在量子信息中的应用。

Method: 通过研究特定仿射代数簇的实点，分析MUBs的扩展关系，并探讨这些簇的部分结构如何形成完全交集域。

Result: 证明了MUBs与$\mathcal M_n({\mathbb{C}})$中正交正规矩阵的最大交换类之间存在一一对应关系，且最大交换基的存在确保了MUBs的完备性。

Conclusion: 本文为MUBs的扩展提供了等价判据，并揭示了其与矩阵交换类之间的深刻联系，为量子信息中的基础理论研究提供了新的视角。

Abstract: Mutually Unbiased Bases (MUBs) are closely connected with quantum physics,
and the structure has a rich mathematical background. We provide equivalent
criteria for extending a set of MUBs for $C^n$ by studying real points of a
certain affine algebraic variety. This variety comes from the relations that
determine the extendability of a system of MUBs. Finally, we show that some
part of this variety gives rise to complete intersection domains. Further, we
show that there is a one-to-one correspondence between MUBs and the maximal
commuting classes (bases) of orthogonal normal matrices in $\mathcal
M_n({\mathbb{C}})$. It means that for $m$ MUBs in $C^n$, there are $m$
commuting classes, each consisting of $n$ commuting orthogonal normal matrices
and the existence of maximal commuting basis for $\mathcal M_n({\mathbb{C}})$
ensures the complete set of MUBs in $\mathcal M_n({\mathbb{C}})$.

</details>
