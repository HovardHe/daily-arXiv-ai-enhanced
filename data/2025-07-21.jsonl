{"id": "2507.13821", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.13821", "abs": "https://arxiv.org/abs/2507.13821", "authors": ["Cyriac Antony", "Jacob Antony"], "title": "Some short notes on oriented line graphs and related matrices", "comment": null, "summary": "The notion of oriented line graphs is introduced by Kotani and Sunada, and\nthey are closely related to Hashimato's non-backtracking matrix. It is known\nthat for regular graphs $G$, the eigenvalues of the adjacency matrix of the\noriented line graph $\\vec{L}(G)$ of $G$ are the reciprocals of the poles of the\nIhara zeta function of $G$. We determine the characteristic polynomial of the\nadjacency matrix of the underlying undirected graph of $\\vec{L}(G)$ and the\nskew-symmetric adjacency matrix of $\\vec{L}(G)$ for $d$-regular graphs $G$ with\n$d\\geq 3$. We also exhibit a consequence of this result to star coloring of\nregular graphs."}
{"id": "2507.13656", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.13656", "abs": "https://arxiv.org/abs/2507.13656", "authors": ["Konstantinos Zografos"], "title": "Bounds of Shannon entropy and Extropy and their application in exploring the extreme value behavior of a large set of data", "comment": null, "summary": "This paper derives bounds for two omnipresent information theoretic measures,\nthe Shannon entropy and its complementary dual, the extropy. Based on a large\nsize data set from a logconcave model, the said bounds are obtained for the\nentropy and the extropy of the distribution of the largest order statistic and\nthe respective normalized sequence, in the extreme value theory setting. A\ncharacterization of the exponential distribution is provided as the model that\nmaximizes the Shannon entropy and the extropy which are associated with the\ndistribution of the maximum value, in a large sample size regime. This\ncharacterization is exploited to provide an alternative, immediate proof of the\nconvergence of Shannon entropy and extropy of the normalized maxima of a large\nsize sample to the respective measures for the Gumbel distribution, studied\nrecently for Shannon entropy in Johnson (2024) and references therein."}
{"id": "2507.13764", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.13764", "abs": "https://arxiv.org/abs/2507.13764", "authors": ["Guanfu Liu", "Pengfei Li", "Yukun Liu", "Xiaolong Pu"], "title": "On consistency of the MLE under finite mixtures of location-scale distributions with a structural parameter", "comment": null, "summary": "We provide a general and rigorous proof for the strong consistency of maximum\nlikelihood estimators of the cumulative distribution function of the mixing\ndistribution and structural parameter under finite mixtures of location-scale\ndistributions with a structural parameter. The consistency results do not\nrequire the parameter space of location and scale to be compact. We illustrate\nthe results by applying them to finite mixtures of location-scale distributions\nwith the component density function being one of the commonly used density\nfunctions: normal, logistic, extreme-value, or $t$. An extension of the strong\nconsistency results to finite mixtures of multivariate elliptical distributions\nis also discussed."}
{"id": "2507.13746", "categories": ["math.LO", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13746", "abs": "https://arxiv.org/abs/2507.13746", "authors": ["Jim de Groot"], "title": "Intuitionistic monotone modal logic via translation", "comment": null, "summary": "We introduce a monotone modal analogue of the intuitionistic (normal) modal\nlogic IK using a translation into a suitable (intuitionistic) first-order\nlogic. We axiomatise the logic and give a semantics by means of intuitionistic\nneighbourhood models, which contain neighbourhoods whose value can change when\nmoving along the intuitionistic accessibility relation. We compare the\nresulting logic with other intuitionistic monotone modal logics and show how it\ncan be embedded into a multimodal version of IK."}
{"id": "2507.13367", "categories": ["cs.CR", "cs.CV", "cs.MM", "eess.IV", "68Q80", "I.4.2"], "pdf": "https://arxiv.org/pdf/2507.13367", "abs": "https://arxiv.org/abs/2507.13367", "authors": ["Mehrab Hosain", "Rajiv Kapoor"], "title": "A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security", "comment": "Accepted COMITCON 2023. Lecture Notes in Electrical Engineering, vol\n  1191. Springer", "summary": "Steganography is the process of embedding secret information discreetly\nwithin a carrier, ensuring secure exchange of confidential data. The Adaptive\nPixel Value Differencing (APVD) steganography method, while effective,\nencounters certain challenges like the \"unused blocks\" issue. This problem can\ncause a decrease in security, compromise the embedding capacity, and lead to\nlower visual quality. This research presents a novel steganographic strategy\nthat integrates APVD with pseudorandom pixel selection to effectively mitigate\nthese issues. The results indicate that the new method outperforms existing\ntechniques in aspects of security, data hiding capacity, and the preservation\nof image quality. Empirical results reveal that the combination of APVD with\npseudorandom pixel selection significantly enhances key image quality metrics\nsuch as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),\nand Structural Similarity Index (SSIM), surpassing other contemporary methods\nin performance. The newly proposed method is versatile, able to handle a\nvariety of cover and secret images in both color and grayscale, thereby\nensuring secure data transmission without compromising the aesthetic quality of\nthe image."}
{"id": "2507.13421", "categories": ["math.CO", "cs.GT", "91B32", "F.2.2; J.4"], "pdf": "https://arxiv.org/pdf/2507.13421", "abs": "https://arxiv.org/abs/2507.13421", "authors": ["Pablo Soberón"], "title": "Fair distribution of bundles", "comment": "13 pages, 3 figures", "summary": "In this paper, we study the problem of splitting fairly bundles of items. We\nshow that given $n$ bundles with $m$ kinds of items in them, it is possible to\ndistribute the value of each kind of item fairly among $r$ persons by breaking\napart at most $(r-1)m$ bundles. Moreover, we can guarantee that each\nparticipant will receive roughly $n/r - mr/2$ full bundles. The proof methods\nare topological and use a modified form of the configuration space/test map\nscheme. We obtain optimal results when $r$ is a power of two."}
{"id": "2507.13475", "categories": ["math.OC", "cs.NA", "math.NA", "65K10, 68T07"], "pdf": "https://arxiv.org/pdf/2507.13475", "abs": "https://arxiv.org/abs/2507.13475", "authors": ["Wolfgang Dahmen", "Wuchen Li", "Yuankai Teng", "Zhu Wang"], "title": "Expansive Natural Neural Gradient Flows for Energy Minimization", "comment": "40 pages, 19 figures", "summary": "This paper develops expansive gradient dynamics in deep neural\nnetwork-induced mapping spaces. Specifically, we generate tools and concepts\nfor minimizing a class of energy functionals in an abstract Hilbert space\nsetting covering a wide scope of applications such as PDEs-based inverse\nproblems and supervised learning. The approach hinges on a Hilbert space metric\nin the full diffeomorphism mapping space, which could be viewed as a\ngeneralized Wasserstein-2 metric. We then study a projection gradient descent\nmethod within deep neural network parameterized sets. More importantly, we\ndevelop an adaptation and expanding strategy to step-by-step enlarge the deep\nneural network structures. In particular, the expansion mechanism aims to\nenhance the alignment of the neural manifold induced natural gradient direction\nas well as possible with the ideal Hilbert space gradient descent direction\nleveraging the fact that we can evaluate projections of the Hilbert space\ngradient. We demonstrate the efficacy of the proposed strategy for several\nsimple model problems for energies arising in the context of supervised\nlearning, model reduction, or inverse problems. In particular, we highlight the\nimportance of assembling the neural flow matrix based on the inner product for\nthe ambient Hilbert space. The actual algorithms are the simplest\nspecifications of a broader spectrum based on a correspondingly wider\ndiscussion, postponing a detailed analysis to forthcoming work."}
{"id": "2507.13473", "categories": ["math.NT", "math.AG", "math.RT"], "pdf": "https://arxiv.org/pdf/2507.13473", "abs": "https://arxiv.org/abs/2507.13473", "authors": ["Tony Feng", "Benjamin Howard", "Mikayel Mkrtchyan"], "title": "Higher Siegel--Weil formula for unitary groups II: corank one terms", "comment": null, "summary": "We prove the higher Siegel--Weil formula for \\emph{corank one} terms,\nrelating (1) the $r^{\\rm th}$ central derivatives of corank one Fourier\ncoefficients of Siegel--Eisenstein series, and (2) the degrees of special\ncycles of virtual dimension 0 on the moduli stack of Hermitian shtukas with $r$\nlegs. Notably, the formula holds for all $r$, regardless of the order of\nvanishing of the Eisenstein series. This extends earlier work of\nFeng--Yun--Zhang, who proved the higher Siegel--Weil formula for the\nnon-singular (corank zero) terms."}
{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency."}
{"id": "2507.13505", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13505", "abs": "https://arxiv.org/abs/2507.13505", "authors": ["Steven Lamp", "Jason D. Hiser", "Anh Nguyen-Tuong", "Jack W. Davidson"], "title": "PHASE: Passive Human Activity Simulation Evaluation", "comment": null, "summary": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and\nsandboxes, require realistic human behavior to be effective, yet no\nquantitative method exists to assess the behavioral fidelity of synthetic user\npersonas. This paper presents PHASE (Passive Human Activity Simulation\nEvaluation), a machine learning framework that analyzes Zeek connection logs\nand distinguishes human from non-human activity with over 90\\% accuracy. PHASE\noperates entirely passively, relying on standard network monitoring without any\nuser-side instrumentation or visible signs of surveillance. All network\nactivity used for machine learning is collected via a Zeek network appliance to\navoid introducing unnecessary network traffic or artifacts that could disrupt\nthe fidelity of the simulation environment. The paper also proposes a novel\nlabeling approach that utilizes local DNS records to classify network traffic,\nthereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley\nAdditive exPlanations) analysis to uncover temporal and behavioral signatures\nindicative of genuine human users. In a case study, we evaluate a synthetic\nuser persona and identify distinct non-human patterns that undermine behavioral\nrealism. Based on these insights, we develop a revised behavioral configuration\nthat significantly improves the human-likeness of synthetic activity yielding a\nmore realistic and effective synthetic user persona."}
{"id": "2507.13479", "categories": ["math.CO", "math.NT"], "pdf": "https://arxiv.org/pdf/2507.13479", "abs": "https://arxiv.org/abs/2507.13479", "authors": ["Victor Nicolas Schvöllner"], "title": "Clasificación por 2-switch-degree de grafos split", "comment": "Doctoral thesis, in Spanish language", "summary": "The 2-switch-degree of $G$ is the number of distinct 2-switches acting on a\ngraph $G$. In this work we study structural properties of the 2-switch-degree,\nwith a focus on split graphs. Our approach is motivated by the Tyshkevich\ndecomposition, which uniquely expresses any graph as a composition $G_r \\circ\n\\ldots \\circ G_1$ of irreducible graphs, where $G_2, \\ldots, G_r$ are split.\nOur key tool is the factor graph $\\Phi(S)$, a multigraph associated with a\nsplit graph $S$ that encodes 2-switch-degree information via edge\nmultiplicities between independet vertices of $S$. By leveraging $\\Phi(S)$, we\nreduce the problem of classifying irreducible split graphs to enumerating and\nanalyzing unlabeled connected multigraphs of fixed size. Using this method, we\nfully classify irreducible split graphs of degrees 1, 2, 3, and 4. Further, we\nintroduce and investigate the $\\Delta$-property, a surprising connection\nbetween Graph Theory and Number Theory that arises from $n$-simple triangles\n(3-cycles with uniform edge multiplicity $n$) of the factor graph."}
{"id": "2507.13493", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.13493", "abs": "https://arxiv.org/abs/2507.13493", "authors": ["Dominic Yang"], "title": "A Specialized Simplex Algorithm for Budget-Constrained Total Variation-Regularized Problems", "comment": null, "summary": "We consider a class of linear programs on graphs with total variation\nregularization and a budgetary constraint. For these programs, we give a\ncharacterization of basic solutions in terms of rooted spanning forests with\norientation on the underlying graph. This leads to an interpretation of the\nsimplex method in terms of simple graph operations on these underlying forests.\nWe exploit this structure to produce an accelerated simplex method and\nempirically show that such improvements can lead to an order of magnitude\nimprovement in time when compared to state-of-the-art solvers."}
{"id": "2507.13500", "categories": ["math.NT", "math.RT"], "pdf": "https://arxiv.org/pdf/2507.13500", "abs": "https://arxiv.org/abs/2507.13500", "authors": ["Andrea Dotto", "Bao V. Le Hung"], "title": "Cohomology of $p$-adic Chevalley groups", "comment": "31 pages", "summary": "Let $G$ be a split connected reductive group over the ring of integers of a\nfinite unramified extension $K$ of $\\mathbf{Q}_p$. Under a standard assumption\non the Coxeter number of $G$, we compute the cohomology algebra of\n$G(\\mathcal{O}_K)$ and its Iwahori subgroups, with coefficients in the residue\nfield of $K$. Our methods involve a new presentation of some graded Lie\nalgebras appearing in Lazard's theory of saturated $p$-valued groups, and a\nreduction to coherent cohomology of the flag variety in positive\ncharacteristic. We also consider the case of those inner forms of\n$\\mathrm{GL}_n(K)$ that give rise to the Morava stabilizer groups in stable\nhomotopy theory."}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications."}
{"id": "2507.13591", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13591", "abs": "https://arxiv.org/abs/2507.13591", "authors": ["Sahar Ghoflsaz Ghinani", "Elaheh Sadredini"], "title": "FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning", "comment": "15 Pages, 12 Figures", "summary": "Federated Learning (FL) enables collaborative model training without\ncentralizing client data, making it attractive for privacy-sensitive domains.\nWhile existing approaches employ cryptographic techniques such as homomorphic\nencryption, differential privacy, or secure multiparty computation to mitigate\ninference attacks-including model inversion, membership inference, and gradient\nleakage-they often suffer from high computational, communication, or memory\noverheads. Moreover, many methods overlook the confidentiality of the global\nmodel itself, which may be proprietary and sensitive. These challenges limit\nthe practicality of secure FL, especially in cross-silo deployments involving\nlarge datasets and strict compliance requirements.\n  We present FuSeFL, a fully secure and scalable FL scheme designed for\ncross-silo settings. FuSeFL decentralizes training across client pairs using\nlightweight secure multiparty computation (MPC), while confining the server's\nrole to secure aggregation. This design eliminates server bottlenecks, avoids\ndata offloading, and preserves full confidentiality of data, model, and updates\nthroughout training. FuSeFL defends against inference threats, achieves up to\n95% lower communication latency and 50% lower server memory usage, and improves\naccuracy over prior secure FL solutions, demonstrating strong security and\nefficiency at scale."}
{"id": "2507.13566", "categories": ["math.CO", "05A17, 05A19, 11P81, 11P83"], "pdf": "https://arxiv.org/pdf/2507.13566", "abs": "https://arxiv.org/abs/2507.13566", "authors": ["Eli R. DeWitt", "William J. Keith"], "title": "Combinatorial proof of a congruence for partitions into two sizes of part", "comment": null, "summary": "Previous work showed that, for $\\nu_2(n)$ the number of partitions of $n$\ninto exactly two part sizes, one has $\\nu_2(16n + 14) \\equiv 0 \\pmod{4}$. The\nearlier proof required the technology of modular forms, and a combinatorial\nproof was desired. This article provides the requested proof, in the process\nrefining divisibility to finer subclasses. Some of these subclasses have counts\nclosely related to the divisor function $d(16n + 14)$, and we offer a\nconjecture on a potential rank statistic."}
{"id": "2507.13557", "categories": ["math.OC", "cs.SY", "eess.SY", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.13557", "abs": "https://arxiv.org/abs/2507.13557", "authors": ["Stella Slad", "Burkhard Luy"], "title": "Single spin exact gradients for the optimization of complex pulses and pulse sequences", "comment": null, "summary": "The efficient computer optimization of magnetic resonance pulses and pulse\nsequences involves the calculation of a problem-adapted cost function as well\nas its gradients with respect to all controls applied. The gradients generally\ncan be calculated as a finite difference approximation, as a GRAPE\napproximation, or as an exact function, e.g. by the use of the augmented matrix\nexponentiation, where the exact gradient should lead to best optimization\nconvergence. However, calculation of exact gradients is computationally\nexpensive and analytical exact solutions to the problem would be highly\ndesirable. As the majority of todays pulse optimizations involve a single spin\n1/2, which can be represented by simple rotation matrices in the Bloch space or\nby their corresponding Cayley-Klein/quaternion parameters, the derivations of\nanalytical exact gradient functions appear to be feasible. Taking two\noptimization types, the optimization of point-to-point pulses using\n3D-rotations and the optimization of universal rotation pulses using\nquaternions, analytical solutions for gradients with respect to controls have\nbeen derived. Controls in this case can be conventional $x$ and $y$ pulses, but\nalso $z$-controls, as well as gradients with respect to amplitude and phase of\na pulse shape. In addition, analytical solutions with respect to pseudo\ncontrols, involving holonomic constraints to maximum rf-amplitudes, maximum\nrf-power, or maximum rf-energy, are introduced. Using the hyperbolic tangent\nfunction, maximum values are imposed in a fully continuous and differentiable\nway. The obtained analytical gradients allow the calculation two orders of\nmagnitude faster than the augmented matrix exponential approach. The exact\ngradients for different controls are finally compared in a number of\noptimizations involving broadband pulses for $^{15}$N, $^{13}$C, and $^{19}$F\napplications."}
{"id": "2507.13504", "categories": ["math.NT", "11N05 (Primary), 11M26 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.13504", "abs": "https://arxiv.org/abs/2507.13504", "authors": ["Shubhrajit Bhattacharya", "Greg Martin", "Reginald M. Simpson"], "title": "Correlations of error terms for weighted prime counting functions", "comment": "Comments welcome!", "summary": "Standard prime-number counting functions, such as $\\psi(x)$, $\\theta(x)$, and\n$\\pi(x)$, have error terms with limiting logarithmic distributions once\nsuitably normalized. The same is true of weighted versions of those sums, like\n$\\pi_r(x) = \\sum_{p\\le x} \\frac1p$ and $\\pi_\\ell(x) = \\sum_{p\\le x}\n\\log(1-\\frac1p)^{-1}$, that were studied by Mertens. These limiting\ndistributions are all identical, but passing to the limit loses information\nabout how these error terms are correlated with one another.\n  In this paper, we examine these correlations, showing, for example, that\npersistent inequalities between certain pairs of normalized error terms are\nequivalent to the Riemann hypothesis (RH). Assuming both RH and LI, the linear\nindependence of the positive imaginary parts of the zeros of $\\zeta(s)$, we\ncalculate the logarithmic densities of the set of real numbers for which two\ndifferent error terms have prescribed signs. For example, we conditionally show\nthat $\\psi(x) - x$ and $\\sum_{n\\le x} \\frac{\\Lambda(n)}n - (\\log x - C_0)$ have\nthe same sign on a set of logarithmic density $\\approx 0.9865$."}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merchán", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains."}
{"id": "2507.13598", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13598", "abs": "https://arxiv.org/abs/2507.13598", "authors": ["Amro Abdalla", "Ismail Shaheen", "Dan DeGenaro", "Rupayan Mallick", "Bogdan Raita", "Sarah Adel Bargal"], "title": "GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention", "comment": "Warning: This paper contains NSFW content. Reader discretion is\n  advised", "summary": "We present GIFT: a {G}radient-aware {I}mmunization technique to defend\ndiffusion models against malicious {F}ine-{T}uning while preserving their\nability to generate safe content. Existing safety mechanisms like safety\ncheckers are easily bypassed, and concept erasure methods fail under\nadversarial fine-tuning. GIFT addresses this by framing immunization as a\nbi-level optimization problem: the upper-level objective degrades the model's\nability to represent harmful concepts using representation noising and\nmaximization, while the lower-level objective preserves performance on safe\ndata. GIFT achieves robust resistance to malicious fine-tuning while\nmaintaining safe generative quality. Experimental results show that our method\nsignificantly impairs the model's ability to re-learn harmful concepts while\nmaintaining performance on safe content, offering a promising direction for\ncreating inherently safer generative models resistant to adversarial\nfine-tuning attacks."}
{"id": "2507.13592", "categories": ["math.CO", "05C62 (05C50, 52C35)"], "pdf": "https://arxiv.org/pdf/2507.13592", "abs": "https://arxiv.org/abs/2507.13592", "authors": ["Hiroshi Nozaki", "Masashi Shinohara", "Sho Suda"], "title": "Pseudo-Euclidean representations of switching classes of Johnson and Hamming graphs with minimal dimension", "comment": "17 pages, no figure", "summary": "This paper considers minimum-dimensional representations of graphs in\npseudo-Euclidean spaces, where adjacency and non-adjacency relations are\nreflected in fixed scalar square values. A representation of a simple graph\n$(V,E)$ is a mapping $\\varphi$ from the vertices to the pseudo-Euclidean space\n$\\mathbb{R}^{p,q}$ such that $||\\varphi(u)-\\varphi(v)|| = a$ if $(u,v) \\in E$,\n$b$ if $(u,v) \\notin E$ and $u \\ne v$, and $0$ if $u = v$, for some $a,b \\in\n\\mathbb{R}$, where $||\\boldsymbol{x}|| = \\langle\\langle \\boldsymbol{x},\n\\boldsymbol{x} \\rangle\\rangle = \\sum_{i=1}^p x_i^2 - \\sum_{j=1}^q x_{p+j}^2$ is\nthe scalar square of $\\boldsymbol{x}$ in $\\mathbb{R}^{p,q}$. For a finite set\n$X$ in $\\mathbb{R}^{p,q}$, define $A(X) = \\{||\\boldsymbol{x}-\\boldsymbol{y}|| :\n\\boldsymbol{x},\\boldsymbol{y} \\in X, \\boldsymbol{x} \\ne \\boldsymbol{y} \\}$. We\ncall $X$ an $s$-indefinite-distance set if $|A(X)| = s$. An\n$s$-indefinite-distance set in $\\mathbb{R}^{p,0} = \\mathbb{R}^p$ is called an\n$s$-distance set. Graphs obtained from Seidel switching of a Johnson graph\nsometimes admit Euclidean or pseudo-Euclidean representations in low dimensions\nrelative to the number of vertices. For example, Lison\\v{e}k (1997) obtained a\nlargest 2-distance set in $\\mathbb{R}^8$ and spherical 2-indefinite-distance\nsets in $\\mathbb{R}^{p,1}$ for $p \\ge 10$ from the switching classes of Johnson\ngraphs. In this paper, we consider graphs in the switching classes of Johnson\nand Hamming graphs and classify those that admit representations in\n$\\mathbb{R}^{p,q}$ with the smallest possible dimension $p+q$ among all graphs\nin the same class. This method recovers known results, such as the largest\n2-(indefinite)-distance sets constructed by Lison\\v{e}k, and also provides a\nunified framework for determining the minimum dimension of representations for\nentire switching classes of strongly regular graphs."}
{"id": "2507.13611", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.13611", "abs": "https://arxiv.org/abs/2507.13611", "authors": ["Juan-Alberto Estrada-Garcia", "Ruiwei Jiang", "Alexandre Moreira"], "title": "Dynamic Transmission Line Switching Amidst Wildfire-Prone Weather Under Decision-Dependent Uncertainty", "comment": "29 pages", "summary": "During dry and windy seasons, environmental conditions significantly increase\nthe risk of wildfires, exposing power grids to disruptions caused by\ntransmission line failures. Wildfire propagation exacerbates grid\nvulnerability, potentially leading to prolonged power outages. To address this\nchallenge, we propose a multi-stage optimization model that dynamically adjusts\ntransmission grid topology in response to wildfire propagation, aiming to\ndevelop an optimal response policy. By accounting for decision-dependent\nuncertainty, where line survival probabilities depend on usage, we employ\ndistributionally robust optimization to model uncertainty in line survival\ndistributions. We adapt the stochastic nested decomposition algorithm and\nderive a deterministic upper bound for its finite convergence. To enhance\ncomputational efficiency, we exploit the Lagrangian dual problem structure for\na faster generation of Lagrangian cuts. Using realistic data from the\nCalifornia transmission grid, we demonstrate the superior performance of\ndynamic response policies against two-stage alternatives through a\ncomprehensive case study. In addition, we construct easy-to-implement policies\nthat significantly reduce computational burden while maintaining good\nperformance in real-time deployment."}
{"id": "2507.13645", "categories": ["math.NT", "11D72, 11E20, 11E25, 11F27, 14H42"], "pdf": "https://arxiv.org/pdf/2507.13645", "abs": "https://arxiv.org/abs/2507.13645", "authors": ["Nasser Abdo Saeed Bulkhali", "A. Vanitha", "M. P. Chaudhary"], "title": "Universal quaternary mixed sums involving generalized 3-, 4-, 5- and 8-gonal numbers via products of Ramanujan's theta functions", "comment": null, "summary": "Generalized $m$-gonal numbers are those $p_m(x)= [ (m - 2)x^2 - (m - 4)x ]/2\n$ where $x$ and $m$ are integers with $m \\geq 3$. If any nonnegative integer\ncan be written in the form $ap_r(h)+bp_s(l)+cp_t(m)+dp_u(n)$, where $a,b,c,d$\nare positive integers, then we call $ap_r(h)+bp_s(l)+cp_t(m)+dp_u(n)$ a\nuniversal quaternary sum. In this paper, we determine the universality of many\nquaternary sums when $r,s,t,u \\in \\{3,4,5,8\\}$, using the theory of Ramanujan's\ntheta function identities"}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence."}
{"id": "2507.13629", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13629", "abs": "https://arxiv.org/abs/2507.13629", "authors": ["Niveen O. Jaffal", "Mohammed Alkhanafseh", "David Mohaisen"], "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques", "comment": "21 pages", "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems."}
{"id": "2507.13596", "categories": ["math.CO", "math.MG"], "pdf": "https://arxiv.org/pdf/2507.13596", "abs": "https://arxiv.org/abs/2507.13596", "authors": ["Ekaterina V. Melikhova"], "title": "On the number of faces of marked order polytopes", "comment": "22 pages, in Russian language, 12 figures", "summary": "In this paper, we present a new method for computing the f-vector of a marked\norder polytope. Namely, given an arbitrary (polyhedral) subdivision of an\narbitrary convex polytope, we construct a cochain complex (over the two-element\nfield Z_2) such that the dimensions of its cohomology groups equal the\ncomponents of the f-vector of the original polytope. In the case of a marked\norder polytope and its well-known cubosimplicial subdivision, this cochain\ncomplex can be described purely combinatorially -- which yields the said\ncomputation of the f-vector. Of independent interest may be our combinatorial\ndescription of the said cubosimplicial subdivision (which was originally\nconstructed geometrically)."}
{"id": "2507.13613", "categories": ["math.OC", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13613", "abs": "https://arxiv.org/abs/2507.13613", "authors": ["Sihang Wei", "Melkior Ornik", "Hiroyasu Tsukamoto"], "title": "Conformal Contraction for Robust Nonlinear Control with Distribution-Free Uncertainty Quantification", "comment": "IEEE CDC 2025 submission (accepted)", "summary": "We present a novel robust control framework for continuous-time, perturbed\nnonlinear dynamical systems with uncertainty that depends nonlinearly on both\nthe state and control inputs. Unlike conventional approaches that impose\nstructural assumptions on the uncertainty, our framework enhances\ncontraction-based robust control with data-driven uncertainty prediction,\nremaining agnostic to the models of the uncertainty and predictor. We\nstatistically quantify how reliably the contraction conditions are satisfied\nunder dynamics with uncertainty via conformal prediction, thereby obtaining a\ndistribution-free and finite-time probabilistic guarantee for exponential\nboundedness of the trajectory tracking error. We further propose the\nprobabilistically robust control invariant (PRCI) tube for distributionally\nrobust motion planning, within which the perturbed system trajectories are\nguaranteed to stay with a finite probability, without explicit knowledge of the\nuncertainty model. Numerical simulations validate the effectiveness of the\nproposed robust control framework and the performance of the PRCI tube."}
{"id": "2507.13674", "categories": ["math.NT", "11B39, 11J86"], "pdf": "https://arxiv.org/pdf/2507.13674", "abs": "https://arxiv.org/abs/2507.13674", "authors": ["Jhon J. Bravo", "Pranabesh Das", "Jose L. Herrera", "John C. Saunders"], "title": "On Pell numbers representable as product of two generalized Fibonacci numbers", "comment": null, "summary": "A generalization of the well-known Fibonacci sequence is the $k$-Fibonacci\nsequence with some fixed integer $k\\ge 2$. The first $k$ terms of this sequence\nare $0,0, \\ldots, 1$, and each term afterwards is the sum of the preceding $k$\nterms. In this paper, we find all Pell numbers that can be written as a product\nof two $k$-Fibonacci numbers. The proof of our main theorem uses lower bounds\nfor linear forms in logarithms, properties of continued fractions, and a\nvariation of a result of Dujella and Peth\\H{o} in Diophantine approximation.\nThis work generalizes a prior result of Alekseyev which dealt with determining\nthe intersection of the Fibonacci and Pell sequences, a work of Ddamulira, Luca\nand Rakotomalala who searched for Pell numbers which are products of two\nFibonacci numbers, and a result of Bravo, G\\'omez, and Herrera, who found all\nPell numbers appearing in the $k$-Fibonacci sequence."}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains."}
{"id": "2507.13686", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13686", "abs": "https://arxiv.org/abs/2507.13686", "authors": ["Yulin Chen", "Haoran Li", "Yuexin Li", "Yue Liu", "Yangqiu Song", "Bryan Hooi"], "title": "TopicAttack: An Indirect Prompt Injection Attack via Topic Transition", "comment": "19 pages", "summary": "Large language models (LLMs) have shown remarkable performance across a range\nof NLP tasks. However, their strong instruction-following capabilities and\ninability to distinguish instructions from data content make them vulnerable to\nindirect prompt injection attacks. In such attacks, instructions with malicious\npurposes are injected into external data sources, such as web documents. When\nLLMs retrieve this injected data through tools, such as a search engine and\nexecute the injected instructions, they provide misled responses. Recent attack\nmethods have demonstrated potential, but their abrupt instruction injection\noften undermines their effectiveness. Motivated by the limitations of existing\nattack methods, we propose TopicAttack, which prompts the LLM to generate a\nfabricated conversational transition prompt that gradually shifts the topic\ntoward the injected instruction, making the injection smoother and enhancing\nthe plausibility and success of the attack. Through comprehensive experiments,\nTopicAttack achieves state-of-the-art performance, with an attack success rate\n(ASR) over 90\\% in most cases, even when various defense methods are applied.\nWe further analyze its effectiveness by examining attention scores. We find\nthat a higher injected-to-original attention ratio leads to a greater success\nprobability, and our method achieves a much higher ratio than the baseline\nmethods."}
{"id": "2507.13752", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.13752", "abs": "https://arxiv.org/abs/2507.13752", "authors": ["Ali Ghalavand", "Xueliang Li"], "title": "On the local metric dimension of $K_5$-free graphs", "comment": null, "summary": "Let \\( G \\) be a graph with order \\( n(G) \\geq 5 \\), local metric dimension\n\\( \\dim_l(G) \\), and clique number \\( \\omega(G) \\). In this paper, we\ninvestigate the local metric dimension of \\( K_5 \\)-free graphs and prove that\n\\( \\dim_l(G) \\leq \\lfloor\\frac{2}{3}n(G)\\rfloor \\) when \\( \\omega(G) = 4 \\). As\na consequence of this finding, along with previous publications, we establish\nthat if \\( G \\) is a \\( K_5 \\)-free graph, then \\( \\dim_l(G) \\leq\n\\lfloor\\frac{2}{5}n(G)\\rfloor \\) when \\( \\omega(G) = 2 \\), \\( \\dim_l(G) \\leq\n\\lfloor\\frac{1}{2}n(G)\\rfloor \\) when \\( \\omega(G) = 3 \\), and \\( \\dim_l(G)\n\\leq \\lfloor\\frac{2}{3}n(G)\\rfloor \\) when \\( \\omega(G) = 4 \\). Notably, these\nbounds are sharp for planar graphs. These results for graphs with a clique\nnumber less than or equal to 4 provide a positive answer to the conjecture\nstating that if \\( n(G) \\geq \\omega(G) + 1 \\geq 4 \\), then \\( \\dim_l(G) \\leq\n\\left( \\frac{\\omega(G) - 2}{\\omega(G) - 1} \\right)n(G) \\)."}
{"id": "2507.13776", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.13776", "abs": "https://arxiv.org/abs/2507.13776", "authors": ["Di Hou", "Tianyun Tang", "Kim-Chuan Toh"], "title": "RiNNAL+: a Riemannian ALM Solver for SDP-RLT Relaxations of Mixed-Binary Quadratic Programs", "comment": "52 pages, 2 figures", "summary": "Doubly nonnegative (DNN) relaxation usually provides a tight lower bound for\na mixed-binary quadratic program (MBQP). However, solving DNN problems is\nchallenging because: (1) the problem size is $\\Omega((n+l)^2)$ for an MBQP with\n$n$ variables and $l$ inequality constraints, and (2) the rank of optimal\nsolutions cannot be estimated a priori due to the absence of theoretical\nbounds. In this work, we propose RiNNAL+, a Riemannian augmented Lagrangian\nmethod (ALM) for solving DNN problems. We prove that the DNN relaxation of an\nMBQP, with matrix dimension $(n+l+1)$, is equivalent to the SDP-RLT relaxation\n(based on the reformulation-linearization technique) with a smaller matrix\ndimension $(n+1)$. In addition, we develop a hybrid method that alternates\nbetween two phases to solve the ALM subproblems. In phase one, we apply\nlow-rank matrix factorization and random perturbation to transform the feasible\nregion into a lower-dimensional manifold so that we can use the Riemannian\ngradient descent method. In phase two, we apply a single projected gradient\nstep to update the rank of the underlying variable and escape from spurious\nlocal minima arising in the first phase if necessary. To reduce the computation\ncost of the projected gradient step, we develop pre-processing and warm-start\ntechniques for acceleration. Unlike traditional rank-adaptive methods that\nrequire extensive parameter tuning, our hybrid method requires minimal tuning.\nExtensive experiments confirm the efficiency and robustness of RiNNAL+ in\nsolving various classes of large-scale DNN problems."}
{"id": "2507.13679", "categories": ["math.NT", "math.RT"], "pdf": "https://arxiv.org/pdf/2507.13679", "abs": "https://arxiv.org/abs/2507.13679", "authors": ["Anton Deitmar"], "title": "Distribution of prime geodesic traces", "comment": null, "summary": "This note complements a recent paper of Chatzakos, Harcos and Kaneko\n\\cite{CHK}. We use a Dirichlet style Prime Geodesic Theorem to improve the\nerror term estimate in loc. cit. at the cost of lowering the resolution. The\nproof relies on the Selberg trace formula."}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach."}
{"id": "2507.13720", "categories": ["cs.CR", "cs.DC", "cs.ET", "cs.NI", "68M10, 81P94, 94A60 68M10, 81P94, 94A60 68M10, 81P94, 94A60", "C.2.1; E.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.13720", "abs": "https://arxiv.org/abs/2507.13720", "authors": ["Saurav Ghosh"], "title": "Quantum Blockchain Survey: Foundations, Trends, and Gaps", "comment": "12 Pages, 4 figures", "summary": "Quantum computing poses fundamental risks to classical blockchain systems by\nundermining widely used cryptographic primitives. In response, two major\nresearch directions have emerged: post-quantum blockchains, which integrate\nquantum-resistant algorithms, and quantum blockchains, which leverage quantum\nproperties such as entanglement and quantum key distribution. This survey\nreviews key developments in both areas, analyzing their cryptographic\nfoundations, architectural designs, and implementation challenges. This work\nprovides a comparative overview of technical proposals, highlight trade-offs in\nsecurity, scalability, and deployment, and identify open research problems\nacross hardware, consensus, and network design. The goal is to offer a\nstructured and comprehensive reference for advancing secure blockchain systems\nin the quantum era."}
{"id": "2507.13777", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.13777", "abs": "https://arxiv.org/abs/2507.13777", "authors": ["Ali Ghalavand", "Sandi Klavžar", "Xueliang Li"], "title": "Intertwining local (adjacency) metric dimension with the clique number of a graph", "comment": null, "summary": "Let $G$ be a simple connected graph with order $ n(G)$, local metric\ndimension $ {\\rm dim}_l(G)$, local adjacency metric dimension $ {\\rm\ndim}_{A,l}(G)$, and clique number $ \\omega(G)$, where $G\\not\\cong K_{n(G)}$ and\n$\\omega(G)\\geq3$. It is proved that $ {\\rm dim}_{A,l}(G) \\leq \\left\\lfloor\n\\left(\\frac{\\omega(G) - 2}{\\omega(G) - 1}\\right)n(G)\\right\\rfloor$.\nConsequently, the conjecture asserting that the latter expression is an upper\nbound for ${\\rm dim}_l(G)$ is confirmed. It is important to note that there are\ninfinitely many graphs that satisfy the equalities."}
{"id": "2507.13804", "categories": ["math.OC", "cs.NA", "math.DS", "math.NA", "90C30 (Primary) 65K05, 37C75, 58K05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.13804", "abs": "https://arxiv.org/abs/2507.13804", "authors": ["Andreea-Alexandra Muşat", "Nicolas Boumal"], "title": "Gradient descent avoids strict saddles with a simple line-search method too", "comment": "38 pages", "summary": "It is known that gradient descent (GD) on a $C^2$ cost function generically\navoids strict saddle points when using a small, constant step size. However, no\nsuch guarantee existed for GD with a line-search method. We provide one for a\nmodified version of the standard Armijo backtracking method with generic,\narbitrarily large initial step size. In contrast to previous works, our\nanalysis does not require a globally Lipschitz gradient.\n  We extend this to the Riemannian setting (RGD), assuming the retraction is\nreal analytic (though the cost function still only needs to be $C^2$). In\nclosing, we also improve guarantees for RGD with a constant step size in some\nscenarios."}
{"id": "2507.13780", "categories": ["math.NT", "11M26, 11N05, 11M41, 11N80"], "pdf": "https://arxiv.org/pdf/2507.13780", "abs": "https://arxiv.org/abs/2507.13780", "authors": ["Frederik Broucke"], "title": "On the connection between zero-free regions and the error term in the Prime Number Theorem", "comment": "30 pages", "summary": "We provide for a wide class of zero-free regions an upper bound for the error\nterm in the Prime Number Theorem, refining works of Pintz (1980), Johnston\n(2024), and R\\'ev\\'esz (2024). Our method does not only apply to the Riemann\nzeta function, but to general Beurling zeta functions. Next we construct\nBeurling zeta functions having infinitely many zeros on a prescribed contour,\nand none to the right, for a wide class of such contours. We also deduce an\noscillation result for the corresponding error term in the Prime Number\nTheorem, showing that our aforementioned refinement is close to being sharp."}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps."}
{"id": "2507.13926", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13926", "abs": "https://arxiv.org/abs/2507.13926", "authors": ["Libor Polčák", "Giorgio Maone", "Michael McMahon", "Martin Bednář"], "title": "Developers Insight On Manifest v3 Privacy and Security Webextensions", "comment": "WEBIST'25, Marbella, Spain", "summary": "Webextensions can improve web browser privacy, security, and user experience.\nThe APIs offered by the browser to webextensions affect possible functionality.\nCurrently, Chrome transitions to a modified set of APIs called Manifest v3.\nThis paper studies the challenges and opportunities of Manifest v3 with an\nin-depth structured qualitative research. Even though some projects observed\npositive effects, a majority expresses concerns over limited benefits to users,\nremoval of crucial APIs, or the need to find workarounds. Our findings indicate\nthat the transition affects different types of webextensions differently; some\ncan migrate without losing functionality, while other projects remove\nfunctionality or decline to update. The respondents identified several critical\nmissing APIs, including reliable APIs to inject content scripts, APIs for\nstoring confidential content, and others."}
{"id": "2507.13821", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.13821", "abs": "https://arxiv.org/abs/2507.13821", "authors": ["Cyriac Antony", "Jacob Antony"], "title": "Some short notes on oriented line graphs and related matrices", "comment": null, "summary": "The notion of oriented line graphs is introduced by Kotani and Sunada, and\nthey are closely related to Hashimato's non-backtracking matrix. It is known\nthat for regular graphs $G$, the eigenvalues of the adjacency matrix of the\noriented line graph $\\vec{L}(G)$ of $G$ are the reciprocals of the poles of the\nIhara zeta function of $G$. We determine the characteristic polynomial of the\nadjacency matrix of the underlying undirected graph of $\\vec{L}(G)$ and the\nskew-symmetric adjacency matrix of $\\vec{L}(G)$ for $d$-regular graphs $G$ with\n$d\\geq 3$. We also exhibit a consequence of this result to star coloring of\nregular graphs."}
{"id": "2507.13983", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.13983", "abs": "https://arxiv.org/abs/2507.13983", "authors": ["Roberto Morales", "Umberto Biccari"], "title": "A Multi-Objective Optimization framework for Decentralized Learning with coordination constraints", "comment": null, "summary": "This article introduces a generalized framework for Decentralized Learning\nformulated as a Multi-Objective Optimization problem, in which both distributed\nagents and a central coordinator contribute independent, potentially\nconflicting objectives over a shared model parameter space. Unlike traditional\napproaches that merge local losses under a common goal, our formulation\nexplicitly incorporates coordinator-side criteria, enabling more flexible and\nstructured training dynamics. To navigate the resulting trade-offs, we explore\nscalarization strategies, particularly weighted sums, to construct tractable\nsurrogate problems. These yield solutions that are provably Pareto optimal\nunder standard convexity and smoothness assumptions, while embedding global\npreferences directly into local updates. We propose a decentralized\noptimization algorithm with convergence guarantees, and demonstrate its\nempirical performance through simulations, highlighting the impact of the\ncoordinator's influence on local agent behavior. The proposed approach offers a\nprincipled and customizable strategy for balancing personalization, fairness,\nand coordination in decentralized learning systems."}
{"id": "2507.13790", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2507.13790", "abs": "https://arxiv.org/abs/2507.13790", "authors": ["Eisuke Otsuka"], "title": "Iterated integrals on the Legendre family of elliptic curves", "comment": "20 pages", "summary": "K.T. Chen showed that iterated integrals give comparison isomorphisms between\nthe cohomologies of bar complexes and fundamental group rings. This led to the\ndevelopment of an algebraic-geometric approach to studying periods given by\niterated integrals. In this paper we consider an analogue of this comparison\nisomorphism theorem for iterated integrals on the Legendre family."}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed."}
{"id": "2507.13932", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.13932", "abs": "https://arxiv.org/abs/2507.13932", "authors": ["Feng Yu", "Ryan Laird"], "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology", "comment": null, "summary": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI)."}
{"id": "2507.13948", "categories": ["math.CO", "05D05"], "pdf": "https://arxiv.org/pdf/2507.13948", "abs": "https://arxiv.org/abs/2507.13948", "authors": ["Aanchal Gupta", "Gyula O. H. Katona"], "title": "Finding one excellent element in case of one lie", "comment": "8 pages, 0 figures", "summary": "An n-element set contains an unknown number of excellent elements, and our\ngoal is to identify at least one of these elements. The members of a family of\nsubsets can be asked if they contain at least one excellent element or not. At\nmost one of the answers can be wrong. We find the smallest family that finds\none excellent element or claims that there is none."}
{"id": "2507.14051", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.14051", "abs": "https://arxiv.org/abs/2507.14051", "authors": ["Haihao Lu", "Zedong Peng", "Jinwen Yang"], "title": "cuPDLP+: A Further Enhanced GPU-Based First-Order Solver for Linear Programming", "comment": null, "summary": "We introduce cuPDLP+, a further enhanced GPU-based first-order solver for\nlinear programming. Building on the predecessor cuPDLP, cuPDLP+ incorporates\nrecent algorithmic advances, including the restarted Halpern PDHG method with\nreflection, a novel restart criterion, and a PID-controlled primal weight\nupdate. These innovations are carefully tailored for GPU architectures and\ndeliver substantial empirical gains. On a comprehensive benchmark of MIPLIB LP\nrelaxations, cuPDLP+ achieves 2x - 4x speedup over cuPDLP, with particularly\nstrong improvements in high-accuracy and presolve-enabled settings."}
{"id": "2507.13831", "categories": ["math.NT", "11R04, 11R32"], "pdf": "https://arxiv.org/pdf/2507.13831", "abs": "https://arxiv.org/abs/2507.13831", "authors": ["Ž. Baronėnas", "P. Drungilas", "J. Jankauskas"], "title": "Linear relations of four conjugates of an algebraic number", "comment": null, "summary": "We characterize all algebraic numbers $\\alpha$ of degree $d\\in\\{4,5,6,7\\}$\nfor which there exist four distinct algebraic conjugates $\\alpha_1$,\n$\\alpha_2$, $\\alpha_3$, $\\alpha_4$ of $\\alpha$ satisfying the relation\n$\\alpha_{1}+\\alpha_{2}=\\alpha_{3}+\\alpha_{4}$. In particular, we prove that an\nalgebraic number $\\alpha$ of degree 6 satisfies this relation with\n$\\alpha_{1}+\\alpha_{2}\\notin\\mathbb{Q}$ if and only if $\\alpha$ is the sum of a\nquadratic and a cubic algebraic number. Moreover, we describe all possible\nGalois groups of the normal closure of $\\mathbb{Q}(\\alpha)$ for such algebraic\nnumbers $\\alpha$. We also consider similar relations\n$\\alpha_{1}+\\alpha_{2}+\\alpha_{3}+\\alpha_{4}=0$ and\n$\\alpha_{1}+\\alpha_{2}+\\alpha_{3}=\\alpha_{4}$ for algebraic numbers of degree\nup to 7."}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community."}
{"id": "2507.14007", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.14007", "abs": "https://arxiv.org/abs/2507.14007", "authors": ["Serhan W. Bahar"], "title": "The CryptoNeo Threat Modelling Framework (CNTMF): Securing Neobanks and Fintech in Integrated Blockchain Ecosystems", "comment": null, "summary": "The rapid integration of blockchain, cryptocurrency, and Web3 technologies\ninto digital banks and fintech operations has created an integrated environment\nblending traditional financial systems with decentralised elements. This paper\nintroduces the CryptoNeo Threat Modelling Framework (CNTMF), a proposed\nframework designed to address the risks in these ecosystems, such as oracle\nmanipulation and cross-chain exploits. CNTMF represents a proposed extension of\nestablished methodologies like STRIDE, OWASP Top 10, NIST frameworks, LINDDUN,\nand PASTA, while incorporating tailored components including Hybrid Layer\nAnalysis, the CRYPTOQ mnemonic for cryptocurrency-specific risks, and an\nAI-Augmented Feedback Loop. Drawing on real-world data from 2025 incidents,\nCNTMF supports data-driven mitigation to reduce losses, which totalled\napproximately $2.47 billion in the first half of 2025 across 344 security\nevents (CertiK via GlobeNewswire, 2025; Infosecurity Magazine, 2025). Its\nphases guide asset mapping, risk profiling, prioritisation, mitigation, and\niterative feedback. This supports security against evolving risks like\nstate-sponsored attacks."}
{"id": "2507.13968", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2507.13968", "abs": "https://arxiv.org/abs/2507.13968", "authors": ["Rodolfo E. Maza"], "title": "Continuity of Functions on Bare Representation of Graphs under Star Topology", "comment": null, "summary": "This paper introduces a novel topology, referred to as the star topology, on\nfinite graphs. By treating vertices and edges as points in a unified space, we\nexplore continuous maps between Bare representations of a graph and their\nproperties. The key distinction lies in the fact that while every graph\nhomomorphism induces a continuous map, the converse is not generally true due\nto potential loss of adjacency information."}
{"id": "2507.14122", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.14122", "abs": "https://arxiv.org/abs/2507.14122", "authors": ["Guillaume Garrigos", "Daniel Cortild", "Lucas Ketels", "Juan Peypouquet"], "title": "Last-Iterate Complexity of SGD for Convex and Smooth Stochastic Problems", "comment": null, "summary": "Most results on Stochastic Gradient Descent (SGD) in the convex and smooth\nsetting are presented under the form of bounds on the ergodic function value\ngap. It is an open question whether bounds can be derived directly on the last\niterate of SGD in this context. Recent advances suggest that it should be\npossible. For instance, it can be achieved by making the additional, yet\nunverifiable, assumption that the variance of the stochastic gradients is\nuniformly bounded. In this paper, we show that there is no need of such an\nassumption, and that SGD enjoys a $\\tilde O \\left( T^{-1/2} \\right)$\nlast-iterate complexity rate for convex smooth stochastic problems."}
{"id": "2507.13900", "categories": ["math.NT", "11G35, 14L99"], "pdf": "https://arxiv.org/pdf/2507.13900", "abs": "https://arxiv.org/abs/2507.13900", "authors": ["Ajneet Dhillon"], "title": "Approximation theorems for classifying stacks over number fields", "comment": null, "summary": "Approximation theorems for algebraic stacks over a number field $k$ are\nstudied in this article. For G a connected linear algebraic group over a number\nfield we prove strong approximation with Brauer-Manin obstruction for the\nclassifying stack $BG$. This result answers a very concrete question, given\n$G$-torsors $P_v$ over $k_v$, where $v$ ranges over a finite number of places,\nwhen can you approximate the $P_v$ by a $G$-torsor $P$ defined over $k$."}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed."}
{"id": "2507.14109", "categories": ["cs.CR", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14109", "abs": "https://arxiv.org/abs/2507.14109", "authors": ["Xinyu Cao", "Bimal Adhikari", "Shangqing Zhao", "Jingxian Wu", "Yanjun Pan"], "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting", "comment": null, "summary": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds."}
{"id": "2507.14033", "categories": ["math.CO", "math.RT", "05E10 (Primary), 20F55 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.14033", "abs": "https://arxiv.org/abs/2507.14033", "authors": ["Gaston Burrull", "Nicolas Libedinsky", "Rodrigo Villegas"], "title": "Shape and class of Bruhat Intervals", "comment": "69 pages, 25 pictures, 1 table", "summary": "We study Bruhat intervals in affine Weyl groups by viewing them as regions of\nalcoves. In type $\\widetilde{A}_2$ we show that each interval coincides with a\ngeneralized permutohedron minus a star-shaped polygon, and we prove a subtler\nversion inside the dominant chamber of type $\\widetilde{A}_n$. Motivated by\nthis geometry, we conjecture that whenever two Bruhat intervals are isomorphic,\nthere exists an isomorphism realized by a piecewise isometry. We prove this\nwhen both endpoints are dominant in $\\widetilde{A}_2$ and obtain partial\nresults in $\\widetilde{A}_n$. In the course of proving these results, we made\nthe surprising observation that much of the information contained in a Bruhat\ninterval is already encoded in a tiny portion of it."}
{"id": "2507.14118", "categories": ["math.NT", "11M32, 33E05 (Primary), 11F11 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.14118", "abs": "https://arxiv.org/abs/2507.14118", "authors": ["Hayato Kanno", "Katsumi Kina"], "title": "Multiple $\\wp$-Functions and Their Applications", "comment": null, "summary": "In this paper, we introduce and study multiple $\\wp$-functions, which\ngeneralize the classical Weierstrass $\\wp$-function to iterated sums over\nlattice points, and we establish explicit formulas expressing them in terms of\nsingle $\\wp$-functions with coefficients given by multiple Eisenstein series.\nAs an application, we derive some relations among multiple Eisenstein series\nand multiple zeta values by exploiting the double periodicity of the multiple\n$\\wp$-functions."}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs."}
{"id": "2507.14068", "categories": ["math.CO", "math.AT", "06-08, 55P91, 18M60, 06B05"], "pdf": "https://arxiv.org/pdf/2507.14068", "abs": "https://arxiv.org/abs/2507.14068", "authors": ["Scott Balchin", "Ben Spitz"], "title": "Formal Concept Analysis and Homotopical Combinatorics", "comment": "27 pages, comments welcome!", "summary": "Formal Concept Analysis makes the fundamental observation that any complete\nlattice $(L, \\leq)$ is determined up to isomorphism by the restriction of the\nrelation ${\\leq} \\subseteq L \\times L$ to the set $J(L) \\times M(L)$, where\n$J(L)$ is the set of join-irreducible elements of $L$ and $M(L)$ is the set of\nmeet-irreducible elements of $L$.\n  For any finite lattice $L$ equipped with the action of a finite group $G$, we\nexplicitly describe this restricted relation for the lattice of transfer\nsystems $\\mathsf{Tr}(L)$ in terms of $L$ only. We apply this to give new\ncomputations of the number of transfer systems for certain finite groups, and\nto produce bounds on the number of transfer systems on certain families of\nabelian finite groups. We also provide computer code to enable other\nresearchers' use of these techniques."}
{"id": "2507.13479", "categories": ["math.CO", "math.NT"], "pdf": "https://arxiv.org/pdf/2507.13479", "abs": "https://arxiv.org/abs/2507.13479", "authors": ["Victor Nicolas Schvöllner"], "title": "Clasificación por 2-switch-degree de grafos split", "comment": "Doctoral thesis, in Spanish language", "summary": "The 2-switch-degree of $G$ is the number of distinct 2-switches acting on a\ngraph $G$. In this work we study structural properties of the 2-switch-degree,\nwith a focus on split graphs. Our approach is motivated by the Tyshkevich\ndecomposition, which uniquely expresses any graph as a composition $G_r \\circ\n\\ldots \\circ G_1$ of irreducible graphs, where $G_2, \\ldots, G_r$ are split.\nOur key tool is the factor graph $\\Phi(S)$, a multigraph associated with a\nsplit graph $S$ that encodes 2-switch-degree information via edge\nmultiplicities between independet vertices of $S$. By leveraging $\\Phi(S)$, we\nreduce the problem of classifying irreducible split graphs to enumerating and\nanalyzing unlabeled connected multigraphs of fixed size. Using this method, we\nfully classify irreducible split graphs of degrees 1, 2, 3, and 4. Further, we\nintroduce and investigate the $\\Delta$-property, a surprising connection\nbetween Graph Theory and Number Theory that arises from $n$-simple triangles\n(3-cycles with uniform edge multiplicity $n$) of the factor graph."}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals."}
{"id": "2507.13664", "categories": ["math.HO", "math.CO", "math.SP", "05C65, 15A18, 15A42"], "pdf": "https://arxiv.org/pdf/2507.13664", "abs": "https://arxiv.org/abs/2507.13664", "authors": ["Shashwath S Shetty", "K Arathi Bhat"], "title": "Spectral Theory of Hypergraphs: A Survey", "comment": "55 pages, 5 figures, 304 references", "summary": "Hypergraphs require higher-dimensional representations, which makes it more\ndifficult to compute and interpret their spectral properties. This survey\narticle uses the framework of hypermatrices to give an in-depth overview of the\nspectral theory of hypergraphs. Our focus in this article relies on the\ntheoretical aspects of hypergraphs that help to ease the computational methods.\nSpectral theory hypergraphs, one of the most advanced fields of study, are\nconstantly finding novel applications in various domains such as theoretical\ncomputer science, quantum physics, and theoretical chemistry, among many\nothers. We start our journey by introducing hypergraphs, hypermatrices\n(tensors), resultants, and their properties. We outline some of the results\nused to determine the adjacency spectrum of hypergraphs and go over some of the\ngroundbreaking findings in the development of the theory. On passing through a\nlist of bounds for the spectral radius of uniform hypergraphs, we will have a\nlook into the spectral versions of Tur\\'an-type problems in hypergraphs.\nFinally, in addition to the Estrada index of hypergraphs, some significant\nresults related to the characteristic polynomial and its relationship with the\nmatching polynomial are presented."}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration."}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis."}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Martín Diéguez", "François Olivier", "Torsten Schaub", "Igor Stéphan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm."}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale."}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code."}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy."}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments."}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources."}
{"id": "2507.13505", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13505", "abs": "https://arxiv.org/abs/2507.13505", "authors": ["Steven Lamp", "Jason D. Hiser", "Anh Nguyen-Tuong", "Jack W. Davidson"], "title": "PHASE: Passive Human Activity Simulation Evaluation", "comment": null, "summary": "Cybersecurity simulation environments, such as cyber ranges, honeypots, and\nsandboxes, require realistic human behavior to be effective, yet no\nquantitative method exists to assess the behavioral fidelity of synthetic user\npersonas. This paper presents PHASE (Passive Human Activity Simulation\nEvaluation), a machine learning framework that analyzes Zeek connection logs\nand distinguishes human from non-human activity with over 90\\% accuracy. PHASE\noperates entirely passively, relying on standard network monitoring without any\nuser-side instrumentation or visible signs of surveillance. All network\nactivity used for machine learning is collected via a Zeek network appliance to\navoid introducing unnecessary network traffic or artifacts that could disrupt\nthe fidelity of the simulation environment. The paper also proposes a novel\nlabeling approach that utilizes local DNS records to classify network traffic,\nthereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley\nAdditive exPlanations) analysis to uncover temporal and behavioral signatures\nindicative of genuine human users. In a case study, we evaluate a synthetic\nuser persona and identify distinct non-human patterns that undermine behavioral\nrealism. Based on these insights, we develop a revised behavioral configuration\nthat significantly improves the human-likeness of synthetic activity yielding a\nmore realistic and effective synthetic user persona."}
{"id": "2507.13598", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13598", "abs": "https://arxiv.org/abs/2507.13598", "authors": ["Amro Abdalla", "Ismail Shaheen", "Dan DeGenaro", "Rupayan Mallick", "Bogdan Raita", "Sarah Adel Bargal"], "title": "GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention", "comment": "Warning: This paper contains NSFW content. Reader discretion is\n  advised", "summary": "We present GIFT: a {G}radient-aware {I}mmunization technique to defend\ndiffusion models against malicious {F}ine-{T}uning while preserving their\nability to generate safe content. Existing safety mechanisms like safety\ncheckers are easily bypassed, and concept erasure methods fail under\nadversarial fine-tuning. GIFT addresses this by framing immunization as a\nbi-level optimization problem: the upper-level objective degrades the model's\nability to represent harmful concepts using representation noising and\nmaximization, while the lower-level objective preserves performance on safe\ndata. GIFT achieves robust resistance to malicious fine-tuning while\nmaintaining safe generative quality. Experimental results show that our method\nsignificantly impairs the model's ability to re-learn harmful concepts while\nmaintaining performance on safe content, offering a promising direction for\ncreating inherently safer generative models resistant to adversarial\nfine-tuning attacks."}
{"id": "2507.13629", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13629", "abs": "https://arxiv.org/abs/2507.13629", "authors": ["Niveen O. Jaffal", "Mohammed Alkhanafseh", "David Mohaisen"], "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques", "comment": "21 pages", "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling\nintelligent, adaptive, and automated approaches to threat detection,\nvulnerability assessment, and incident response. With their advanced language\nunderstanding and contextual reasoning, LLMs surpass traditional methods in\ntackling challenges across domains such as IoT, blockchain, and hardware\nsecurity. This survey provides a comprehensive overview of LLM applications in\ncybersecurity, focusing on two core areas: (1) the integration of LLMs into key\ncybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along\nwith mitigation strategies. By synthesizing recent advancements and identifying\nkey limitations, this work offers practical insights and strategic\nrecommendations for leveraging LLMs to build secure, scalable, and future-ready\ncyber defense systems."}
