{"id": "2508.02867", "categories": ["math.LO", "03E05, 03E35, 03E55"], "pdf": "https://arxiv.org/pdf/2508.02867", "abs": "https://arxiv.org/abs/2508.02867", "authors": ["Hannes Jakob"], "title": "Total Failure of Approachability at Successors of Singulars of Countable Cofinality", "comment": "33 pages", "summary": "Relative to class many supercompact cardinals, we construct a model of\n$\\ZFC+\\GCH$ where for every singular cardinal $\\delta$ of countable cofinality\nand every regular uncountable $\\mu<\\delta$ there are stationarily many\nnon-approachable points of cofinality $\\mu$ in $\\delta^+$. This answers a\nquestion of Mitchell and provides a decisive answer to a question of Foreman\nand Shelah."}
{"id": "2508.03432", "categories": ["math.LO", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.03432", "abs": "https://arxiv.org/abs/2508.03432", "authors": ["Célia Borlido", "Ganna Kudryavtseva", "Brett McLean"], "title": "Difference-restriction algebras with operators", "comment": "35 pages", "summary": "We exhibit an adjunction between a category of abstract algebras of partial\nfunctions that we call difference-restriction algebras and a category of\nHausdorff \\'etale spaces. Difference-restriction algebras are those algebras\nisomorphic to a collection of partial functions closed under relative\ncomplement and domain restriction. Our adjunction generalises the adjunction\nbetween the category of generalised Boolean algebras and the category of\nHausdorff spaces. We define the finitary compatible completion of a\ndifference-restriction algebra and show that the monad induced by our\nadjunction yields the finitary compatible completion of any\ndifference-restriction algebra. As a corollary, the adjunction restricts to a\nduality between the finitarily compatibly complete difference-restriction\nalgebras and the locally compact zero-dimensional Hausdorff \\'etale spaces,\ngeneralising the duality between generalised Boolean algebras and locally\ncompact zero-dimensional Hausdorff spaces. We then extend these adjunction,\nduality, and completion results to difference-restriction algebras equipped\nwith arbitrary additional compatibility preserving operators."}
{"id": "2508.03499", "categories": ["math.LO", "math.CA", "math.CV", "03C64, 32B20, 32C05, 58A07, 58A12"], "pdf": "https://arxiv.org/pdf/2508.03499", "abs": "https://arxiv.org/abs/2508.03499", "authors": ["Annette Huber", "Tobias Kaiser", "Abhishek Oswal"], "title": "On the de Rham theorem in the globally subanalytic setting", "comment": "27 pages. Comments are welcome!", "summary": "For globally subanalytic manifolds we define de Rham complexes of globally\nsubanalytic differential forms and of constructible differential forms. Whereas\nthe de Rham theorem does not hold for the former in the non-compact case, it\ndoes hold for the latter in full generality. We deduce that the constructible\nde Rham cohomology groups are canonically isomorphic to the classical ones. We\nstress that our results apply already in the $C^1$-setting."}
{"id": "2508.02904", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02904", "abs": "https://arxiv.org/abs/2508.02904", "authors": ["Zhong Zhang", "Xiang Guo", "Di Wu", "Hexi Baoyin", "Junfeng Li", "Francesco Topputo"], "title": "Global Optimality in Multi-Flyby Asteroid Trajectory Optimization: Theory and Application Techniques", "comment": null, "summary": "Designing optimal trajectories for multi-flyby asteroid missions is\nscientifically critical but technically challenging due to nonlinear dynamics,\nintermediate constraints, and numerous local optima. This paper establishes a\nmethod that approaches global optimality for multi-flyby trajectory\noptimization under a given sequence. The original optimal control problem with\ninterior-point equality constraints is transformed into a multi-stage decision\nformulation. This reformulation enables direct application of dynamic\nprogramming in lower dimensions, and follows Bellman's principle of optimality.\nMoreover, the method provides a quantifiable bound on global optima errors\nintroduced by discretization and approximation assumptions, thus ensuring a\nmeasure of confidence in the obtained solution. The method accommodates both\nimpulsive and low-thrust maneuver schemes in rendezvous and flyby scenarios.\nSeveral computational techniques are introduced to enhance efficiency,\nincluding a specialized solution for bi-impulse cases and an adaptive step\nrefinement strategy. The proposed method is validated through three problems:\n1) an impulsive variant of the fourth Global Trajectory Optimization\ncompetition problem (GTOC4), 2) the GTOC11 problem, and 3) the original\nlow-thrust GTOC4 problem. Each case demonstrates improvements in fuel\nconsumption over the best-known trajectories. These results give evidence of\nthe generality and effectiveness of the proposed method in global trajectory\noptimization."}
{"id": "2508.02688", "categories": ["math.NT", "11D61, 11D72, 11B39"], "pdf": "https://arxiv.org/pdf/2508.02688", "abs": "https://arxiv.org/abs/2508.02688", "authors": ["Japhet Odjoumani"], "title": "Narayana numbers that are products of two Fibonacci numbers", "comment": null, "summary": "Let $\\{N_m\\}_{m\\ge0}$ be the Narayana's cows sequence given by $N_0=0$,\n$N_1=1=N_2=1$ and \\[ N_{m+3}=N_{m+2}+N_m,\\quad \\text{ for }\\; m\\geq 0 \\] and\nlet $\\{F_n\\}_{n\\ge0}$ be the Fibonacci sequence. In this paper we solve\nexplicitely the Diophantine equation \\[ N_m=F_nF_k, \\] in positive unknowns\n$m,\\,n$ and $k$. That is, we find the non-zero narayana numbers that are\nproducts of two Fibonacci numbers."}
{"id": "2508.02763", "categories": ["math.ST", "cs.NA", "math.NA", "math.PR", "stat.CO", "stat.TH", "Primary: 60J22, Secondary: 65C05, 65C40, 60J05, 60K35"], "pdf": "https://arxiv.org/pdf/2508.02763", "abs": "https://arxiv.org/abs/2508.02763", "authors": ["Ruiyu Han", "Gautam Iyer", "Dejan Slepčev"], "title": "Polynomial complexity sampling from multimodal distributions using Sequential Monte Carlo", "comment": "58 pages, 5 figures", "summary": "We study a sequential Monte Carlo algorithm to sample from the Gibbs measure\nwith a non-convex energy function at a low temperature. We use the practical\nand popular geometric annealing schedule, and use a Langevin diffusion at each\ntemperature level. The Langevin diffusion only needs to run for a time that is\nlong enough to ensure local mixing within energy valleys, which is much shorter\nthan the time required for global mixing. Our main result shows convergence of\nMonte Carlo estimators with time complexity that, approximately, scales like\nthe forth power of the inverse temperature, and the square of the inverse\nallowed error. We also study this algorithm in an illustrative model scenario\nwhere more explicit estimates can be given."}
{"id": "2508.02804", "categories": ["math.CO", "math.PR", "05C81 (primary), 05C05 (secondary), 60J10 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.02804", "abs": "https://arxiv.org/abs/2508.02804", "authors": ["Andrew Beveridge", "Ben Bridenbaugh", "Ari Holcombe Pomerance"], "title": "Random Walks and the Meeting Time for Trees", "comment": "27 pages, 5 figures", "summary": "Consider a random walk on a tree $G=(V,E)$. For $v,w \\in V$, let the hitting\ntime $H(v,w)$ denote the expected number of steps required for the random walk\nstarted at $v$ to reach $w$, and let $\\pi_v = \\mathrm{deg}(v)/2|E|$ denote the\nstationary distribution for the random walk. We characterize the extremal tree\nstructures for the meeting time $T_{\\mathrm{meet}}(G) = \\max_{w \\in V} \\sum_{v\n\\in V} \\pi_v H(v,w)$. For fixed order $n$ and diameter $d$, the meeting time is\nmaximized by the broom graph. The meeting time is minimized by the balanced\ndouble broom graph, or a slight variant, depending on the relative parities of\n$n$ and $d$."}
{"id": "2508.03361", "categories": ["cs.DM", "math.CO", "math.PR", "05C80, 68R10, 05C38"], "pdf": "https://arxiv.org/pdf/2508.03361", "abs": "https://arxiv.org/abs/2508.03361", "authors": ["Samuel Baguley", "Andreas Göbel", "Nicolas Klodt", "George Skretas", "John Sylvester", "Viktor Zamaraev"], "title": "Temporal Exploration of Random Spanning Tree Models", "comment": "42 pages, 8 Figures", "summary": "The Temporal Graph Exploration problem (TEXP) takes as input a temporal\ngraph, i.e., a sequence of graphs $(G_i)_{i\\in \\mathbb{N}}$ on the same vertex\nset, and asks for a walk of shortest length visiting all vertices, where the\n$i$-th step uses an edge from $G_i$. If each such $G_i$ is connected, then an\nexploration of length $n^2$ exists, and this is known to be the best possible\nup to a constant. More fine-grained lower and upper bounds have been obtained\nfor restricted temporal graph classes, however, for several fundamental\nclasses, a large gap persists between known bounds, and it remains unclear\nwhich properties of a temporal graph make it inherently difficult to explore.\n  Motivated by this limited understanding and the central role of the Temporal\nGraph Exploration problem in temporal graph theory, we study the problem in a\nrandomised setting. We introduce the Random Spanning Tree (RST) model, which\nconsists of a set of $n$-vertex trees together with an arbitrary probability\ndistribution $\\mu$ over this set. A random temporal graph generated by the RST\nmodel is a sequence of independent samples drawn from $\\mu$.\n  We initiate a systematic study of the Temporal Graph Exploration problem in\nsuch random temporal graphs and establish tight general bounds on exploration\ntime. Our first main result proves that any RST model can, with high\nprobability (w.h.p.), be explored in $O(n^{3/2})$ time, and we show that this\nbound is tight up to a constant factor. This demonstrates a fundamental\ndifference between the adversarial and random settings. Our second main result\nshows that if all trees of an RST are subgraphs of a fixed graph with $m$ edges\nthen, w.h.p.\\ , it can be explored in $O(m)$ time."}
{"id": "2508.02805", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02805", "abs": "https://arxiv.org/abs/2508.02805", "authors": ["Jean Michel Tine", "Mohammed Aldeen", "Abyad Enan", "M Sabbir Salek", "Long Cheng", "Mashrur Chowdhury"], "title": "Real-World Evaluation of Protocol-Compliant Denial-of-Service Attacks on C-V2X-based Forward Collision Warning Systems", "comment": "This paper was submitted to the Transportation Research Board (TRB)\n  2026 and is under review", "summary": "Cellular Vehicle-to-Everything (C-V2X) technology enables low-latency,\nreliable communications essential for safety applications such as a Forward\nCollision Warning (FCW) system. C-V2X deployments operate under strict protocol\ncompliance with the 3rd Generation Partnership Project (3GPP) and the Society\nof Automotive Engineers Standard (SAE) J2735 specifications to ensure\ninteroperability. This paper presents a real-world testbed evaluation of\nprotocol-compliant Denial-of-Service (DoS) attacks using User Datagram Protocol\n(UDP) flooding and oversized Basic Safety Message (BSM) attacks that 7 exploit\ntransport- and application-layer vulnerabilities in C-V2X. The attacks\npresented in this study transmit valid messages over standard PC5 sidelinks,\nfully adhering to 3GPP and SAE J2735 specifications, but at abnormally high\nrates and with oversized payloads that overload the receiver resources without\nbreaching any protocol rules such as IEEE 1609. Using a real-world connected\nvehicle 11 testbed with commercially available On-Board Units (OBUs), we\ndemonstrate that high-rate UDP flooding and oversized payload of BSM flooding\ncan severely degrade FCW performance. Results show that UDP flooding alone\nreduces packet delivery ratio by up to 87% and increases latency to over 400ms,\nwhile oversized BSM floods overload receiver processing resources, delaying or\ncompletely suppressing FCW alerts. When UDP and BSM attacks are executed\nsimultaneously, they cause near-total communication failure, preventing FCW\nwarnings entirely. These findings reveal that protocol-compliant communications\ndo not necessarily guarantee safe or reliable operation of C-V2X-based safety\napplications."}
{"id": "2508.02694", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions."}
{"id": "2508.02971", "categories": ["q-fin.MF", "q-fin.PR", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2508.02971", "abs": "https://arxiv.org/abs/2508.02971", "authors": ["Srisht Fateh Singh", "Reina Ke Xin Li", "Samuel Gaskin", "Yuntao Wu", "Jeffrey Klinck", "Panagiotis Michalopoulos", "Zissis Poulos", "Andreas Veneris"], "title": "Modeling Loss-Versus-Rebalancing in Automated Market Makers via Continuous-Installment Options", "comment": null, "summary": "This paper mathematically models a constant-function automated market maker\n(CFAMM) position as a portfolio of exotic options, known as perpetual American\ncontinuous-installment (CI) options. This model replicates an AMM position's\ndelta at each point in time over an infinite time horizon, thus taking into\naccount the perpetual nature and optionality to withdraw of liquidity\nprovision. This framework yields two key theoretical results: (a) It proves\nthat the AMM's adverse-selection cost, loss-versus-rebalancing (LVR), is\nanalytically identical to the continuous funding fees (the time value decay or\ntheta) earned by the at-the-money CI option embedded in the replicating\nportfolio. (b) A special case of this model derives an AMM liquidity position's\ndelta profile and boundaries that suffer approximately constant LVR, up to a\nbounded residual error, over an arbitrarily long forward window. Finally, the\npaper describes how the constant volatility parameter required by the perpetual\noption can be calibrated from the term structure of implied volatilities and\nestimates the errors for both implied volatility calibration and LVR residual\nerror. Thus, this work provides a practical framework enabling liquidity\nproviders to choose an AMM liquidity profile and price boundaries for an\narbitrarily long, forward-looking time window where they can expect an\napproximately constant, price-independent LVR. The results establish a rigorous\noption-theoretic interpretation of AMMs and their LVR, and provide actionable\nguidance for liquidity providers in estimating future adverse-selection costs\nand optimizing position parameters."}
{"id": "2508.02920", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02920", "abs": "https://arxiv.org/abs/2508.02920", "authors": ["Zhong Zhang", "Niccolò Michelotti", "Gonçalo Oliveira Pinho", "Francesco Topputo"], "title": "A Comparative Study of Optimal Control and Neural Networks in Asteroid Rendezvous Mission Analysis", "comment": null, "summary": "This paper presents a comparative study of the applicability and accuracy of\noptimal control methods and neural network-based estimators in the context of\nporkchop plots for preliminary asteroid rendezvous mission design. The scenario\nconsidered involves a deep-space CubeSat equipped with a low-thrust engine,\ndeparting from Earth and rendezvousing with a near-Earth asteroid within a\nthree-year launch window. A low-thrust trajectory optimization model is\nformulated, incorporating variable specific impulse, maximum thrust, and path\nconstraints. The optimal control problem is efficiently solved using Sequential\nConvex Programming (SCP) combined with a solution continuation strategy. The\nneural network framework consists of two models: one predicts the minimum fuel\nconsumption ($\\Delta v$), while the other estimates the minimum flight time\n($\\Delta t$) which is used to assess transfer feasibility. Case results\ndemonstrate that, in simplified scenarios without path constraints, the neural\nnetwork approach achieves low relative errors across most of the design space\nand successfully captures the main structural features of the porkchop plots.\nIn cases where the SCP-based continuation method fails due to the presence of\nmultiple local optima, the neural network still provides smooth and globally\nconsistent predictions, significantly improving the efficiency of early-stage\nasteroid candidate screening. However, the deformation of the feasible region\ncaused by path constraints leads to noticeable discrepancies in certain\nboundary regions, thereby limiting the applicability of the network in detailed\nmission design phases. Overall, the integration of neural networks with\nporkchop plot analysis offers a effective decision-making tool for mission\ndesigners and planetary scientists, with significant potential for engineering\napplications."}
{"id": "2508.02690", "categories": ["math.NT", "math.HO", "11N05 (Primary), 11-03 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.02690", "abs": "https://arxiv.org/abs/2508.02690", "authors": ["Benoit Cloitre"], "title": "An effective analytic recurrence for prime numbers: from asymptotics to explicit bounds", "comment": "12 pages, 1 figure. A survey and a new contribution to the study of\n  recurrence formulas for prime numbers", "summary": "We present an explicit and effective recurrence formula for prime numbers,\nbridging arithmetic and analytic approaches. Building upon foundational work by\nGandhi (1971), Golomb (1976), and Keller (2007), we establish the effective\nbound $s_n \\le 2p_n$ for all $n \\ge 1$ within the Golomb-Keller analytic\nrecurrence. This transforms their asymptotic formula into an explicit\nrecurrence using twice the n-th prime as the exponent: $$ p_{n+1} = \\left\\lceil\n\\left( -1 + \\zeta(2p_n) \\prod_{j=1}^{n} \\left(1 - \\frac{1}{p_j^{2p_n}}\\right)\n\\right)^{-1/(2p_n)} \\right\\rceil $$ The proof is self-contained and relies on\nBertrand's postulate. We also present strong numerical and heuristic evidence\nfor a sharper conjecture: $s_n \\le p_n$ for all $n \\ge 1$, suggesting that the\nformula works with the n-th prime as the exponent."}
{"id": "2508.03617", "categories": ["math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.03617", "abs": "https://arxiv.org/abs/2508.03617", "authors": ["Robert Richardson", "H. Dennis Tolley", "Kenneth Kuttler"], "title": "Expanding the Standard Diffusion Process to Specified Non-Gaussian Marginal Distributions", "comment": null, "summary": "We develop a class of non-Gaussian translation processes that extend\nclassical stochastic differential equations (SDEs) by prescribing arbitrary\nabsolutely continuous marginal distributions. Our approach uses a copula-based\ntransformation to flexibly model skewness, heavy tails, and other non-Gaussian\nfeatures often observed in real data. We rigorously define the process,\nestablish key probabilistic properties, and construct a corresponding diffusion\nmodel via stochastic calculus, including proofs of existence and uniqueness. A\nsimplified approximation is introduced and analyzed, with error bounds derived\nfrom asymptotic expansions. Simulations demonstrate that both the full and\nsimplified models recover target marginals with high accuracy. Examples using\nthe Student's t, asymmetric Laplace, and Exponentialized Generalized Beta of\nthe Second Kind (EGB2) distributions illustrate the flexibility and\ntractability of the framework."}
{"id": "2508.02813", "categories": ["math.CO", "math.PR", "05C80, 60B20, 94B05"], "pdf": "https://arxiv.org/pdf/2508.02813", "abs": "https://arxiv.org/abs/2508.02813", "authors": ["Remco van der Hofstad", "Noela Müller", "Haodong Zhu"], "title": "The asymptotic rank of adjacency matrices of weighted configuration models over arbitrary fields", "comment": "66 pages, 1 figure", "summary": "We study the asymptotic rank of adjacency matrices of a large class of\nedge-weighted configuration models. Here, the weight of a (multi-)edge can be\nany fixed non-zero element from an arbitrary field, as long as it is\nindependent of the (multi-)graph. Our main result demonstrates that the\nasymptotic behavior of the normalized rank of the adjacency matrix neither\ndepends on the fixed edge-weights, nor on which field they are chosen from. Our\napproach relies on a novel adaptation of the component exploration method of\n\\cite{janson2009new}, which enables the application of combinatorial techniques\nfrom \\cite{coja2022rank, HofMul25}."}
{"id": "2508.03549", "categories": ["cs.DM", "math.CO", "05C15, 68R10", "G.2.1; G.2.2"], "pdf": "https://arxiv.org/pdf/2508.03549", "abs": "https://arxiv.org/abs/2508.03549", "authors": ["Diptimaya Behera", "Mathew C. Francis", "Sreejith K. Pallathumadam"], "title": "Adjacent vertex distinguishing total coloring of 3-degenerate graphs", "comment": null, "summary": "A total coloring of a simple undirected graph $G$ is an assignment of colors\nto its vertices and edges such that the colors given to the vertices form a\nproper vertex coloring, the colors given to the edges form a proper edge\ncoloring, and the color of every edge is different from that of its two\nendpoints. That is, $\\phi:V(G)\\cup E(G)\\rightarrow\\mathbb{N}$ is a total\ncoloring of $G$ if $\\phi(u)\\neq\\phi(v)$ and $\\phi(uv)\\neq\\phi(u)$ for all\n$uv\\in E(G)$, and $\\phi(uv)\\neq\\phi(uw)$ for any $u \\in V(G)$ and distinct $v,w\n\\in N(u)$ (here, $N(u)$ denotes the set of neighbours of $u$). A total coloring\n$\\phi$ of a graph $G$ is said to be ``Adjacent Vertex Distinguishing'' (or AVD\nfor short) if for all $uv\\in E(G)$, we have that $\\phi(\\{u\\}\\cup\\{uw:w\\in\nN(u)\\})\\neq\\phi(\\{v\\}\\cup\\{vw\\colon w\\in N(v)\\})$. The AVD Total Coloring\nConjecture of Zhang, Chen, Li, Yao, Lu, and Wang (Science in China Series A:\nMathematics, 48(3):289--299, 2005) states that every graph $G$ has an AVD total\ncoloring using at most $\\Delta(G)+3$ colors, where $\\Delta(G)$ denotes the\nmaximum degree of $G$. For some $s\\in\\mathbb{N}$, a graph $G$ is said to be\n$s$-degenerate if every subgraph of $G$ has minimum degree at most $s$. Miao,\nShi, Hu, and Luo (Discrete Mathematics, 339(10):2446--2449, 2016) showed that\nthe AVD Total Coloring Conjecture is true for 2-degenerate graphs. We verify\nthe conjecture for 3-degenerate graphs."}
{"id": "2508.02816", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.02816", "abs": "https://arxiv.org/abs/2508.02816", "authors": ["Dylan Stow", "Russell Barnes", "Eren Kurshan", "Yuan Xie"], "title": "Thermal-Aware 3D Design for Side-Channel Information Leakage", "comment": null, "summary": "Side-channel attacks are important security challenges as they reveal\nsensitive information about on-chip activities. Among such attacks, the thermal\nside-channel has been shown to disclose the activities of key functional blocks\nand even encryption keys. This paper proposes a novel approach to proactively\nconceal critical activities in the functional layers while minimizing the power\ndissipation by (i) leveraging inherent characteristics of 3D integration to\nprotect from side-channel attacks and (ii) dynamically generating custom\nactivity patterns to match the activity to be concealed in the functional\nlayers. Experimental analysis shows that 3D technology combined with the\nproposed run-time algorithm effectively reduces the Side channel vulnerability\nFactor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)\nbelow 0.59."}
{"id": "2508.02697", "categories": ["cs.AI", "cs.CE", "03B35 (Primary) 03A99, 03B10, 03B25, 68V15, 03C07 (Secondary)", "I.2.3; I.2.4; F.4.1; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.02697", "abs": "https://arxiv.org/abs/2508.02697", "authors": ["Mikhail Soutchanski", "Yongmei Liu"], "title": "Planning with Dynamically Changing Domains", "comment": "A revised version of the paper accepted to the 1st International\n  Workshop on Trends in Knowledge Representation and Reasoning organized as a\n  IJCAI 2025 workshop that takes place in August 2025 in Montreal, Canada. See\n  the details at https://tkr2025.krportal.org/programme.html", "summary": "In classical planning and conformant planning, it is assumed that there are\nfinitely many named objects given in advance, and only they can participate in\nactions and in fluents. This is the Domain Closure Assumption (DCA). However,\nthere are practical planning problems where the set of objects changes\ndynamically as actions are performed; e.g., new objects can be created, old\nobjects can be destroyed. We formulate the planning problem in first-order\nlogic, assume an initial theory is a finite consistent set of fluent literals,\ndiscuss when this guarantees that in every situation there are only finitely\nmany possible actions, impose a finite integer bound on the length of the plan,\nand propose to organize search over sequences of actions that are grounded at\nplanning time. We show the soundness and completeness of our approach. It can\nbe used to solve the bounded planning problems without DCA that belong to the\nintersection of sequential generalized planning (without sensing actions) and\nconformant planning, restricted to the case without the disjunction over fluent\nliterals. We discuss a proof-of-the-concept implementation of our planner."}
{"id": "2508.02969", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02969", "abs": "https://arxiv.org/abs/2508.02969", "authors": ["Mingze Li", "Lei Fan", "Zhu Han"], "title": "Quantum Hamiltonian Descent based Augmented Lagrangian Method for Constrained Nonconvex Nonlinear Optimization", "comment": null, "summary": "Nonlinear programming (NLP) plays a critical role in domains such as power\nenergy systems, chemical engineering, communication networks, and financial\nengineering. However, solving large-scale, nonconvex NLP problems remains a\nsignificant challenge due to the complexity of the solution landscape and the\npresence of nonlinear nonconvex constraints. In this paper, we develop a\nQuantum Hamiltonian Descent based Augmented Lagrange Method (QHD-ALM) framework\nto address largescale, constrained nonconvex NLP problems. The augmented\nLagrange method (ALM) can convert a constrained NLP to an unconstrained NLP,\nwhich can be solved by using Quantum Hamiltonian Descent (QHD). To run the QHD\non a classical machine, we propose to use the Simulated Bifurcation algorithm\nas the engine to simulate the dynamic process. We apply our algorithm to a\nPower-to-Hydrogen System, and the simulation results verify the effectiveness\nof our algorithm."}
{"id": "2508.02701", "categories": ["math.NT", "11M06, 11N37"], "pdf": "https://arxiv.org/pdf/2508.02701", "abs": "https://arxiv.org/abs/2508.02701", "authors": ["Vitalii V. Iudelevich"], "title": "On the sum-of-squares function", "comment": "81 pages, the paper accepted for publication in the journal Izvestiya\n  RAN", "summary": "In this paper, we derive the following asymptotic formula $$\n\\mathop{{\\sum}'}_{n\\leqslant x}\\dfrac{r(n)}{r(n+1)} = {x}{(\\ln\nx)^{-3/4}}(c+o(1)),\\ \\ x \\to +\\infty,$$ where $r(n)$ is the number of\nrepresentations of $n$ as a sum of two squares, $c$ is a positive constant, and\nthe prime indicates summation over those $n$ for which $r(n+1)\\neq 0$."}
{"id": "2508.02891", "categories": ["math.CO", "hep-th", "math-ph", "math.AG", "math.MP", "13F60, 05E14, 14M15, 81T99"], "pdf": "https://arxiv.org/pdf/2508.02891", "abs": "https://arxiv.org/abs/2508.02891", "authors": ["Chaim Even-Zohar", "Matteo Parisi", "Melissa Sherman-Bennett", "Ran Tessler", "Lauren Williams"], "title": "Plabic Tangles and Cluster Promotion Maps", "comment": "80 pages, 13 figures", "summary": "Inspired by the BCFW recurrence for tilings of the amplituhedron, we\nintroduce the general framework of `plabic tangles' that utilizes plabic graphs\nto define rational maps between products of Grassmannians called `promotions'.\nThe central conjecture of the paper is that promotion maps are quasi-cluster\nhomomorphisms, which we prove for several classes of promotions. In order to\ndefine promotion maps, we utilize $m$-vector-relation configurations ($m$-VRCs)\non plabic graphs. We relate $m$-VRCs to the degree (a.k.a `intersection\nnumber') of the amplituhedron map on positroid varieties and characterize all\nplabic trees with intersection number one and their VRCs. Finally, we show that\npromotion maps admit an operad structure and, supported by the class of\n`$4$-mass box' promotion, we point at new positivity properties for\nnon-rational maps beyond cluster algebras. Promotion maps have important\nconnections to the geometry and cluster structure of the amplituhedron and\nsingularities of scattering amplitudes in planar $\\mathcal{N}=4$ super\nYang-Mills theory."}
{"id": "2508.02980", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.02980", "abs": "https://arxiv.org/abs/2508.02980", "authors": ["Júlio Araújo", "Nicolas Nisse", "Lucas Picasarri-Arrieta"], "title": "Backbone colouring of chordal graphs", "comment": null, "summary": "A proper $k$-colouring of a graph $G=(V,E)$ is a function $c: V(G)\\to\n\\{1,\\ldots,k\\}$ such that $c(u)\\neq c(v)$ for every edge $uv\\in E(G)$. The\nchromatic number $\\chi(G)$ is the minimum $k$ such that there exists a proper\n$k$-colouring of $G$. Given a spanning subgraph $H$ of $G$, a $q$-backbone\n$k$-colouring of $(G,H)$ is a proper $k$-colouring $c$ of $G$ such that $\\lvert\nc(u)-c(v)\\rvert \\ge q$ for every edge $uv\\in E(H)$. The $q$-backbone chromatic\nnumber ${\\rm BBC}_q(G,H)$ is the smallest $k$ for which there exists a\n$q$-backbone $k$-colouring of $(G,H)$. In their seminal paper, Broersma et\nal.~\\cite{BFGW07} ask whether, for any chordal graph $G$ and any spanning\nforest $H$ of $G$, we have that ${\\rm BBC}_2(G,H)\\leq \\chi(G)+O(1)$.\n  In this work, we first show that this is true as long as $H$ is bipartite and\n$G$ is an interval graph in which each vertex belongs to at most two maximal\ncliques. We then show that this does not extend to bipartite graphs as backbone\nby exhibiting a family of chordal graphs $G$ with spanning bipartite subgraphs\n$H$ satisfying ${\\rm BBC}_2(G,H)\\geq \\frac{5\\chi(G)}{3}$. Then, we show that if\n$G$ is chordal and $H$ has bounded maximum average degree (in particular, if\n$H$ is a forest), then ${\\rm BBC}_2(G,H)\\leq \\chi(G)+O(\\sqrt{\\chi(G)})$. We\nfinally show that ${\\rm BBC}_2(G,H)\\leq \\frac{3}{2}\\chi(G)+O(1)$ holds whenever\n$G$ is chordal and $H$ is $C_4$-free."}
{"id": "2508.02836", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02836", "abs": "https://arxiv.org/abs/2508.02836", "authors": ["Mengyu Zhang", "Zhuotao Liu", "Jingwen Huang", "Xuanqi Liu"], "title": "Agentic Privacy-Preserving Machine Learning", "comment": null, "summary": "Privacy-preserving machine learning (PPML) is critical to ensure data privacy\nin AI. Over the past few years, the community has proposed a wide range of\nprovably secure PPML schemes that rely on various cryptography primitives.\nHowever, when it comes to large language models (LLMs) with billions of\nparameters, the efficiency of PPML is everything but acceptable. For instance,\nthe state-of-the-art solution for confidential LLM inference represents at\nleast 10,000-fold slower performance compared to plaintext inference. The\nperformance gap is even larger when the context length increases. In this\nposition paper, we propose a novel framework named Agentic-PPML to make PPML in\nLLMs practical. Our key insight is to employ a general-purpose LLM for intent\nunderstanding and delegate cryptographically secure inference to specialized\nmodels trained on vertical domains. By modularly separating language intent\nparsing - which typically involves little or no sensitive information - from\nprivacy-critical computation, Agentic-PPML completely eliminates the need for\nthe LLMs to process the encrypted prompts, enabling practical deployment of\nprivacy-preserving LLM-centric services."}
{"id": "2508.02734", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02734", "abs": "https://arxiv.org/abs/2508.02734", "authors": ["Weiyu Luo", "Chenfeng Xiong"], "title": "Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model", "comment": "20 pages, 5 figures", "summary": "Location-Based Service (LBS) data provides critical insights into human\nmobility, yet its sparsity often yields incomplete trip and activity sequences,\nmaking accurate inferences about trips and activities difficult. We raise a\nresearch problem: Can we use activity sequences derived from high-quality LBS\ndata to recover incomplete activity sequences at the individual level? This\nstudy proposes a new solution, the Variable Selection Network-fused Insertion\nTransformer (VSNIT), integrating the Insertion Transformer's flexible sequence\nconstruction with the Variable Selection Network's dynamic covariate handling\ncapability, to recover missing segments in incomplete activity sequences while\npreserving existing data. The findings show that VSNIT inserts more diverse,\nrealistic activity patterns, more closely matching real-world variability, and\nrestores disrupted activity transitions more effectively aligning with the\ntarget. It also performs significantly better than the baseline model across\nall metrics. These results highlight VSNIT's superior accuracy and diversity in\nactivity sequence recovery tasks, demonstrating its potential to enhance LBS\ndata utility for mobility analysis. This approach offers a promising framework\nfor future location-based research and applications."}
{"id": "2508.03048", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2508.03048", "abs": "https://arxiv.org/abs/2508.03048", "authors": ["Chang He", "Jiaxiang Li", "Bo Jiang", "Shiqian Ma", "Shuzhong Zhang"], "title": "On Relatively Smooth Optimization over Riemannian Manifolds", "comment": null, "summary": "We study optimization over Riemannian embedded submanifolds, where the\nobjective function is relatively smooth in the ambient Euclidean space. Such\nproblems have broad applications but are still largely unexplored. We introduce\ntwo Riemannian first-order methods, namely the retraction-based and\nprojection-based Riemannian Bregman gradient methods, by incorporating the\nBregman distance into the update steps. The retraction-based method can handle\nnonsmooth optimization; at each iteration, the update direction is generated by\nsolving a convex optimization subproblem constrained to the tangent space. We\nshow that when the reference function is of the quartic form $h(x) =\n\\frac{1}{4}\\|x\\|^4 + \\frac{1}{2}\\|x\\|^2$, the constraint subproblem admits a\nclosed-form solution. The projection-based approach can be applied to smooth\nRiemannian optimization, which solves an unconstrained subproblem in the\nambient Euclidean space. Both methods are shown to achieve an iteration\ncomplexity of $\\mathcal{O}(1/\\epsilon^2)$ for finding an $\\epsilon$-approximate\nRiemannian stationary point. When the manifold is compact, we further develop\nstochastic variants and establish a sample complexity of\n$\\mathcal{O}(1/\\epsilon^4)$. Numerical experiments on the nonlinear eigenvalue\nproblem and low-rank quadratic sensing problem demonstrate the advantages of\nthe proposed methods."}
{"id": "2508.02722", "categories": ["math.NT", "12D10 05E05 11C08"], "pdf": "https://arxiv.org/pdf/2508.02722", "abs": "https://arxiv.org/abs/2508.02722", "authors": ["Laura De Carli", "Maurizio Laporta"], "title": "Divisibility criteria and coefficient formulas for cyclotomic polynomials", "comment": null, "summary": "We establish necessary and sufficient conditions for a polynomial to be\ndivisible by a cyclotomic polynomials and derive new formulas involving\nRamanujan sums as an application of our results. Additionally, we provide new\ninsights into the coefficients of cyclotomic polynomials and we propose a\nrecursive relation between the coefficients of two cyclotomic polynomials whose\nindexes differ by a prime factor."}
{"id": "2508.02907", "categories": ["math.CO", "math.AG"], "pdf": "https://arxiv.org/pdf/2508.02907", "abs": "https://arxiv.org/abs/2508.02907", "authors": ["Matthew Baker", "June Huh", "Mario Kummer", "Oliver Lorscheid"], "title": "Lorentzian polynomials and matroids over triangular hyperfields 1: Topological aspects", "comment": null, "summary": "Lorentzian polynomials serve as a bridge between continuous and discrete\nconvexity, connecting analysis and combinatorics. In this article, we study the\ntopology of the space $\\mathbb{P}\\textrm{L}_J$ of Lorentzian polynomials on $J$\nmodulo $\\mathbb{R}_{>0}$, which is nonempty if and only if $J$ is the set of\nbases of a polymatroid. We prove that $\\mathbb{P}\\textrm{L}_J$ is a manifold\nwith boundary of dimension equal to the Tutte rank of $J$, and more precisely,\nthat it is homeomorphic to a closed Euclidean ball with the Dressian of $J$\nremoved from its boundary. Furthermore, we show that $\\mathbb{P}\\textrm{L}_J$\nis homeomorphic to the thin Schubert cell $\\textrm{Gr}_J(\\mathbb{T}_q)$ of $J$\nover the triangular hyperfield $\\mathbb{T}_q$, introduced by Viro in the\ncontext of tropical geometry and Maslov dequantization, for any $q>0$. This\nidentification enables us to apply the representation theory of polymatroids\ndeveloped in a companion paper, as well as earlier work by the first and fourth\nauthors on foundations of matroids, to give a simple explicit description of\n$\\mathbb{P}\\textrm{L}_J$ up to homeomorphism in several key cases. Our results\nshow that $\\mathbb{P}\\textrm{L}_J$ always admits a compactification\nhomeomorphic to a closed Euclidean ball. They can also be used to answer a\nquestion of Br\\\"and\\'en in the negative by showing that the closure of\n$\\mathbb{P}\\textrm{L}_J$ within the space of all polynomials modulo\n$\\mathbb{R}_{>0}$ is not homeomorphic to a closed Euclidean ball in general. In\naddition, we introduce the Hausdorff compactification of the space of rescaling\nclasses of Lorentzian polynomials and show that the Chow quotient of a complex\nGrassmannian maps naturally to this compactification. This provides a geometric\nframework that connects the asymptotic structure of the space of Lorentzian\npolynomials with classical constructions in algebraic geometry."}
{"id": "2508.02985", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.02985", "abs": "https://arxiv.org/abs/2508.02985", "authors": ["Timothée Corsini", "Lucas Picasarri-Arrieta", "Théo Pierron", "François Pirot", "Eileen Robinson"], "title": "Chromatic discrepancy of locally $s$-colourable graphs", "comment": null, "summary": "The chromatic discrepancy of a graph $G$, denoted $\\phi(G)$, is the least\nover all proper colourings $\\sigma$ of $G$ of the greatest difference between\nthe number of colours $|\\sigma(V(H))|$ spanned by an induced subgraph $H$ of\n$G$ and its chromatic number $\\chi(H)$. We prove that the chromatic discrepancy\nof a triangle-free graph $G$ is at least $\\chi(G)-2$. This is best possible and\npositively answers a question raised by Aravind, Kalyanasundaram, Sandeep, and\nSivadasan.\n  More generally, we say that a graph $G$ is locally $s$-colourable if the\nclosed neighbourhood of any vertex $v\\in V(G)$ is properly $s$-colourable; in\nparticular, a triangle-free graph is locally $2$-colourable. We conjecture that\nevery locally $s$-colourable graph $G$ satisfies $\\phi(G) \\geq \\chi(G)-s$, and\nshow that this would be almost best possible. We prove the conjecture when\n$\\chi(G) \\le 11s/6$, and as a partial result towards the general case, we prove\nthat every locally $s$-colourable graph $G$ satisfies $\\phi(G) \\geq \\chi(G) -\ns\\ln \\chi(G)$.\n  If the conjecture holds, it implies in particular, for every integer\n$\\ell\\geq 2$, that any graph $G$ without any copy of $C_{\\ell+1}$, the cycle of\nlength $\\ell+1$, satisfies $\\phi(G) \\geq \\chi(G) - \\ell$. When $\\ell \\ge 3$ and\n$G\\neq K_\\ell$, we conjecture that we actually have $\\phi(G)\\ge \\chi(G) - \\ell\n+ 1$, and prove it in the special case $\\ell = 3$ or $\\chi(G) \\le 5\\ell/3$. In\ngeneral, we further obtain that every $C_{\\ell+1}$-free graph $G$ satisfies\n$\\phi(G) \\geq \\chi(G) - O_{\\ell}(\\ln \\ln \\chi(G))$. We do so by determining an\nalmost tight bound on the chromatic number of balls of radius at most $\\ell/2$\nin $G$, which could be of independent interest."}
{"id": "2508.02942", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02942", "abs": "https://arxiv.org/abs/2508.02942", "authors": ["Anas Mabrouk", "Mohamed Hatem", "Mohammad Mamun", "Sherif Saad"], "title": "LMDG: Advancing Lateral Movement Detection Through High-Fidelity Dataset Generation", "comment": null, "summary": "Lateral Movement (LM) attacks continue to pose a significant threat to\nenterprise security, enabling adversaries to stealthily compromise critical\nassets. However, the development and evaluation of LM detection systems are\nimpeded by the absence of realistic, well-labeled datasets. To address this\ngap, we propose LMDG, a reproducible and extensible framework for generating\nhigh-fidelity LM datasets. LMDG automates benign activity generation,\nmulti-stage attack execution, and comprehensive labeling of system and network\nlogs, dramatically reducing manual effort and enabling scalable dataset\ncreation. A central contribution of LMDG is Process Tree Labeling, a novel\nagent-based technique that traces all malicious activity back to its origin\nwith high precision. Unlike prior methods such as Injection Timing or\nBehavioral Profiling, Process Tree Labeling enables accurate, step-wise\nlabeling of malicious log entries, correlating each with a specific attack step\nand MITRE ATT\\&CK TTPs. To our knowledge, this is the first approach to support\nfine-grained labeling of multi-step attacks, providing critical context for\ndetection models such as attack path reconstruction. We used LMDG to generate a\n25-day dataset within a 25-VM enterprise environment containing 22 user\naccounts. The dataset includes 944 GB of host and network logs and embeds 35\nmulti-stage LM attacks, with malicious events comprising less than 1% of total\nactivity, reflecting a realistic benign-to-malicious ratio for evaluating\ndetection systems. LMDG-generated datasets improve upon existing ones by\noffering diverse LM attacks, up-to-date attack patterns, longer attack\ntimeframes, comprehensive data sources, realistic network architectures, and\nmore accurate labeling."}
{"id": "2508.02744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02744", "abs": "https://arxiv.org/abs/2508.02744", "authors": ["Peiran Wang", "Yaoning Yu", "Ke Chen", "Xianyang Zhan", "Haohan Wang"], "title": "Large Language Model-based Data Science Agent: A Survey", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science."}
{"id": "2508.03075", "categories": ["math.OC", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2508.03075", "abs": "https://arxiv.org/abs/2508.03075", "authors": ["Sergey Zaborsky"], "title": "Complete Integral of Primer-Vector Equations for Transfers in a Central Gravitational Field", "comment": "12 pages", "summary": "This paper demonstrates the existence of a complete integral for the system\nof differential equations of Lawden's primer-vector, which is used in the\noptimization of space transfers in a central gravitational field. The derived\ncomplete integral has been shown to significantly reduce the order of the\ndifferential system for the primer-vector from sixth to second, thereby\nsimplifying the optimization problem into a boundary value problem with four\nparameters. The presence of a complete integral enables the exclusion of the\ntransversality conditions, which introduce significant complexity to the\nboundary value problem. The problem of transfer optimization is considerably\nsimplified due to the existence of the full integral and generating solutions.\nThe analysis reveals that, depending on the given constraints, there are six\ntypes of optimization problems, each corresponding to a specific boundary value\nproblem."}
{"id": "2508.02761", "categories": ["math.NT", "11F33, 11F85"], "pdf": "https://arxiv.org/pdf/2508.02761", "abs": "https://arxiv.org/abs/2508.02761", "authors": ["Eunsu Hur"], "title": "Slopes of modular forms and the Ghost conjecture", "comment": "22 pages", "summary": "We give an algorithm to compute the slope sequence of modular forms with\nfixed Galois components from its first few entries, which is a refined version\nof the conjecture of [Buz05]. We use the results of arXiv:2302.07697 on the\nghost conjecture from axXiv:1710.01572. These symmetries in slope sequences\nhave potential implication to unexplained symmetries in many Coleman-Mazur\neigencurves."}
{"id": "2508.02939", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2508.02939", "abs": "https://arxiv.org/abs/2508.02939", "authors": ["Rachel Galindo", "Jessica McDonald", "Songling Shan"], "title": "Cliques and High Odd Holes in Graphs with Chromatic Number Equal to Maximum Degree", "comment": null, "summary": "We prove that if $G$ is a connected graph with $\\chi(G) = \\Delta(G)$ and\n$G\\neq \\overline{C_7}$, then $G$ contains either $K_{\\Delta(G)}$ or an odd hole\nwhere every vertex has degree at least $\\Delta(G)-1$ in $G$. This extends\nearlier work by Chen, Lan, Lin and Zhou, who proved the result for\n$\\Delta(G)\\geq 7$."}
{"id": "2508.03335", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.03335", "abs": "https://arxiv.org/abs/2508.03335", "authors": ["Neel Kaul", "David R. Wood"], "title": "On universal graphs for trees and treewidth $k$ graphs", "comment": null, "summary": "Let $s(n)$ be the minimum number of edges in a graph that contains every\n$n$-vertex tree as a subgraph. Chung and Graham [J. London Math. Soc. 1983]\nclaim to prove that $s(n)\\leqslant O(n\\log n)$. We point out a mistake in their\nproof. The previously best known upper bound is $s(n)\\leqslant O(n(\\log\nn)(\\log\\log n)^{2})$ by Chung, Graham and Pippenger [Proc. Hungarian Coll. on\nCombinatorics 1976], the proof of which is missing many crucial details. We\ngive a fully self-contained proof of the new and improved upper bound\n$s(n)\\leqslant O(n(\\log n)(\\log\\log n))$. The best known lower bound is\n$s(n)\\geqslant \\Omega(n\\log n)$.\n  We generalise these results for graphs of treewidth $k$. For an integer\n$k\\geqslant 1$, let $s_k(n)$ be the minimum number of edges in a graph that\ncontains every $n$-vertex graph with treewidth $k$ as a subgraph. So\n$s(n)=s_1(n)$. We show that $\\Omega(k n\\log n) \\leqslant s_k(n) \\leqslant\nO(kn(\\log n)(\\log\\log n))$."}
{"id": "2508.02943", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02943", "abs": "https://arxiv.org/abs/2508.02943", "authors": ["Baigang Chen", "Dongfang Zhao"], "title": "A Non-leveled and Reliable Approximate FHE Framework through Binarized Polynomial Rings", "comment": null, "summary": "Homomorphic encryption (HE) enables secure computation on encrypted data,\nsafeguarding user privacy in domains such as cloud computing, healthcare, and\nfinance. Among fully homomorphic encryption (FHE) schemes, CKKS is notable for\nsupporting approximate arithmetic over complex numbers, a key requirement for\nmachine-learning and numerical workloads. However, CKKS incurs rapid noise\ngrowth, complex parameter tuning, and relies on costly modulus switching. We\npropose a binary variant of CKKS that operates entirely over binary-coefficient\npolynomial rings and replaces rescaling with a lightweight bootstrapping\nmechanism. To mitigate additional bit-flip errors introduced by binary\nencoding, we integrate BCH error-correcting codes for robust decryption. Our\nopen-source implementation, built on the HElib library, preserves the core\nalgebraic structure of CKKS while introducing binary-coefficient encoding,\nenabling efficient evaluation in small ring dimensions and unbounded-depth\ncomputation. Empirical evaluations demonstrate the framework's practicality and\nscalability across a range of settings."}
{"id": "2508.02789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02789", "abs": "https://arxiv.org/abs/2508.02789", "authors": ["Newman Cheng", "Gordon Broadbent", "William Chappell"], "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science", "comment": null, "summary": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes."}
{"id": "2508.03160", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2508.03160", "abs": "https://arxiv.org/abs/2508.03160", "authors": ["Arash Khojaste", "Jonathan Pearce", "Golbon Zakeri", "Yuanrui Sang"], "title": "Electricity Price-Aware Scheduling of Data Center Cooling", "comment": null, "summary": "Data centers are becoming a major consumer of electricity on the grid, with\ncooling accounting for about 40\\% of that energy. As electricity prices vary\nthroughout the day and year, there is a need for cooling strategies that adapt\nto these fluctuations to reduce data center cooling costs. In this paper, we\npresent a model for electricity price-aware cooling scheduling using a Markov\nDecision Process(MDP) framework to reliably estimate the cooling system\noperational costs and facilitate investment-phase decision-making. We utilize\nQuantile Fourier Regression (QFR) fits to classify electricity prices into\ndifferent regimes while capturing both daily and seasonal patterns. We simulate\n14 years of operation using historical electricity price and outdoor\ntemperature data, and compare our model against heuristic baselines. The\nresults demonstrate that our approach consistently achieves lower cooling\ncosts. This model is useful for grid operators interested in demand response\nprograms and data center investors looking to make investment decisions."}
{"id": "2508.02769", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2508.02769", "abs": "https://arxiv.org/abs/2508.02769", "authors": ["Mizuki Akeno"], "title": "Small gaps between Goldbach primes", "comment": null, "summary": "We study small gaps between Goldbach primes $\\mathbb{P} \\cap (N-\\mathbb{P})$\nusing the Bombieri-Davenport method and the Maynard-Tao method, and compare the\ntwo.\n  We show that for almost all even integers $N$, the smallest gap in\n$\\mathbb{P} \\cap (N-\\mathbb{P})$ can be $0.765...$ times smaller than the\naverage gap using the Bombieri-Davenport method. This is an improvement on a\nrecent result of Tsuda. We also demonstrate that a straightforward application\nof the Maynard-Tao method is insufficient to improve this bound. However, it\nallows us to establish the existence of bounded gaps between Goldbach primes\nwith bounded error for almost all even integers $N$."}
{"id": "2508.02980", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.02980", "abs": "https://arxiv.org/abs/2508.02980", "authors": ["Júlio Araújo", "Nicolas Nisse", "Lucas Picasarri-Arrieta"], "title": "Backbone colouring of chordal graphs", "comment": null, "summary": "A proper $k$-colouring of a graph $G=(V,E)$ is a function $c: V(G)\\to\n\\{1,\\ldots,k\\}$ such that $c(u)\\neq c(v)$ for every edge $uv\\in E(G)$. The\nchromatic number $\\chi(G)$ is the minimum $k$ such that there exists a proper\n$k$-colouring of $G$. Given a spanning subgraph $H$ of $G$, a $q$-backbone\n$k$-colouring of $(G,H)$ is a proper $k$-colouring $c$ of $G$ such that $\\lvert\nc(u)-c(v)\\rvert \\ge q$ for every edge $uv\\in E(H)$. The $q$-backbone chromatic\nnumber ${\\rm BBC}_q(G,H)$ is the smallest $k$ for which there exists a\n$q$-backbone $k$-colouring of $(G,H)$. In their seminal paper, Broersma et\nal.~\\cite{BFGW07} ask whether, for any chordal graph $G$ and any spanning\nforest $H$ of $G$, we have that ${\\rm BBC}_2(G,H)\\leq \\chi(G)+O(1)$.\n  In this work, we first show that this is true as long as $H$ is bipartite and\n$G$ is an interval graph in which each vertex belongs to at most two maximal\ncliques. We then show that this does not extend to bipartite graphs as backbone\nby exhibiting a family of chordal graphs $G$ with spanning bipartite subgraphs\n$H$ satisfying ${\\rm BBC}_2(G,H)\\geq \\frac{5\\chi(G)}{3}$. Then, we show that if\n$G$ is chordal and $H$ has bounded maximum average degree (in particular, if\n$H$ is a forest), then ${\\rm BBC}_2(G,H)\\leq \\chi(G)+O(\\sqrt{\\chi(G)})$. We\nfinally show that ${\\rm BBC}_2(G,H)\\leq \\frac{3}{2}\\chi(G)+O(1)$ holds whenever\n$G$ is chordal and $H$ is $C_4$-free."}
{"id": "2508.03062", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03062", "abs": "https://arxiv.org/abs/2508.03062", "authors": ["Rourab Paul", "Paresh Baidya", "Krishnendu Guha"], "title": "Lightweight Fault Detection Architecture for NTT on FPGA", "comment": null, "summary": "Post-Quantum Cryptographic (PQC) algorithms are mathematically secure and\nresistant to quantum attacks but can still leak sensitive information in\nhardware implementations due to natural faults or intentional fault injections.\nThe intent fault injection in side-channel attacks reduces the reliability of\ncrypto implementation in future generation network security procesors. In this\nregard, this research proposes a lightweight, efficient, recomputation-based\nfault detection module implemented on a Field Programmable Gate Array (FPGA)\nfor Number Theoretic Transform (NTT). The NTT is primarily composed of memory\nunits and the Cooley-Tukey Butterfly Unit (CT-BU), a critical and\ncomputationally intensive hardware component essential for polynomial\nmultiplication. NTT and polynomial multiplication are fundamental building\nblocks in many PQC algorithms, including Kyber, NTRU, Ring-LWE, and others. In\nthis paper, we present a fault detection method called : Recomputation with a\nModular Offset (REMO) for the logic blocks of the CT-BU using Montgomery\nReduction and another method called Memory Rule Checkers for the memory\ncomponents used within the NTT. The proposed fault detection framework sets a\nnew benchmark by achieving high efficiency with significant low implementation\ncost. It occupies only 16 slices and a single DSP block, with a power\nconsumption of just 3mW in Artix-7 FPGA. The REMO-based detection mechanism\nachieves a fault coverage of 87.2% to 100%, adaptable across various word\nsizes, fault bit counts, and fault injection modes. Similarly, the Memory Rule\nCheckers demonstrate robust performance, achieving 50.7% to 100% fault\ndetection depending on and the nature of injected faults."}
{"id": "2508.02841", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02841", "abs": "https://arxiv.org/abs/2508.02841", "authors": ["Ziruo Yi", "Jinyu Liu", "Ting Xiao", "Mark V. Albert"], "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering", "comment": null, "summary": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning."}
{"id": "2508.03242", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2508.03242", "abs": "https://arxiv.org/abs/2508.03242", "authors": ["Kaijing Lyu", "Umberto Biccari", "Junmin Wang"], "title": "Robust stabilization of hyperbolic PDE-ODE systems via Neural Operator-approximated gain kernels", "comment": null, "summary": "This paper investigates the mean square exponential stabilization problem for\na class of coupled PDE-ODE systems with Markov jump parameters. The considered\nsystem consists of multiple coupled hyperbolic PDEs and a finite-dimensional\nODE, where all system parameters evolve according to a homogeneous\ncontinuous-time Markov process. The control design is based on a backstepping\napproach. To address the computational complexity of solving kernel equations,\na DeepONet framework is proposed to learn the mapping from system parameters to\nthe backstepping kernels. By employing Lyapunov-based analysis, we further\nprove that the controller obtained from the neural operator ensures stability\nof the closed-loop stochastic system. Numerical simulations demonstrate that\nthe proposed approach achieves more than two orders of magnitude speedup\ncompared to traditional numerical solvers, while maintaining high accuracy and\nensuring robust closed-loop stability under stochastic switching."}
{"id": "2508.02818", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2508.02818", "abs": "https://arxiv.org/abs/2508.02818", "authors": ["Tsz Ho Chan", "Laura Holmes", "Michael Liu", "Jose Villarreal"], "title": "Numbers with Four Close Factorizations", "comment": "20 pages, any comments are welcome", "summary": "In this paper, we study numbers $n$ that can be factored in four different\nways as $n = A B = (A + a_1) (B - b_1) = (A + a_2) (B - b_2) = (A + a_3) (B -\nb_3)$ with $B \\le A$, $1 \\le a_1 < a_2 < a_3 \\le C$ and $1 \\le b_1 < b_2 < b_3\n\\le C$. We obtain the optimal upper bound $A \\le 0.04742 \\ldots \\cdot C^3 +\nO(C)$. The key idea is to transform the original question into generalized Pell\nequations $a x^2 - b y^2 = c$ and study their solutions."}
{"id": "2508.02985", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.02985", "abs": "https://arxiv.org/abs/2508.02985", "authors": ["Timothée Corsini", "Lucas Picasarri-Arrieta", "Théo Pierron", "François Pirot", "Eileen Robinson"], "title": "Chromatic discrepancy of locally $s$-colourable graphs", "comment": null, "summary": "The chromatic discrepancy of a graph $G$, denoted $\\phi(G)$, is the least\nover all proper colourings $\\sigma$ of $G$ of the greatest difference between\nthe number of colours $|\\sigma(V(H))|$ spanned by an induced subgraph $H$ of\n$G$ and its chromatic number $\\chi(H)$. We prove that the chromatic discrepancy\nof a triangle-free graph $G$ is at least $\\chi(G)-2$. This is best possible and\npositively answers a question raised by Aravind, Kalyanasundaram, Sandeep, and\nSivadasan.\n  More generally, we say that a graph $G$ is locally $s$-colourable if the\nclosed neighbourhood of any vertex $v\\in V(G)$ is properly $s$-colourable; in\nparticular, a triangle-free graph is locally $2$-colourable. We conjecture that\nevery locally $s$-colourable graph $G$ satisfies $\\phi(G) \\geq \\chi(G)-s$, and\nshow that this would be almost best possible. We prove the conjecture when\n$\\chi(G) \\le 11s/6$, and as a partial result towards the general case, we prove\nthat every locally $s$-colourable graph $G$ satisfies $\\phi(G) \\geq \\chi(G) -\ns\\ln \\chi(G)$.\n  If the conjecture holds, it implies in particular, for every integer\n$\\ell\\geq 2$, that any graph $G$ without any copy of $C_{\\ell+1}$, the cycle of\nlength $\\ell+1$, satisfies $\\phi(G) \\geq \\chi(G) - \\ell$. When $\\ell \\ge 3$ and\n$G\\neq K_\\ell$, we conjecture that we actually have $\\phi(G)\\ge \\chi(G) - \\ell\n+ 1$, and prove it in the special case $\\ell = 3$ or $\\chi(G) \\le 5\\ell/3$. In\ngeneral, we further obtain that every $C_{\\ell+1}$-free graph $G$ satisfies\n$\\phi(G) \\geq \\chi(G) - O_{\\ell}(\\ln \\ln \\chi(G))$. We do so by determining an\nalmost tight bound on the chromatic number of balls of radius at most $\\ell/2$\nin $G$, which could be of independent interest."}
{"id": "2508.03067", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03067", "abs": "https://arxiv.org/abs/2508.03067", "authors": ["Jiewei Lai", "Lan Zhang", "Chen Tang", "Pengcheng Sun", "Xinming Wang", "Yunhao Wang"], "title": "Untraceable DeepFakes via Traceable Fingerprint Elimination", "comment": null, "summary": "Recent advancements in DeepFakes attribution technologies have significantly\nenhanced forensic capabilities, enabling the extraction of traces left by\ngenerative models (GMs) in images, making DeepFakes traceable back to their\nsource GMs. Meanwhile, several attacks have attempted to evade attribution\nmodels (AMs) for exploring their limitations, calling for more robust AMs.\nHowever, existing attacks fail to eliminate GMs' traces, thus can be mitigated\nby defensive measures. In this paper, we identify that untraceable DeepFakes\ncan be achieved through a multiplicative attack, which can fundamentally\neliminate GMs' traces, thereby evading AMs even enhanced with defensive\nmeasures. We design a universal and black-box attack method that trains an\nadversarial model solely using real data, applicable for various GMs and\nagnostic to AMs. Experimental results demonstrate the outstanding attack\ncapability and universal applicability of our method, achieving an average\nattack success rate (ASR) of 97.08\\% against 6 advanced AMs on DeepFakes\ngenerated by 9 GMs. Even in the presence of defensive mechanisms, our method\nmaintains an ASR exceeding 72.39\\%. Our work underscores the potential\nchallenges posed by multiplicative attacks and highlights the need for more\nrobust AMs."}
{"id": "2508.02900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02900", "abs": "https://arxiv.org/abs/2508.02900", "authors": ["Michael Katz", "Harsha Kokel", "Sarath Sreedharan"], "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game", "comment": null, "summary": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches."}
{"id": "2508.03459", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2508.03459", "abs": "https://arxiv.org/abs/2508.03459", "authors": ["Arsen Hnatiuk", "Daniel Walter"], "title": "Lazifying point insertion algorithms in spaces of measures", "comment": "37 pages", "summary": "Greedy point insertion algorithms have emerged as an attractive tool for the\nsolution of minimization problems over the space of Radon measures.\nConceptually, these methods can be split into two phases: first, the\ncomputation of a new candidate point via maximizing a continuous function over\nthe spatial domain, and second, updating the weights and/or support points of\nall Dirac-Deltas forming the iterate. Under additional structural assumptions\non the problem, full resolution of the subproblems in both steps guarantees an\nasymptotic linear rate of convergence for pure coefficient updates, or finite\nstep convergence, if, in addition, the position of all Dirac-Deltas is\noptimized. In the present paper, we lazify point insertion algorithms and allow\nfor the inexact solution of both subproblems based on computable error\nmeasures, while provably retaining improved theoretical convergence guarantees.\nAs a specific example, we present a new method with a quadratic rate of\nconvergence based on Newton steps for the weight-position pairs, which we\nglobalize by point-insertion as well as clustering steps."}
{"id": "2508.02821", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2508.02821", "abs": "https://arxiv.org/abs/2508.02821", "authors": ["Sudarshan Kumaresan", "Shipra Kumari", "Neha Mishra"], "title": "Redefining Euler-Rabinowitsch Polynomials with Heegner Number Based Quadratic Formulation", "comment": "39 pages, 4 figures", "summary": "This paper introduces a novel class of prime-generating quadratic polynomials\ndefined by $f_{Z,k,H}(n) = n^2 - (2Zk - 1)n + \\frac{(2Zk - 1)^2 + H}{4}$, where\n$Zk \\in \\mathbb{Z}_{\\geq 0}$ and $H$ belongs to the set of Heegner numbers.\nThis form is closely related to the Euler-Rabinowitsch polynomials through\nspecific substitutions. The structure enables algebraic tuning for prime-rich\noutputs and provides deeper insight into the impact of Heegner numbers on prime\ndistribution. Using tools such as the Bateman-Horn conjecture and\nprime-counting functions, we demonstrate that this family can be optimized to\ngenerate a high density of primes. This work offers new directions for research\nin analytic number theory and potential applications in cryptography and signal\nprocessing."}
{"id": "2508.03150", "categories": ["math.CO", "math.NT", "05E05, 11M32"], "pdf": "https://arxiv.org/pdf/2508.03150", "abs": "https://arxiv.org/abs/2508.03150", "authors": ["Wataru Takeda", "Yoshinori Yamasaki"], "title": "Quadratic relations for ninth variations of Schur functions and application to Schur multiple zeta functions", "comment": "26 pages", "summary": "Macdonald's ninth variation of Schur functions is a broad generalization of\nthe classical Schur function and its variants, defined via the Jacobi-Trudi\ndeterminant formula. In this paper, we establish various algebraic relations\nfor $S^{(r)}_{\\lambda/\\mu}(X)$, a class of the ninth variation introduced by\nNakagawa, Noumi, Shirakawa, and Yamada, by combining the Jacobi-Trudi formula\nwith determinant formulas such as the Desnanot-Jacobi adjoint matrix theorem\nand the Pl\\\"ucker relations, which generalize the corresponding relations for\nSchur functions. As an application, we investigate algebraic relations for\n\"diagonally constant\" Schur multiple zeta functions and examine their specific\nspecial values when the shape is rectangular."}
{"id": "2508.03097", "categories": ["cs.CR", "cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2508.03097", "abs": "https://arxiv.org/abs/2508.03097", "authors": ["Zixuan Gu", "Qiufeng Fan", "Long Sun", "Yang Liu", "Xiaojun Ye"], "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs", "comment": "12 pages, 10 figures, published in KDD2025", "summary": "With the advancement of Large Language Models (LLMs), LLM applications have\nexpanded into a growing number of fields. However, users with data privacy\nconcerns face limitations in directly utilizing LLM APIs, while private\ndeployments incur significant computational demands. This creates a substantial\nchallenge in achieving secure LLM adaptation under constrained local resources.\nTo address this issue, collaborative learning methods, such as Split Learning\n(SL), offer a resource-efficient and privacy-preserving solution for adapting\nLLMs to private domains. In this study, we introduce VFLAIR-LLM (available at\nhttps://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split\nlearning framework for LLMs, enabling privacy-preserving LLM inference and\nfine-tuning in resource-constrained environments. Our library provides two LLM\npartition settings, supporting three task types and 18 datasets. In addition,\nwe provide standard modules for implementing and evaluating attacks and\ndefenses. We benchmark 5 attacks and 9 defenses under various Split Learning\nfor LLM(SL-LLM) settings, offering concrete insights and recommendations on the\nchoice of model partition configurations, defense strategies, and relevant\nhyperparameters for real-world applications."}
{"id": "2508.02913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02913", "abs": "https://arxiv.org/abs/2508.02913", "authors": ["Carolina Minami Oguchi", "Leo Wei", "Koyo Kobayashi", "Hsin-Tai Wu", "Dipak Ghosal"], "title": "Enhancing Japanese Large Language Models with Reasoning Vectors", "comment": null, "summary": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages."}
{"id": "2508.03503", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2508.03503", "abs": "https://arxiv.org/abs/2508.03503", "authors": ["Gianluca Bianchin", "Bryan Van Scoy"], "title": "Feedback Optimization of Dynamical Systems in Time-Varying Environments: An Internal Model Principle Approach", "comment": null, "summary": "Feedback optimization has emerged as a promising approach for regulating\ndynamical systems to optimal steady states that are implicitly defined by\nunderlying optimization problems. Despite their effectiveness, existing methods\nface two key limitations: (i) reliable performance is restricted to\ntime-invariant or slowly varying settings, and (ii) convergence rates are\nlimited by the need for the controller to operate orders of magnitude slower\nthan the plant. These limitations can be traced back to the reliance of\nexisting techniques on numerical optimization algorithms. In this paper, we\npropose a novel perspective on the design of feedback optimization algorithms,\nby framing these objectives as an output regulation problem. We place\nparticular emphasis on time-varying optimization problems, and show that an\nalgorithm can track time-varying optimizers if and only if it incorporates a\nmodel of the temporal variability inherent to the optimization - a requirement\nthat we term the internal model principle of feedback optimization. Building on\nthis insight, we introduce a new design methodology that couples\noutput-feedback stabilization with a control component that drives the system\ntoward the critical points of the optimization problem. This framework enables\nfeedback optimization algorithms to overcome the classical limitations of slow\ntracking and poor adaptability to time variations."}
{"id": "2508.03023", "categories": ["math.NT", "11M41, 11Y35, 11L07"], "pdf": "https://arxiv.org/pdf/2508.03023", "abs": "https://arxiv.org/abs/2508.03023", "authors": ["Neea Palojärvi", "Tianyu Zhao"], "title": "On Turing's method for Artin $L$-functions and the Selberg class", "comment": null, "summary": "We derive explicit bounds for two general classes of $L$-functions, improving\nand generalizing earlier known estimates. These bounds can be used, for\nexample, to apply Turing's method for determining the number of zeros up to a\ngiven height."}
{"id": "2508.03335", "categories": ["math.CO", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.03335", "abs": "https://arxiv.org/abs/2508.03335", "authors": ["Neel Kaul", "David R. Wood"], "title": "On universal graphs for trees and treewidth $k$ graphs", "comment": null, "summary": "Let $s(n)$ be the minimum number of edges in a graph that contains every\n$n$-vertex tree as a subgraph. Chung and Graham [J. London Math. Soc. 1983]\nclaim to prove that $s(n)\\leqslant O(n\\log n)$. We point out a mistake in their\nproof. The previously best known upper bound is $s(n)\\leqslant O(n(\\log\nn)(\\log\\log n)^{2})$ by Chung, Graham and Pippenger [Proc. Hungarian Coll. on\nCombinatorics 1976], the proof of which is missing many crucial details. We\ngive a fully self-contained proof of the new and improved upper bound\n$s(n)\\leqslant O(n(\\log n)(\\log\\log n))$. The best known lower bound is\n$s(n)\\geqslant \\Omega(n\\log n)$.\n  We generalise these results for graphs of treewidth $k$. For an integer\n$k\\geqslant 1$, let $s_k(n)$ be the minimum number of edges in a graph that\ncontains every $n$-vertex graph with treewidth $k$ as a subgraph. So\n$s(n)=s_1(n)$. We show that $\\Omega(k n\\log n) \\leqslant s_k(n) \\leqslant\nO(kn(\\log n)(\\log\\log n))$."}
{"id": "2508.03125", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03125", "abs": "https://arxiv.org/abs/2508.03125", "authors": ["Bingyu Yan", "Ziyi Zhou", "Xiaoming Zhang", "Chaozhuo Li", "Ruilin Zeng", "Yirui Qi", "Tianbo Wang", "Litian Zhang"], "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS", "comment": null, "summary": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS."}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments."}
{"id": "2508.03071", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2508.03071", "abs": "https://arxiv.org/abs/2508.03071", "authors": ["Zeping Hao", "Chao Qin", "Yang Zhou"], "title": "Explicit Hecke eigenform product identities for Hilbert modular forms", "comment": null, "summary": "Let $F$ be a totally real number field, and $g,f,h$ be Hilbert modular forms\nover $F$ that are Hecke eigenforms satisfying $g=f\\cdot h$. We characterize\nsuch product identities among all real quadratic fields of narrow class number\none, proving they occur only for $F=\\mathbb Q(\\sqrt{5})$, with precisely two\nsuch identities. We also shed some light on the general totally real case by\nshowing that no such identity exists when both $f$ and $h$ are Eisenstein\nseries of distinct weights."}
{"id": "2508.03377", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2508.03377", "abs": "https://arxiv.org/abs/2508.03377", "authors": ["Reimbay Reimbayev"], "title": "The Subgraphs of Order Six of the Family of Strongly Regular Graphs with Parameters $λ=1$ and $μ=2$", "comment": "25 pages, 3 figures", "summary": "Strongly regular graphs are highly symmetrical and can be described fully\nwith just a few parameters yet the existence of many of them is still under the\nquestion. Due to this uncertainty, it is of immense interest to study their\nstructure, in particular to obtain all the possible subgraphs of lower order.\nIn this paper we study the family of strongly regular graphs with parameters\n$\\lambda =1$ and $\\mu =2$ and establish all their subgraphs of order six."}
{"id": "2508.03130", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03130", "abs": "https://arxiv.org/abs/2508.03130", "authors": ["Rama Carl Hoetzlein"], "title": "Protecting Small Organizations from AI Bots with Logrip: Hierarchical IP Hashing", "comment": "11 pages, 4 figures", "summary": "Small organizations, start ups, and self-hosted servers face increasing\nstrain from automated web crawlers and AI bots, whose online presence has\nincreased dramatically in the past few years. Modern bots evade traditional\nthrottling and can degrade server performance through sheer volume even when\nthey are well-behaved. We introduce a novel security approach that leverages\ndata visualization and hierarchical IP hashing to analyze server event logs,\ndistinguishing human users from automated entities based on access patterns. By\naggregating IP activity across subnet classes and applying statistical\nmeasures, our method detects coordinated bot activity and distributed crawling\nattacks that conventional tools fail to identify. Using a real world example we\nestimate that 80 to 95 percent of traffic originates from AI crawlers,\nunderscoring the need for improved filtering mechanisms. Our approach enables\nsmall organizations to regulate automated traffic effectively, preserving\npublic access while mitigating performance degradation."}
{"id": "2508.02936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02936", "abs": "https://arxiv.org/abs/2508.02936", "authors": ["Songkun Yan", "Zhi Li", "Siyu Zhu", "Yixin Wen", "Mofan Zhang", "Mengye Chen", "Jie Cao", "Yang Hong"], "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology", "comment": "8 pages, 5 figures, 2025 ICCV SEA workshop paper", "summary": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers."}
{"id": "2508.03233", "categories": ["math.NT"], "pdf": "https://arxiv.org/pdf/2508.03233", "abs": "https://arxiv.org/abs/2508.03233", "authors": ["Oussama Hamza", "Donghyeok Lim", "Christian Maire"], "title": "Massey products and unipotent extensions with restricted ramification", "comment": null, "summary": "We fix a prime p and construct new cases of pro-p extensions of number fields\nwith restricted ramification and splitting, whose Galois groups decompose as\ncoproducts of pro-p absolute Galois groups of local fields. As a consequence,\nthese pro-p extensions satisfy the strong Massey vanishing property and thus\nadmit large unipotent quotients."}
{"id": "2508.03531", "categories": ["math.CO", "math.MG", "05C45 (Primary) 05C10, 52B05, 52B10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.03531", "abs": "https://arxiv.org/abs/2508.03531", "authors": ["Tobias Schnieders"], "title": "Barnette Graphs with Faces up to Size 8 are Hamiltonian", "comment": "16 pages, 56 figures", "summary": "Barnette's conjecture states that every cubic, bipartite, planar and\n3-connected graph is Hamiltonian. Goodey verified Barnette's conjecture for all\ngraphs with faces of size up to 6.\n  We substantially strengthen Goodey's result by proving Hamiltonicity for\ncubic, bipartite, planar and (2-)connected graphs with faces of size up to 8.\nParts of the proof are computational, including a distinction of 339.068.624\ncases."}
{"id": "2508.03151", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03151", "abs": "https://arxiv.org/abs/2508.03151", "authors": ["Ronghua Li", "Shinan Liu", "Haibo Hu", "Qingqing Ye", "Nick Feamster"], "title": "WiFinger: Fingerprinting Noisy IoT Event Traffic Using Packet-level Sequence Matching", "comment": null, "summary": "IoT environments such as smart homes are susceptible to privacy inference\nattacks, where attackers can analyze patterns of encrypted network traffic to\ninfer the state of devices and even the activities of people. While most\nexisting attacks exploit ML techniques for discovering such traffic patterns,\nthey underperform on wireless traffic, especially Wi-Fi, due to its heavy noise\nand packet losses of wireless sniffing. In addition, these approaches commonly\ntarget at distinguishing chunked IoT event traffic samples, and they failed at\neffectively tracking multiple events simultaneously. In this work, we propose\nWiFinger, a fine-grained multi-IoT event fingerprinting approach against noisy\ntraffic. WiFinger turns the traffic pattern classification task into a\nsubsequence matching problem and introduces novel techniques to account for the\nhigh time complexity while maintaining high accuracy. Experiments demonstrate\nthat our method outperforms existing approaches on Wi-Fi traffic, achieving an\naverage recall of 85% (vs. 0.49% and 0.46%) for various IoT events while\nmaintaining almost zero false positives for most of them."}
{"id": "2508.02951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02951", "abs": "https://arxiv.org/abs/2508.02951", "authors": ["Mahtab Bigverdi", "Wisdom Ikezogwo", "Kevin Zhang", "Hyewon Jeong", "Mingyu Lu", "Sungjae Cho", "Linda Shapiro", "Ranjay Krishna"], "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine", "comment": null, "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page."}
{"id": "2508.03308", "categories": ["math.NT", "math.DS", "37P15, 11R09, 37P20"], "pdf": "https://arxiv.org/pdf/2508.03308", "abs": "https://arxiv.org/abs/2508.03308", "authors": ["Vefa Goksel"], "title": "Iterates of post-critically finite polynomials of the form $\\boldsymbol{x^d+c}$", "comment": "17 pages", "summary": "Fix a prime number $d$. The post-critically finite polynomials of the form\n$f_{d,c} = x^d+c\\in \\mathbb{C}[x]$ play a fundamental role in polynomial\ndynamics. While many results are known in the complex dynamical setting, much\nless is understood about the arithmetic properties of these polynomials. In\nthis paper, we describe the factorization of the iterates of post-critically\nfinite polynomials $f_{d,c}$ over their fields of definition. As a consequence,\nwe prove new cases of a conjecture of Andrews and Petsche on abelian arboreal\nGalois representations."}
{"id": "2508.03568", "categories": ["math.CO", "math.AG"], "pdf": "https://arxiv.org/pdf/2508.03568", "abs": "https://arxiv.org/abs/2508.03568", "authors": ["Alessandro D'Andrea", "Enrico Fatighenti", "Claudio Onorati"], "title": "Computing plethysms via derivations", "comment": "18 pages. Comments are welcome", "summary": "We consider a derivation D on the ring ${\\Lambda}$ of symmetric functions and\ninvestigate its combinatorial, algebraic and geometric properties. More\nprecisely, we show that D restricts to a quasi-isometry, with respect to the\nHall product, on the graded component of ${\\Lambda}$ of each positive degree\nand provide a chain-rule formula with respect to the plethysm operation.\nFurthermore, we relate the geometric shape of D(f), where f $\\in {\\Lambda}$ is\nan homogeneous symmetric function, to that of f. An application to the shape of\nthe partitions appearing in a given plethysms is proved."}
{"id": "2508.03221", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03221", "abs": "https://arxiv.org/abs/2508.03221", "authors": ["Yu Pan", "Jiahao Chen", "Lin Wang", "Bingrong Dai", "Yi Du"], "title": "BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models", "comment": null, "summary": "In recent years,Diffusion models have achieved remarkable progress in the\nfield of image generation.However,recent studies have shown that diffusion\nmodels are susceptible to backdoor attacks,in which attackers can manipulate\nthe output by injecting covert triggers such as specific visual patterns or\ntextual phrases into the training dataset.Fortunately,with the continuous\nadvancement of defense techniques,defenders have become increasingly capable of\nidentifying and mitigating most backdoor attacks using visual inspection and\nneural network-based detection methods.However,in this paper,we identify a\nnovel type of backdoor threat that is more lightweight and covert than existing\napproaches,which we name BadBlocks,requires only about 30\\% of the\ncomputational resources and 20\\% GPU time typically needed by previous backdoor\nattacks,yet it successfully injects backdoors and evades the most advanced\ndefense frameworks.BadBlocks enables attackers to selectively contaminate\nspecific blocks within the UNet architecture of diffusion models while\nmaintaining normal functionality in the remaining components.Experimental\nresults demonstrate that BadBlocks achieves a high attack success rate (ASR)\nand low perceptual quality loss (as measured by FID Score),even under extremely\nconstrained computational resources and GPU time.Moreover,BadBlocks is able to\nbypass existing defense frameworks,especially the attention-based backdoor\ndetection method, highlighting it as a novel and noteworthy threat.Ablation\nstudies further demonstrate that effective backdoor injection does not require\nfine-tuning the entire network and highlight the pivotal role of certain neural\nnetwork layers in backdoor mapping.Overall,BadBlocks significantly reduces the\nbarrier to conducting backdoor attacks in all aspects.It enables attackers to\ninject backdoors into large-scale diffusion models even using consumer-grade\nGPUs."}
{"id": "2508.02959", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02959", "abs": "https://arxiv.org/abs/2508.02959", "authors": ["Chia-Tung Ho", "Jing Gong", "Xufeng Yao", "Yunsheng Bai", "Abhishek B Akkur", "Haoxing Ren"], "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow", "comment": "18 pages, 12 figures, under review for AAAI2026", "summary": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines."}
{"id": "2508.03359", "categories": ["math.NT", "math.DS"], "pdf": "https://arxiv.org/pdf/2508.03359", "abs": "https://arxiv.org/abs/2508.03359", "authors": ["Yubin He"], "title": "A dimensional mass transference principle from balls to open sets and applications to dynamical Diophantine approximation", "comment": null, "summary": "The mass transference principle of Beresnevich and Velani is a powerful\nmechanism for determining the Hausdorff dimension/measure of $\\limsup$ sets\nthat arise naturally in Diophantine approximation. However, in the setting of\ndynamical Diophantine approximation, this principle often fails to apply\neffectively, as the radii of the balls defining the dynamical $\\limsup$ sets\ngenerally depend on the orbit of the point $x$ itself.\n  In this paper, we develop a dimensional mass transference principle that\nenables us to recover and extend classical results on shrinking target\nproblems, particularly for the $\\beta$-transformation and the Gauss map.\nMoreover, our result shows that the corresponding $\\limsup$ sets have large\nintersection properties. A potentially interesting feature of our method is\nthat, in many cases, shrinking target problems are closely related to finding\nan appropriate Gibbs measure, which may reveal new aspects of the link between\nthermodynamic formalism and dynamical Diophantine approximation."}
{"id": "2508.03634", "categories": ["math.CO"], "pdf": "https://arxiv.org/pdf/2508.03634", "abs": "https://arxiv.org/abs/2508.03634", "authors": ["Zach Hunter", "Teng Liu", "Aleksa Milojević", "Benny Sudakov"], "title": "Cyclic subsets of tournaments", "comment": "13 pages", "summary": "Let $G$ be a Dirac graph, and let $S$ be a vertex subset of $G$, chosen\nuniformly at random. How likely is the induced subgraph $G[S]$ to be\nHamiltonian? This question, proposed by Erd\\H{o}s and Faudree in 1996, was\nrecently resolved by Dragani\\'c, Keevash and M\\\"uyesser, in the setting of\ngraphs. In this paper, we study a similar question for tournaments -- if $T$ is\na tournament of high minimum degree, how likely is it for a random induced\nsubtournament of $T$ to be Hamiltonian? We prove an optimal bound on this\nprobability, and extend the results to the regime where the subset is not\nsampled uniformly at random, but according to a $p$-biased measure."}
{"id": "2508.03307", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03307", "abs": "https://arxiv.org/abs/2508.03307", "authors": ["Ye Li", "Chengcheng Zhu", "Yanchao Zhao", "Jiale Zhang"], "title": "BDFirewall: Towards Effective and Expeditiously Black-Box Backdoor Defense in MLaaS", "comment": "18 pages", "summary": "In this paper, we endeavor to address the challenges of backdoor attacks\ncountermeasures in black-box scenarios, thereby fortifying the security of\ninference under MLaaS. We first categorize backdoor triggers from a new\nperspective, i.e., their impact on the patched area, and divide them into:\nhigh-visibility triggers (HVT), semi-visibility triggers (SVT), and\nlow-visibility triggers (LVT). Based on this classification, we propose a\nprogressive defense framework, BDFirewall, that removes these triggers from the\nmost conspicuous to the most subtle, without requiring model access. First, for\nHVTs, which create the most significant local semantic distortions, we identify\nand eliminate them by detecting these salient differences. We then restore the\npatched area to mitigate the adverse impact of such removal process. The\nlocalized purification designed for HVTs is, however, ineffective against SVTs,\nwhich globally perturb benign features. We therefore model an SVT-poisoned\ninput as a mixture of a trigger and benign features, where we unconventionally\ntreat the benign features as \"noise\". This formulation allows us to reconstruct\nSVTs by applying a denoising process that removes these benign \"noise\"\nfeatures. The SVT-free input is then obtained by subtracting the reconstructed\ntrigger. Finally, to neutralize the nearly imperceptible but fragile LVTs, we\nintroduce lightweight noise to disrupt the trigger pattern and then apply DDPM\nto restore any collateral impact on clean features. Comprehensive experiments\ndemonstrate that our method outperforms state-of-the-art defenses. Compared\nwith baselines, BDFirewall reduces the Attack Success Rate (ASR) by an average\nof 33.25%, improving poisoned sample accuracy (PA) by 29.64%, and achieving up\nto a 111x speedup in inference time. Code will be made publicly available upon\nacceptance."}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms."}
{"id": "2508.03570", "categories": ["math.NT", "Primary: 14K15, 16H10, Secondary: 14G15, 11G10, 13H10"], "pdf": "https://arxiv.org/pdf/2508.03570", "abs": "https://arxiv.org/abs/2508.03570", "authors": ["Sarah Arpin", "Stefano Marseglia", "Caleb Springer"], "title": "Isogeny graphs of abelian varieties and singular ideals in orders", "comment": "comments are welcome", "summary": "Famously, Kohel proved that isogeny graphs of ordinary elliptic curves are\nbeautifully structured objects, now called volcanos. We prove graph structural\ntheorems for abelian varieties of any dimension with commutative endomorphism\nring and containing a fixed locally Bass order, leveraging an ideal-theoretic\nperspective on isogeny graphs. This generalizes previous results, which relied\non restrictive additional assumptions, such as maximal real multiplication,\nordinary, and absolutely simple (Brooks, Jetchev, Wesolowski 2017). In\nparticular, our work also applies to non-simple and non-ordinary isogeny\nclasses. To obtain our results, we first prove a structure theorem for the\nlattice of inclusion of the overorders of a locally Bass order in an \\'etale\nalgebra which is of independent interest. This analysis builds on a careful\nstudy of local singularities of the orders. We include several examples of\nvolcanoes and isogeny graphs exhibiting unexpected properties ultimately due to\nour more general setting."}
{"id": "2508.03361", "categories": ["cs.DM", "math.CO", "math.PR", "05C80, 68R10, 05C38"], "pdf": "https://arxiv.org/pdf/2508.03361", "abs": "https://arxiv.org/abs/2508.03361", "authors": ["Samuel Baguley", "Andreas Göbel", "Nicolas Klodt", "George Skretas", "John Sylvester", "Viktor Zamaraev"], "title": "Temporal Exploration of Random Spanning Tree Models", "comment": "42 pages, 8 Figures", "summary": "The Temporal Graph Exploration problem (TEXP) takes as input a temporal\ngraph, i.e., a sequence of graphs $(G_i)_{i\\in \\mathbb{N}}$ on the same vertex\nset, and asks for a walk of shortest length visiting all vertices, where the\n$i$-th step uses an edge from $G_i$. If each such $G_i$ is connected, then an\nexploration of length $n^2$ exists, and this is known to be the best possible\nup to a constant. More fine-grained lower and upper bounds have been obtained\nfor restricted temporal graph classes, however, for several fundamental\nclasses, a large gap persists between known bounds, and it remains unclear\nwhich properties of a temporal graph make it inherently difficult to explore.\n  Motivated by this limited understanding and the central role of the Temporal\nGraph Exploration problem in temporal graph theory, we study the problem in a\nrandomised setting. We introduce the Random Spanning Tree (RST) model, which\nconsists of a set of $n$-vertex trees together with an arbitrary probability\ndistribution $\\mu$ over this set. A random temporal graph generated by the RST\nmodel is a sequence of independent samples drawn from $\\mu$.\n  We initiate a systematic study of the Temporal Graph Exploration problem in\nsuch random temporal graphs and establish tight general bounds on exploration\ntime. Our first main result proves that any RST model can, with high\nprobability (w.h.p.), be explored in $O(n^{3/2})$ time, and we show that this\nbound is tight up to a constant factor. This demonstrates a fundamental\ndifference between the adversarial and random settings. Our second main result\nshows that if all trees of an RST are subgraphs of a fixed graph with $m$ edges\nthen, w.h.p.\\ , it can be explored in $O(m)$ time."}
{"id": "2508.03342", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03342", "abs": "https://arxiv.org/abs/2508.03342", "authors": ["Mehdi Akbari Gurabi", "Lasse Nitz", "Radu-Mihai Castravet", "Roman Matzutt", "Avikarsha Mandal", "Stefan Decker"], "title": "From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format", "comment": "20 pages, including appendices, 32 references, 4 tables, 7 main\n  figures (some of them has sub-figures)", "summary": "Existing cybersecurity playbooks are often written in heterogeneous,\nnon-machine-readable formats, which limits their automation and\ninteroperability across Security Orchestration, Automation, and Response\nplatforms. This paper explores the suitability of Large Language Models,\ncombined with Prompt Engineering, to automatically translate legacy incident\nresponse playbooks into the standardized, machine-readable CACAO format. We\nsystematically examine various Prompt Engineering techniques and carefully\ndesign prompts aimed at maximizing syntactic accuracy and semantic fidelity for\ncontrol flow preservation. Our modular transformation pipeline integrates a\nsyntax checker to ensure syntactic correctness and features an iterative\nrefinement mechanism that progressively reduces syntactic errors. We evaluate\nthe proposed approach on a custom-generated dataset comprising diverse legacy\nplaybooks paired with manually created CACAO references. The results\ndemonstrate that our method significantly improves the accuracy of playbook\ntransformation over baseline models, effectively captures complex workflow\nstructures, and substantially reduces errors. It highlights the potential for\npractical deployment in automated cybersecurity playbook transformation tasks."}
{"id": "2508.02979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development."}
{"id": "2508.03650", "categories": ["math.NT", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.03650", "abs": "https://arxiv.org/abs/2508.03650", "authors": ["Christian Dean", "Haley Havard", "Elizabeth Hawkins", "Patch Heard", "Andrew Lott", "Alex Rice"], "title": "Notes and computations on forbidden differences", "comment": "10 pages, 5 tables", "summary": "We explore from several perspectives the following question: given\n$X\\subseteq \\mathbb{Z}$ and $N\\in \\mathbb{N}$, what is the maximum size\n$D(X,N)$ of $A\\subseteq \\{1,2,\\dots,N\\}$ before $A$ is forced to contain two\ndistinct elements that differ by an element of $X$? The set of forbidden\ndifferences, $X$, is called \\textit{intersective} if $D(X,N)=o(N)$, with the\nmost well-studied examples being $X=S=\\{n^2: n\\in \\mathbb{N}\\}$ and\n$X=\\mathcal{P}-1=\\{p-1: p\\text{ prime}\\}$. In addition to some new results,\nincluding exact formulas and estimates for $D(X,N)$ in some non-intersective\ncases like $X=\\mathcal{P}$ and $X=S+k$, $k\\in \\mathbb{N}$, we also provide a\ncomprehensive survey of known bounds and extensive computational data. In\nparticular, we utilize an existing algorithm for finding maximum cliques in\ngraphs to determine $D(S,N)$ for $N\\leq 300$ and $D(\\mathcal{P}-1,N)$ for\n$N\\leq 500$. None of these exact values appear previously in the literature."}
{"id": "2508.03549", "categories": ["cs.DM", "math.CO", "05C15, 68R10", "G.2.1; G.2.2"], "pdf": "https://arxiv.org/pdf/2508.03549", "abs": "https://arxiv.org/abs/2508.03549", "authors": ["Diptimaya Behera", "Mathew C. Francis", "Sreejith K. Pallathumadam"], "title": "Adjacent vertex distinguishing total coloring of 3-degenerate graphs", "comment": null, "summary": "A total coloring of a simple undirected graph $G$ is an assignment of colors\nto its vertices and edges such that the colors given to the vertices form a\nproper vertex coloring, the colors given to the edges form a proper edge\ncoloring, and the color of every edge is different from that of its two\nendpoints. That is, $\\phi:V(G)\\cup E(G)\\rightarrow\\mathbb{N}$ is a total\ncoloring of $G$ if $\\phi(u)\\neq\\phi(v)$ and $\\phi(uv)\\neq\\phi(u)$ for all\n$uv\\in E(G)$, and $\\phi(uv)\\neq\\phi(uw)$ for any $u \\in V(G)$ and distinct $v,w\n\\in N(u)$ (here, $N(u)$ denotes the set of neighbours of $u$). A total coloring\n$\\phi$ of a graph $G$ is said to be ``Adjacent Vertex Distinguishing'' (or AVD\nfor short) if for all $uv\\in E(G)$, we have that $\\phi(\\{u\\}\\cup\\{uw:w\\in\nN(u)\\})\\neq\\phi(\\{v\\}\\cup\\{vw\\colon w\\in N(v)\\})$. The AVD Total Coloring\nConjecture of Zhang, Chen, Li, Yao, Lu, and Wang (Science in China Series A:\nMathematics, 48(3):289--299, 2005) states that every graph $G$ has an AVD total\ncoloring using at most $\\Delta(G)+3$ colors, where $\\Delta(G)$ denotes the\nmaximum degree of $G$. For some $s\\in\\mathbb{N}$, a graph $G$ is said to be\n$s$-degenerate if every subgraph of $G$ has minimum degree at most $s$. Miao,\nShi, Hu, and Luo (Discrete Mathematics, 339(10):2446--2449, 2016) showed that\nthe AVD Total Coloring Conjecture is true for 2-degenerate graphs. We verify\nthe conjecture for 3-degenerate graphs."}
{"id": "2508.03413", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03413", "abs": "https://arxiv.org/abs/2508.03413", "authors": ["Akshay Madhav Deshmukh"], "title": "Smart Car Privacy: Survey of Attacks and Privacy Issues", "comment": "13 pages, 16 figures", "summary": "Automobiles are becoming increasingly important in our day to day life.\nModern automobiles are highly computerized and hence potentially vulnerable to\nattack. Providing many wireless connectivity for vehicles enables a bridge\nbetween vehicles and their external environments. Such a connected vehicle\nsolution is expected to be the next frontier for automotive revolution and the\nkey to the evolution to next generation intelligent transportation systems.\nVehicular Ad hoc Networks (VANETs) are emerging mobile ad hoc network\ntechnologies incorporating mobile routing protocols for inter-vehicle data\ncommunications to support intelligent transportation systems. Thus security and\nprivacy are the major concerns in VANETs due to the mobility of the vehicles.\nThus designing security mechanisms to remove adversaries from the network\nremarkably important in VANETs.\n  This paper provides an overview of various vehicular network architectures.\nThe evolution of security in modern vehicles. Various security and privacy\nattacks in VANETs with their defending mechanisms with examples and classify\nthese mechanisms. It also provides an overview of various privacy implication\nthat a vehicular network possess."}
{"id": "2508.02994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02994", "abs": "https://arxiv.org/abs/2508.02994", "authors": ["Fangyi Yu"], "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs", "comment": null, "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs."}
{"id": "2508.03150", "categories": ["math.CO", "math.NT", "05E05, 11M32"], "pdf": "https://arxiv.org/pdf/2508.03150", "abs": "https://arxiv.org/abs/2508.03150", "authors": ["Wataru Takeda", "Yoshinori Yamasaki"], "title": "Quadratic relations for ninth variations of Schur functions and application to Schur multiple zeta functions", "comment": "26 pages", "summary": "Macdonald's ninth variation of Schur functions is a broad generalization of\nthe classical Schur function and its variants, defined via the Jacobi-Trudi\ndeterminant formula. In this paper, we establish various algebraic relations\nfor $S^{(r)}_{\\lambda/\\mu}(X)$, a class of the ninth variation introduced by\nNakagawa, Noumi, Shirakawa, and Yamada, by combining the Jacobi-Trudi formula\nwith determinant formulas such as the Desnanot-Jacobi adjoint matrix theorem\nand the Pl\\\"ucker relations, which generalize the corresponding relations for\nSchur functions. As an application, we investigate algebraic relations for\n\"diagonally constant\" Schur multiple zeta functions and examine their specific\nspecial values when the shape is rectangular."}
{"id": "2508.03650", "categories": ["math.NT", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.03650", "abs": "https://arxiv.org/abs/2508.03650", "authors": ["Christian Dean", "Haley Havard", "Elizabeth Hawkins", "Patch Heard", "Andrew Lott", "Alex Rice"], "title": "Notes and computations on forbidden differences", "comment": "10 pages, 5 tables", "summary": "We explore from several perspectives the following question: given\n$X\\subseteq \\mathbb{Z}$ and $N\\in \\mathbb{N}$, what is the maximum size\n$D(X,N)$ of $A\\subseteq \\{1,2,\\dots,N\\}$ before $A$ is forced to contain two\ndistinct elements that differ by an element of $X$? The set of forbidden\ndifferences, $X$, is called \\textit{intersective} if $D(X,N)=o(N)$, with the\nmost well-studied examples being $X=S=\\{n^2: n\\in \\mathbb{N}\\}$ and\n$X=\\mathcal{P}-1=\\{p-1: p\\text{ prime}\\}$. In addition to some new results,\nincluding exact formulas and estimates for $D(X,N)$ in some non-intersective\ncases like $X=\\mathcal{P}$ and $X=S+k$, $k\\in \\mathbb{N}$, we also provide a\ncomprehensive survey of known bounds and extensive computational data. In\nparticular, we utilize an existing algorithm for finding maximum cliques in\ngraphs to determine $D(S,N)$ for $N\\leq 300$ and $D(\\mathcal{P}-1,N)$ for\n$N\\leq 500$. None of these exact values appear previously in the literature."}
{"id": "2508.03474", "categories": ["cs.CR", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2508.03474", "abs": "https://arxiv.org/abs/2508.03474", "authors": ["Oriol Saguillo", "Vahid Ghafouri", "Lucianna Kiffer", "Guillermo Suarez-Tangil"], "title": "Unravelling the Probabilistic Forest: Arbitrage in Prediction Markets", "comment": null, "summary": "Polymarket is a prediction market platform where users can speculate on\nfuture events by trading shares tied to specific outcomes, known as conditions.\nEach market is associated with a set of one or more such conditions. To ensure\nproper market resolution, the condition set must be exhaustive -- collectively\naccounting for all possible outcomes -- and mutually exclusive -- only one\ncondition may resolve as true. Thus, the collective prices of all related\noutcomes should be \\$1, representing a combined probability of 1 of any\noutcome. Despite this design, Polymarket exhibits cases where dependent assets\nare mispriced, allowing for purchasing (or selling) a certain outcome for less\nthan (or more than) \\$1, guaranteeing profit. This phenomenon, known as\narbitrage, could enable sophisticated participants to exploit such\ninconsistencies.\n  In this paper, we conduct an empirical arbitrage analysis on Polymarket data\nto answer three key questions: (Q1) What conditions give rise to arbitrage (Q2)\nDoes arbitrage actually occur on Polymarket and (Q3) Has anyone exploited these\nopportunities. A major challenge in analyzing arbitrage between related markets\nlies in the scalability of comparisons across a large number of markets and\nconditions, with a naive analysis requiring $O(2^{n+m})$ comparisons. To\novercome this, we employ a heuristic-driven reduction strategy based on\ntimeliness, topical similarity, and combinatorial relationships, further\nvalidated by expert input.\n  Our study reveals two distinct forms of arbitrage on Polymarket: Market\nRebalancing Arbitrage, which occurs within a single market or condition, and\nCombinatorial Arbitrage, which spans across multiple markets. We use on-chain\nhistorical order book data to analyze when these types of arbitrage\nopportunities have existed, and when they have been executed by users. We find\na realized estimate of 40 million USD of profit extracted."}
{"id": "2508.02999", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "Mónica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs."}
{"id": "2508.03517", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03517", "abs": "https://arxiv.org/abs/2508.03517", "authors": ["Mabin Umman Varghese", "Zahra Taghiyarrenani"], "title": "Intrusion Detection in Heterogeneous Networks with Domain-Adaptive Multi-Modal Learning", "comment": null, "summary": "Network Intrusion Detection Systems (NIDS) play a crucial role in\nsafeguarding network infrastructure against cyberattacks. As the prevalence and\nsophistication of these attacks increase, machine learning and deep neural\nnetwork approaches have emerged as effective tools for enhancing NIDS\ncapabilities in detecting malicious activities. However, the effectiveness of\ntraditional deep neural models is often limited by the need for extensive\nlabelled datasets and the challenges posed by data and feature heterogeneity\nacross different network domains. To address these limitations, we developed a\ndeep neural model that integrates multi-modal learning with domain adaptation\ntechniques for classification. Our model processes data from diverse sources in\na sequential cyclic manner, allowing it to learn from multiple datasets and\nadapt to varying feature spaces. Experimental results demonstrate that our\nproposed model significantly outperforms baseline neural models in classifying\nnetwork intrusions, particularly under conditions of varying sample\navailability and probability distributions. The model's performance highlights\nits ability to generalize across heterogeneous datasets, making it an efficient\nsolution for real-world network intrusion detection."}
{"id": "2508.03018", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03018", "abs": "https://arxiv.org/abs/2508.03018", "authors": ["Yutong Wang", "Pengliang Ji", "Kaixin Li", "Baolong Bi", "Tao Feng", "Guillaume Sartoretti"], "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning", "comment": null, "summary": "Large Language Reasoning Models have demonstrated remarkable success on\nstatic tasks, yet their application to multi-round agentic planning in\ninteractive environments faces two fundamental challenges. First, the\nintractable credit assignment problem renders conventional reinforcement\nlearning ineffective in sparse-reward settings. Second, the computational\noverhead of verbose, step-by-step reasoning histories is prohibitive. To\naddress these challenges, we propose BPO, a three-stage framework\n(bootstrapping, extrapolation, and refinement) that establishes a\nself-improving data flywheel to develop robust reasoning models for\nlong-horizon, sparse-reward environments. Our framework first bootstraps\nefficient reasoning using the proposed planning quaternions with long-short\nchain-of-thought fusion. It then extrapolates to out-of-distribution tasks\nthrough complexity-stratified curriculum learning. Finally, the model\niteratively refines itself by learning exclusively on experiences selected via\nreward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and\nWebShop demonstrate that our approach achieves state-of-the-art with\nsignificant token efficiency, providing a new recipe for reasoning models in\nagentic planning."}
{"id": "2508.03588", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03588", "abs": "https://arxiv.org/abs/2508.03588", "authors": ["Zhaoyi Meng", "Fenglei Xu", "Wenxiang Zhao", "Wansen Wang", "Wenchao Huang", "Jie Cui", "Hong Zhong", "Yan Xiong"], "title": "MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection", "comment": "Submitted to TDSC", "summary": "Static analysis, a fundamental technique in Android app examination, enables\nthe extraction of control flows, data flows, and inter-component communications\n(ICCs), all of which are essential for malware detection. However, existing\nmethods struggle to leverage the semantic complementarity across different\ntypes of flows for representing program behaviors, and their context-unaware\nnature further hinders the accuracy of cross-flow semantic integration. We\npropose and implement MalFlows, a novel technique that achieves context-aware\nfusion of heterogeneous flow semantics for Android malware detection. Our goal\nis to leverage complementary strengths of the three types of flow-related\ninformation for precise app profiling. We adopt a heterogeneous information\nnetwork (HIN) to model the rich semantics across these program flows. We\nfurther propose flow2vec, a context-aware HIN embedding technique that\ndistinguishes the semantics of HIN entities as needed based on contextual\nconstraints across different flows and learns accurate app representations\nthrough the joint use of multiple meta-paths. The representations are finally\nfed into a channel-attention-based deep neural network for malware\nclassification. To the best of our knowledge, this is the first study to\ncomprehensively aggregate the strengths of diverse flow-related information for\nassessing maliciousness within apps. We evaluate MalFlows on a large-scale\ndataset comprising over 20 million flow instances extracted from more than\n31,000 real-world apps. Experimental results demonstrate that MalFlows\noutperforms representative baselines in Android malware detection, and\nmeanwhile, validate the effectiveness of flow2vec in accurately learning app\nrepresentations from the HIN constructed over the heterogeneous flows."}
{"id": "2508.03030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03030", "abs": "https://arxiv.org/abs/2508.03030", "authors": ["Siyuan Li", "Yifan Yu", "Yanchen Deng", "Zhihao Zhang", "Mengjing Chen", "Fangzhou Zhu", "Tao Zhong", "Jianye Hao", "Peng Liu", "Bo An"], "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming", "comment": null, "summary": "Mixed-integer linear programming (MILP) has been a fundamental problem in\ncombinatorial optimization. Previous works have designed a plethora of\nhard-coded heuristics to accomplish challenging MILP solving with domain\nknowledge. Driven by the high capability of neural networks, recent research is\ndevoted to replacing manually designed heuristics with learned policies.\nAlthough learning-based MILP methods have shown great promise, existing\nworksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without\nconsidering their interdependence, severely hurting the solving speed and\nquality. To address this issue, we propose a novel multi-agent-based policy\nlearning framework for MILP (Collab-Solver), which can collaboratively optimize\nthe policies for multiple modules. Specifically, we formulate the collaboration\nof cut selection and branching in MILP solving as a Stackelberg game. Under\nthis formulation, we develop a two-phase learning paradigm to stabilize the\ncollaborative policy learning, where the first phase achieves the\ndata-communicated policy pretraining and the second phase further orchestrates\nthe policy learning for various modules. The jointly learned policy\nsignificantly improves the solving performance on both synthetic and\nlarge-scale real-world MILP datasets. Moreover, the policies learned by\nCollab-Solver have also demonstrated excellent generalization abilities across\ndifferent instance sets."}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments."}
{"id": "2508.03031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03031", "abs": "https://arxiv.org/abs/2508.03031", "authors": ["Ziyang Ma", "Baojian Zhou", "Deqing Yang", "Yanghua Xiao"], "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems."}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms."}
{"id": "2508.03038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03038", "abs": "https://arxiv.org/abs/2508.03038", "authors": ["Qi Peng", "Jialin Cui", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree", "comment": "Accepted by ACM MM 2025", "summary": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods."}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure."}
{"id": "2508.03054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03054", "abs": "https://arxiv.org/abs/2508.03054", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Litian Zhang", "Lirong Qiu", "Xi Zhang"], "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning", "comment": null, "summary": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks."}
{"id": "2508.03080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03080", "abs": "https://arxiv.org/abs/2508.03080", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "comment": null, "summary": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs."}
{"id": "2508.03082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03082", "abs": "https://arxiv.org/abs/2508.03082", "authors": ["Fei Liu", "Yilu Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan"], "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "comment": null, "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements."}
{"id": "2508.03083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03083", "abs": "https://arxiv.org/abs/2508.03083", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for missing data\nimputation by modeling the joint distribution of observed and unobserved\nvariables. However, existing methods, typically based on stochastic denoising\ndiffusion probabilistic models (DDPMs), suffer from high inference latency and\nvariable outputs, limiting their applicability in real-world tabular settings.\nTo address these deficiencies, we present in this paper MissDDIM, a conditional\ndiffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for\ntabular imputation. While stochastic sampling enables diverse completions, it\nalso introduces output variability that complicates downstream processing."}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure."}
{"id": "2508.03092", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking."}
{"id": "2508.03109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03109", "abs": "https://arxiv.org/abs/2508.03109", "authors": ["Wen-Xi Yang", "Tian-Fang Zhao"], "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education", "comment": null, "summary": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models."}
{"id": "2508.03117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03117", "abs": "https://arxiv.org/abs/2508.03117", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "comment": "25 pages", "summary": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications."}
{"id": "2508.03149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03149", "abs": "https://arxiv.org/abs/2508.03149", "authors": ["Linda Smail", "David Santandreu Calonge", "Firuz Kamalov", "Nur H. Orak"], "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?", "comment": "20 pages, 3 figures, 7 tables. No external funding", "summary": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided."}
{"id": "2508.03167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03167", "abs": "https://arxiv.org/abs/2508.03167", "authors": ["Charles Tapley Hoyt", "Craig Bakker", "Richard J. Callahan", "Joseph Cottam", "August George", "Benjamin M. Gyori", "Haley M. Hummel", "Nathaniel Merrill", "Sara Mohammad Taheri", "Pruthvi Prakash Navada", "Marc-Antoine Parent", "Adam Rupe", "Olga Vitek", "Jeremy Zucker"], "title": "Causal identification with $Y_0$", "comment": null, "summary": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0."}
{"id": "2508.03173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03173", "abs": "https://arxiv.org/abs/2508.03173", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Honghao He", "Linzhuang Sun", "Conghui He", "Lijun Wu", "Bihui Yu", "Cheng Tan"], "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions", "comment": null, "summary": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions."}
{"id": "2508.03174", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03174", "abs": "https://arxiv.org/abs/2508.03174", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang"], "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "comment": null, "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent."}
{"id": "2508.03251", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03251", "abs": "https://arxiv.org/abs/2508.03251", "authors": ["Osama Mohammed", "Jiaxin Pan", "Mojtaba Nayyeri", "Daniel Hernández", "Steffen Staab"], "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning", "comment": "European Conference of Artificial Intelligence 2025", "summary": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph."}
{"id": "2508.03284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03284", "abs": "https://arxiv.org/abs/2508.03284", "authors": ["Shaofeng Yin", "Ting Lei", "Yang Liu"], "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools", "comment": null, "summary": "Integrating external tools into Large Foundation Models (LFMs) has emerged as\na promising approach to enhance their problem-solving capabilities. While\nexisting studies have demonstrated strong performance in tool-augmented Visual\nQuestion Answering (VQA), recent benchmarks reveal significant gaps in\nreal-world tool-use proficiency, particularly in functionally diverse\nmultimodal settings requiring multi-step reasoning. In this work, we introduce\nToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to\nbridge this gap. Unlike previous datasets that rely on synthetic scenarios and\nsimplified queries, ToolVQA features real-world visual contexts and challenging\nimplicit multi-step reasoning tasks, better aligning with real user\ninteractions. To construct this dataset, we propose ToolEngine, a novel data\ngeneration pipeline that employs Depth-First Search (DFS) with a dynamic\nin-context example matching mechanism to simulate human-like tool-use\nreasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task\ndomains, with an average inference length of 2.78 reasoning steps per instance.\nThe fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on\nour test set but also surpass the large close-sourced model GPT-3.5-turbo on\nvarious out-of-distribution (OOD) datasets, demonstrating strong\ngeneralizability to real-world tool-use scenarios."}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts."}
{"id": "2508.03345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03345", "abs": "https://arxiv.org/abs/2508.03345", "authors": ["Xingdan Wang", "Jiayi He", "Zhiqing Tang", "Jianxiong Guo", "Jiong Lou", "Liping Qian", "Tian Wang", "Weijia Jia"], "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems", "comment": null, "summary": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs."}
{"id": "2508.03346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03346", "abs": "https://arxiv.org/abs/2508.03346", "authors": ["Zeju Li", "Jianyuan Zhong", "Ziyang Zheng", "Xiangyu Wen", "Zhijian Xu", "Yingying Cheng", "Fan Zhang", "Qiang Xu"], "title": "Compressing Chain-of-Thought in LLMs via Step Entropy", "comment": null, "summary": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures."}
{"id": "2508.03360", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03360", "abs": "https://arxiv.org/abs/2508.03360", "authors": ["Feng Rui", "Zhiyao Luo", "Wei Wang", "Yuting Song", "Yong Liu", "Tingting Zhu", "Jianqing Li", "Xingyao Wang"], "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment", "comment": "19 pages, 9 figures, 12 tables", "summary": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools."}
{"id": "2508.03366", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input."}
{"id": "2508.03368", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.03368", "abs": "https://arxiv.org/abs/2508.03368", "authors": ["Lucia Cipolina-Kun", "Marianna Nezhurina", "Jenia Jitsev"], "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play", "comment": null, "summary": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior"}
{"id": "2508.03379", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency."}
{"id": "2508.03396", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03396", "abs": "https://arxiv.org/abs/2508.03396", "authors": ["Rui Zou", "Mengqi Wei", "Yutao Zhu", "Jirong Wen", "Xin Zhao", "Jing Chen"], "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research."}
{"id": "2508.03406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03406", "abs": "https://arxiv.org/abs/2508.03406", "authors": ["Kai Li", "Ruihao Zheng", "Xinye Hao", "Zhenkun Wang"], "title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models", "comment": null, "summary": "In real-world routing problems, users often propose conflicting or\nunreasonable requirements, which result in infeasible optimization models due\nto overly restrictive or contradictory constraints, leading to an empty\nfeasible solution set. Existing Large Language Model (LLM)-based methods\nattempt to diagnose infeasible models, but modifying such models often involves\nmultiple potential adjustments that these methods do not consider. To fill this\ngap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which\ncombines LLM agents and multi-objective optimization within an automatic\nrouting solver, to provide a set of representative actionable suggestions.\nSpecifically, MOID employs multi-objective optimization to consider both path\ncost and constraint violation, generating a set of trade-off solutions, each\nencompassing varying degrees of model adjustments. To extract practical\ninsights from these solutions, MOID utilizes LLM agents to generate a solution\nanalysis function for the infeasible model. This function analyzes these\ndistinct solutions to diagnose the original infeasible model, providing users\nwith diverse diagnostic insights and suggestions. Finally, we compare MOID with\nseveral LLM-based methods on 50 types of infeasible routing problems. The\nresults indicate that MOID automatically generates multiple diagnostic\nsuggestions in a single run, providing more practical insights for restoring\nmodel feasibility and decision-making compared to existing methods."}
{"id": "2508.03438", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03438", "abs": "https://arxiv.org/abs/2508.03438", "authors": ["Taine J. Elliott", "Stephen P. Levitt", "Ken Nixon", "Martin Bekker"], "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction", "comment": "18 pages, 8 figures, Published in the Annual Conference of South\n  African Institute of Computer Scientists and Information Technologists,\n  Preprint (author original)", "summary": "The rapid expansion of publicly-available medical data presents a challenge\nfor clinicians and researchers alike, increasing the gap between the volume of\nscientific literature and its applications. The steady growth of studies and\nfindings overwhelms medical professionals at large, hindering their ability to\nsystematically review and understand the latest knowledge. This paper presents\nan approach to information extraction and automatic knowledge graph (KG)\ngeneration to identify and connect biomedical knowledge. Through a pipeline of\nlarge language model (LLM) agents, the system decomposes 44 PubMed abstracts\ninto semantically meaningful proposition sentences and extracts KG triples from\nthese sentences. The triples are enhanced using a combination of open domain\nand ontology-based information extraction methodologies to incorporate\nontological categories. On top of this, a context variable is included during\nextraction to allow the triple to stand on its own - thereby becoming\n`quadruples'. The extraction accuracy of the LLM is validated by comparing\nnatural language sentences generated from the enhanced triples to the original\npropositions, achieving an average cosine similarity of 0.874. The similarity\nfor generated sentences of enhanced triples were compared with generated\nsentences of ordinary triples showing an increase as a result of the context\nvariable. Furthermore, this research explores the ability for LLMs to infer new\nrelationships and connect clusters in the knowledge base of the knowledge\ngraph. This approach leads the way to provide medical practitioners with a\ncentralised, updated in real-time, and sustainable knowledge source, and may be\nthe foundation of similar gains in a wide variety of fields."}
{"id": "2508.03465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03465", "abs": "https://arxiv.org/abs/2508.03465", "authors": ["Saleh Nikooroo"], "title": "Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence", "comment": null, "summary": "Belief systems are often treated as globally consistent sets of propositions\nor as scalar-valued probability distributions. Such representations tend to\nobscure the internal structure of belief, conflate external credibility with\ninternal coherence, and preclude the modeling of fragmented or contradictory\nepistemic states. This paper introduces a minimal formalism for belief systems\nas directed, weighted graphs. In this framework, nodes represent individual\nbeliefs, edges encode epistemic relationships (e.g., support or contradiction),\nand two distinct functions assign each belief a credibility (reflecting source\ntrust) and a confidence (derived from internal structural support). Unlike\nclassical probabilistic models, our approach does not assume prior coherence or\nrequire belief updating. Unlike logical and argumentation-based frameworks, it\nsupports fine-grained structural representation without committing to binary\njustification status or deductive closure. The model is purely static and\ndeliberately excludes inference or revision procedures. Its aim is to provide a\nfoundational substrate for analyzing the internal organization of belief\nsystems, including coherence conditions, epistemic tensions, and\nrepresentational limits. By distinguishing belief structure from belief\nstrength, this formalism enables a richer classification of epistemic states\nthan existing probabilistic, logical, or argumentation-based approaches."}
{"id": "2508.03484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03484", "abs": "https://arxiv.org/abs/2508.03484", "authors": ["Zhiyao Xu", "Dan Zhao", "Qingsong Zou", "Qing Li", "Yong Jiang", "Yuhang Wang", "Jingyu Xiao"], "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes", "comment": null, "summary": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen."}
{"id": "2508.03488", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03488", "abs": "https://arxiv.org/abs/2508.03488", "authors": ["Khaled Bachir Delassi", "Lakhdar Zeggane", "Hadda Cherroun", "Abdelhamid Haouhat", "Kaoutar Bouzouad"], "title": "VQA support to Arabic Language Learning Educational Tool", "comment": null, "summary": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences."}
{"id": "2508.03500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03500", "abs": "https://arxiv.org/abs/2508.03500", "authors": ["Yijin Yang", "Cristina Cornelio", "Mario Leiva", "Paulo Shakarian"], "title": "Error Detection and Correction for Interpretable Mathematics in Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have demonstrated the ability to perform\nexplicit multi-step reasoning such as chain-of-thought prompting. However,\ntheir intermediate steps often contain errors that can propagate leading to\ninaccurate final predictions. Additionally, LLMs still struggle with\nhallucinations and often fail to adhere to prescribed output formats, which is\nparticularly problematic for tasks like generating mathematical expressions or\nsource code. This work introduces EDCIM (Error Detection and Correction for\nInterpretable Mathematics), a method for detecting and correcting these errors\nin interpretable mathematics tasks, where the model must generate the exact\nfunctional form that explicitly solve the problem (expressed in natural\nlanguage) rather than a black-box solution. EDCIM uses LLMs to generate a\nsystem of equations for a given problem, followed by a symbolic error-detection\nframework that identifies errors and provides targeted feedback for LLM-based\ncorrection. To optimize efficiency, EDCIM integrates lightweight, open-source\nLLMs with more powerful proprietary models, balancing cost and accuracy. This\nbalance is controlled by a single hyperparameter, allowing users to control the\ntrade-off based on their cost and accuracy requirements. Experimental results\nacross different datasets show that EDCIM significantly reduces both\ncomputational and financial costs, while maintaining, and even improving,\nprediction accuracy when the balance is properly configured."}
{"id": "2508.03616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03616", "abs": "https://arxiv.org/abs/2508.03616", "authors": ["Jorge Gallego-Feliciano", "S. Aaron McClendon", "Juan Morinelli", "Stavros Zervoudakis", "Antonios Saravanos"], "title": "Hidden Dynamics of Massive Activations in Transformer Training", "comment": null, "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins."}
{"id": "2508.03622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03622", "abs": "https://arxiv.org/abs/2508.03622", "authors": ["Jialin Li", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework", "comment": null, "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models."}
{"id": "2508.03661", "categories": ["cs.AI", "astro-ph.HE", "astro-ph.IM", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.03661", "abs": "https://arxiv.org/abs/2508.03661", "authors": ["He Wang", "Liang Zeng"], "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search", "comment": "89 pages (37 main), 6+6 figures, 1 table. Initial submission; subject\n  to revision", "summary": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains."}
{"id": "2508.03680", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03680", "abs": "https://arxiv.org/abs/2508.03680", "authors": ["Xufang Luo", "Yuge Zhang", "Zhiyuan He", "Zilong Wang", "Siyun Zhao", "Dongsheng Li", "Luna K. Qiu", "Yuqing Yang"], "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning", "comment": null, "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment."}
{"id": "2508.03067", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03067", "abs": "https://arxiv.org/abs/2508.03067", "authors": ["Jiewei Lai", "Lan Zhang", "Chen Tang", "Pengcheng Sun", "Xinming Wang", "Yunhao Wang"], "title": "Untraceable DeepFakes via Traceable Fingerprint Elimination", "comment": null, "summary": "Recent advancements in DeepFakes attribution technologies have significantly\nenhanced forensic capabilities, enabling the extraction of traces left by\ngenerative models (GMs) in images, making DeepFakes traceable back to their\nsource GMs. Meanwhile, several attacks have attempted to evade attribution\nmodels (AMs) for exploring their limitations, calling for more robust AMs.\nHowever, existing attacks fail to eliminate GMs' traces, thus can be mitigated\nby defensive measures. In this paper, we identify that untraceable DeepFakes\ncan be achieved through a multiplicative attack, which can fundamentally\neliminate GMs' traces, thereby evading AMs even enhanced with defensive\nmeasures. We design a universal and black-box attack method that trains an\nadversarial model solely using real data, applicable for various GMs and\nagnostic to AMs. Experimental results demonstrate the outstanding attack\ncapability and universal applicability of our method, achieving an average\nattack success rate (ASR) of 97.08\\% against 6 advanced AMs on DeepFakes\ngenerated by 9 GMs. Even in the presence of defensive mechanisms, our method\nmaintains an ASR exceeding 72.39\\%. Our work underscores the potential\nchallenges posed by multiplicative attacks and highlights the need for more\nrobust AMs."}
{"id": "2508.03097", "categories": ["cs.CR", "cs.AI", "I.2.11"], "pdf": "https://arxiv.org/pdf/2508.03097", "abs": "https://arxiv.org/abs/2508.03097", "authors": ["Zixuan Gu", "Qiufeng Fan", "Long Sun", "Yang Liu", "Xiaojun Ye"], "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs", "comment": "12 pages, 10 figures, published in KDD2025", "summary": "With the advancement of Large Language Models (LLMs), LLM applications have\nexpanded into a growing number of fields. However, users with data privacy\nconcerns face limitations in directly utilizing LLM APIs, while private\ndeployments incur significant computational demands. This creates a substantial\nchallenge in achieving secure LLM adaptation under constrained local resources.\nTo address this issue, collaborative learning methods, such as Split Learning\n(SL), offer a resource-efficient and privacy-preserving solution for adapting\nLLMs to private domains. In this study, we introduce VFLAIR-LLM (available at\nhttps://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split\nlearning framework for LLMs, enabling privacy-preserving LLM inference and\nfine-tuning in resource-constrained environments. Our library provides two LLM\npartition settings, supporting three task types and 18 datasets. In addition,\nwe provide standard modules for implementing and evaluating attacks and\ndefenses. We benchmark 5 attacks and 9 defenses under various Split Learning\nfor LLM(SL-LLM) settings, offering concrete insights and recommendations on the\nchoice of model partition configurations, defense strategies, and relevant\nhyperparameters for real-world applications."}
{"id": "2508.03125", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03125", "abs": "https://arxiv.org/abs/2508.03125", "authors": ["Bingyu Yan", "Ziyi Zhou", "Xiaoming Zhang", "Chaozhuo Li", "Ruilin Zeng", "Yirui Qi", "Tianbo Wang", "Litian Zhang"], "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS", "comment": null, "summary": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS."}
{"id": "2508.03342", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03342", "abs": "https://arxiv.org/abs/2508.03342", "authors": ["Mehdi Akbari Gurabi", "Lasse Nitz", "Radu-Mihai Castravet", "Roman Matzutt", "Avikarsha Mandal", "Stefan Decker"], "title": "From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format", "comment": "20 pages, including appendices, 32 references, 4 tables, 7 main\n  figures (some of them has sub-figures)", "summary": "Existing cybersecurity playbooks are often written in heterogeneous,\nnon-machine-readable formats, which limits their automation and\ninteroperability across Security Orchestration, Automation, and Response\nplatforms. This paper explores the suitability of Large Language Models,\ncombined with Prompt Engineering, to automatically translate legacy incident\nresponse playbooks into the standardized, machine-readable CACAO format. We\nsystematically examine various Prompt Engineering techniques and carefully\ndesign prompts aimed at maximizing syntactic accuracy and semantic fidelity for\ncontrol flow preservation. Our modular transformation pipeline integrates a\nsyntax checker to ensure syntactic correctness and features an iterative\nrefinement mechanism that progressively reduces syntactic errors. We evaluate\nthe proposed approach on a custom-generated dataset comprising diverse legacy\nplaybooks paired with manually created CACAO references. The results\ndemonstrate that our method significantly improves the accuracy of playbook\ntransformation over baseline models, effectively captures complex workflow\nstructures, and substantially reduces errors. It highlights the potential for\npractical deployment in automated cybersecurity playbook transformation tasks."}
