<div id=toc></div>

# Table of Contents

- [math.ST](#math.ST) [Total: 4]
- [math.OC](#math.OC) [Total: 10]
- [math.NT](#math.NT) [Total: 8]
- [math.LO](#math.LO) [Total: 2]
- [math.GM](#math.GM) [Total: 2]
- [math.CO](#math.CO) [Total: 22]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.DM](#cs.DM) [Total: 2]


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [1] [Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives](https://arxiv.org/abs/2506.19025)
*Sivaraman Balakrishnan,Tudor Manole,Larry Wasserman*

Main category: math.ST

TL;DR: 本文综述了最优传输(OT)映射的最新研究进展，包括从样本中估计OT映射的方法及其极限定理，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在最优传输的众多应用中，核心关注点是最优传输映射，它通过最小化特定成本实现概率分布间的最优质量重排。

Method: 回顾了基于样本估计OT映射的方法，并建立了基础OT框架下及特殊变体的极限定理。

Result: 总结了OT映射估计的理论成果，包括标准设定及变体情况下的类似结论。

Conclusion: 提出未来研究的关键方向，旨在为实践者提供可靠的统计推断工具。

Abstract: In many applications of optimal transport (OT), the object of primary
interest is the optimal transport map. This map rearranges mass from one
probability distribution to another in the most efficient way possible by
minimizing a specified cost. In this paper we review recent advances in
estimating and developing limit theorems for the OT map, using samples from the
underlying distributions. We also review parallel lines of work that establish
similar results for special cases and variants of the basic OT setup. We
conclude with a discussion of key directions for future research with the goal
of providing practitioners with reliable inferential tools.

</details>


### [2] [Regularity of the score function in generative models](https://arxiv.org/abs/2506.19559)
*Arthur Stéphanovitch*

Main category: math.ST

TL;DR: 本文研究了基于分数的生成模型中分数函数的正则性，证明其能自然适应数据分布的平滑性，并建立了支持扩散和ODE生成模型收敛性与稳定性的Lipschitz估计。


<details>
  <summary>Details</summary>
Motivation: 探索分数函数在生成模型中的正则性特性，以支持模型的理论分析和实际应用。

Method: 在最小假设下建立Lipschitz估计，并推导高阶正则性边界，简化神经网络近似分数函数的最优论证。

Result: 分数函数能自适应数据分布的平滑性，Lipschitz估计直接支持扩散和ODE生成模型的收敛与稳定性分析。

Conclusion: 研究为基于分数的生成模型提供了理论支持，简化了分数函数近似的最优论证，推动了生成模型的发展。

Abstract: We study the regularity of the score function in score-based generative
models and show that it naturally adapts to the smoothness of the data
distribution. Under minimal assumptions, we establish Lipschitz estimates that
directly support convergence and stability analyses in both diffusion and
ODE-based generative models. In addition, we derive higher-order regularity
bounds, which simplify existing arguments for optimally approximating the score
function using neural networks.

</details>


### [3] [Generative model for optimal density estimation on unknown manifold](https://arxiv.org/abs/2506.19587)
*Arthur Stéphanovitch*

Main category: math.ST

TL;DR: 提出了一种在未知低维流形上估计概率分布的生成模型，该模型通过几何适应实现极小极大最优收敛率，并在实验中优于Wasserstein GAN和基于分数的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的概率分布估计方法在低维流形数据上难以达到最优收敛率，需要一种能适应数据几何结构的生成模型。

Method: 基于Fefferman对几何Whitney问题的解决方案，构建了一个与数据支撑流形正则性匹配的子流形上的估计器，适用于所有$\gamma \geq 1$的H\"older积分概率度量。

Result: 实验表明，该模型在合成和真实数据集上表现优异，性能优于或与Wasserstein GAN及基于分数的生成模型相当。

Conclusion: 通过几何适应设计的生成模型能够实现极小极大最优收敛率，为低维流形数据分布估计提供了有效工具。

Abstract: We propose a generative model that achieves minimax-optimal convergence rates
for estimating probability distributions supported on unknown low-dimensional
manifolds. Building on Fefferman's solution to the geometric Whitney problem,
our estimator is itself supported on a submanifold that matches the regularity
of the data's support. This geometric adaptation enables the estimator to be
simultaneously minimax-optimal for all \( \gamma \)-H\"older Integral
Probability Metrics (IPMs) with \( \gamma \geq 1 \). We validate our approach
through experiments on synthetic and real datasets, demonstrating competitive
or superior performance compared to Wasserstein GAN and score-based generative
models.

</details>


### [4] [Copula-Based Modeling of Fractional Inaccuracy: A Unified Framework](https://arxiv.org/abs/2506.19748)
*Aman Pandey,Chanchal Kundu*

Main category: math.ST

TL;DR: 本文引入了基于多元累积连接函数和生存连接函数的新型信息论测度，扩展了分数不准确度的概念，并通过连接函数整合依赖结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将分数不准确度概念推广到多元设置，并通过连接函数捕捉变量间的依赖关系。

Method: 方法包括构建多元累积连接函数分数不准确度测度和生存连接函数分数不准确度测度，利用Frechet-Hoeffding边界建立测度界限，并研究其在随机序下的行为。

Result: 结果表明，这些测度在上下正交随机序下具有可比较性，并进一步定义了多元共连接函数和双连接函数分数不准确度测度。

Conclusion: 结论指出，这些新型测度为多元依赖结构分析提供了理论工具，并展示了其在比较分析中的潜在应用价值。

Abstract: We introduce novel information-theoretic measures termed the multivariate
cumulative copula fractional inaccuracy measure and the multivariate survival
copula fractional inaccuracy measure, constructed respectively from
multivariate copulas and multivariate survival copulas. These measures
generalize the concept of fractional inaccuracy to multivariate settings by
incorporating dependence structures through copulas. We establish bounds for
these measures using the Frechet-Hoeffding bounds and investigate their
behavior under lower and upper orthant stochastic orderings to facilitate
comparative analysis. Furthermore, we define the multivariate co-copula
fractional inaccuracy measure and the multivariate dual copula fractional
inaccuracy measure, derived from the multivariate co-copula and dual copula,
respectively, and examine several analogous properties for these extended
forms.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [5] [First-Order Sparse Convex Optimization: Better Rates with Sparse Updates](https://arxiv.org/abs/2506.19075)
*Dan Garber*

Main category: math.OC

TL;DR: 该研究针对具有稀疏最优解的凸优化问题，提出了一种仅使用稀疏更新的方法，实现了线性收敛率，并显著降低了每次迭代的运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然能实现依赖于改进混合范数条件数$\frac{\beta_1{}s}{\alpha_2}$的线性收敛率，但无法利用最优解的稀疏性降低每次迭代的运行时间，限制了其在高维问题中的应用。

Method: 提出了一种仅使用稀疏更新的优化方法，该方法不仅保持了线性收敛率，还显著降低了每次迭代的计算复杂度。

Result: 新方法在保持线性收敛率的同时，通过稀疏更新显著降低了运行时间，且实现更为简单。

Conclusion: 该研究为高维稀疏优化问题提供了一种高效且易于实现的解决方案，具有重要的理论和实践意义。

Abstract: In was recently established that for convex optimization problems with a
sparse optimal solution (may it be entry-wise sparsity or matrix rank-wise
sparsity) it is possible to have linear convergence rates which depend on an
improved mixed-norm condition number of the form $\frac{\beta_1{}s}{\alpha_2}$,
where $\beta_1$ is the $\ell_1$-Lipchitz continuity constant of the gradient,
$\alpha_2$ is the $\ell_2$-quadratic growth constant, and $s$ is the sparsity
of the optimal solution. However, beyond the improved convergence rate, these
methods are unable to leverage the sparsity of optimal solutions towards
improving also the runtime of each iteration, which may still be prohibitively
high for high-dimensional problems. In this work, we establish that linear
convergence rates which depend on this improved condition number can be
obtained using only sparse updates, which may result in overall significantly
improved running times. Moreover, our methods are considerably easier to
implement.

</details>


### [6] [Global regularity of the value function in a stopper vs. singular-controller game](https://arxiv.org/abs/2506.19129)
*Andrea Bovo,Alessandro Milazzo*

Main category: math.OC

TL;DR: 研究一类零和随机博弈，涉及停止者与奇异控制者，分析其价值函数的正则性及自由边界结构。


<details>
  <summary>Details</summary>
Motivation: 探索奇异控制动力学下的零和随机博弈，旨在理解价值函数的正则性及其对玩家最优策略的影响。

Method: 基于有限时间范围内的抛物型变分不等式，结合空间导数和障碍约束，分析价值函数的连续性。

Result: 证明价值函数在$[0,T)\times\mathcal{O}$上属于$C^1$类，且二阶空间导数和混合导数在除停止边界外的区域连续。

Conclusion: 价值函数的正则性为研究随机博弈的自由边界及玩家最优策略提供了重要基础。

Abstract: We study a class of zero-sum stochastic games between a stopper and a
singular-controller, previously considered in [Bovo and De Angelis (2025)]. The
underlying singularly-controlled dynamics takes values in
$\mathcal{O}\subseteq\mathbb{R}$. The problem is set on a finite time-horizon
and is connected to a parabolic variational inequality of min-max type with
spatial-derivative and obstacle constraints.
  We show that the value function of the problem is of class $C^1$ in the whole
domain $[0,T)\times\mathcal{O}$ and that the second-order spatial derivative
and the second-order mixed derivative are continuous everywhere except for a
(potential) jump across a non-decreasing curve (the stopping boundary of the
game). The latter discontinuity is a natural consequence of the partial
differential equation associated to the problem. Beyond its intrinsic
analytical value, such a regularity for the value function is a stepping stone
for further exploring the structure and properties of the free-boundaries of
the stochastic game, which in turn determine the optimal strategies of the
players.

</details>


### [7] [Relative Explanations for Contextual Problems with Endogenous Uncertainty: An Application to Competitive Facility Location](https://arxiv.org/abs/2506.19155)
*Jasone Ramírez-Ayerbe,Emma Frejinger*

Main category: math.OC

TL;DR: 本文提出了一种针对具有内生不确定性的上下文随机优化问题的解释方法，特别关注二元决策变量问题。通过引入Wasserstein距离作为正则化项，显著提升了计算效率，并在竞争性设施选址问题中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在实践应用中，决策结果需要具备可解释性和可信度。针对具有内生不确定性的优化问题（决策会影响底层概率分布），现有方法缺乏对二元决策变量的专门处理。本文旨在填补这一空白。

Method: 采用Wasserstein距离作为正则化项并计算下界，相比未正则化方法大幅减少计算时间。提出相对反事实解释框架，为从业者提供满足特定约束所需的上下文协变量具体变化方案。

Result: 数值实验表明，该方法能高效计算稀疏且可解释的反事实解释。在竞争性设施选址案例中，计算时间显著优于未正则化方法，同时保持解释的直观性。

Conclusion: 本研究首次针对二元决策变量和内生不确定性问题提出了可解释的优化解决方案。Wasserstein距离的正则化应用为类似问题提供了计算效率提升的新思路，其解释框架具有实际应用价值。

Abstract: In this paper, we consider contextual stochastic optimization problems
subject to endogenous uncertainty, where the decisions affect the underlying
distributions. To implement such decisions in practice, it is crucial to ensure
that their outcomes are interpretable and trustworthy. To this end, we compute
relative counterfactual explanations, providing practitioners with concrete
changes in the contextual covariates required for a solution to satisfy
specific constraints. Whereas relative explanations have been introduced in
prior literature, to the best of our knowledge, this is the first work focused
on problems with binary decision variables and subject to endogenous
uncertainty. We propose a methodology that uses Wasserstein distance as
regularization and to compute a lower bound. It leads to a drastic reduction in
computation times, compared to the unregularized counterpart. We illustrate the
method using a choice-based competitive facility location problem, and present
numerical experiments that demonstrate its ability to efficiently compute
sparse and interpretable explanations.

</details>


### [8] [Duality and Policy Evaluation in Distributionally Robust Bayesian Diffusion Control](https://arxiv.org/abs/2506.19294)
*Jose Blanchet,Jiayi Cheng,Hao Liu,Yang Liu*

Main category: math.OC

TL;DR: 本文研究了一种贝叶斯扩散控制问题，通过引入分布鲁棒贝叶斯控制（DRBC）框架来应对模型误设，并开发了高效的计算方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，贝叶斯最优控制依赖于对未知漂移的先验分布假设，但先验误设会显著影响策略性能。为避免过度悲观，需要一种鲁棒性更强的控制方法。

Method: 提出分布鲁棒贝叶斯控制（DRBC）框架，控制器与对手在基线先验的散度邻域内博弈。结合随机分析工具，推导出可通过神经网络高效训练的损失函数。

Result: 在Kullback-Leibler分布不确定性集下，DRBC最优策略的计算方法大幅简化，并通过数值实验验证了算法的有效性。

Conclusion: DRBC框架有效解决了模型误设问题，简化了最优策略的计算，为实际应用提供了可行的解决方案。

Abstract: We consider a Bayesian diffusion control problem of expected terminal utility
maximization. The controller imposes a prior distribution on the unknown drift
of an underlying diffusion. The Bayesian optimal control, tracking the
posterior distribution of the unknown drift, can be characterized explicitly.
However, in practice, the prior will generally be incorrectly specified, and
the degree of model misspecification can have a significant impact on policy
performance. To mitigate this and reduce overpessimism, we introduce a
distributionally robust Bayesian control (DRBC) formulation in which the
controller plays a game against an adversary who selects a prior in divergence
neighborhood of a baseline prior. The adversarial approach has been studied in
economics and efficient algorithms have been proposed in static optimization
settings. We develop a strong duality result for our DRBC formulation.
Combining these results together with tools from stochastic analysis, we are
able to derive a loss that can be efficiently trained (as we demonstrate in our
numerical experiments) using a suitable neural network architecture. As a
result, we obtain an effective algorithm for computing the DRBC optimal
strategy. The methodology for computing the DRBC optimal strategy is greatly
simplified, as we show, in the important case in which the adversary chooses a
prior from a Kullback-Leibler distributional uncertainty set.

</details>


### [9] [A Stochastic Electric Vehicle Routing Problem under Uncertain Energy Consumption](https://arxiv.org/abs/2506.19426)
*Andrea Spinelli,Dario Bezzi,Ola Jabali,Francesca Maggioni*

Main category: math.OC

TL;DR: 本文研究随机电动车辆路径问题（SEVRP-T），考虑能源消耗的不确定性，并提出一种两阶段随机混合整数二阶锥模型及启发式算法以优化路线总时长。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车（EVs）在物流配送中的广泛应用，其有限续航和充电需求带来了新的路径规划挑战。现有研究多假设能源消耗确定，而实际中存在不确定性，因此需考虑随机性以优化运营效率。

Method: 提出两阶段随机混合整数二阶锥模型：第一阶段确定客户访问顺序，第二阶段加入充电活动。为降低计算复杂度，设计基于迭代局部搜索（ILS）和集合划分问题的启发式算法，并开发两种下界以加速求解。采用场景缩减技术处理大量能源消耗场景。

Result: 大量计算实验验证了所提策略的有效性，并证明考虑能源消耗随机性的重要性。启发式算法显著提升了求解效率，同时场景缩减技术有效处理了大规模问题。

Conclusion: 本研究丰富了电动车辆路径问题（EVRP）的文献，为不确定环境下电动汽车在物流活动中的运营部署提供了实用见解。所提出的模型和算法为实际应用提供了可靠工具。

Abstract: The increasing adoption of Electric Vehicles (EVs) for service and goods
distribution operations has led to the emergence of Electric Vehicle Routing
Problems (EVRPs), a class of vehicle routing problems addressing the unique
challenges posed by the limited driving range and recharging needs of EVs.
While the majority of EVRP variants have considered deterministic energy
consumption, this paper focuses on the Stochastic Electric Vehicle Routing
Problem with a Threshold recourse policy (SEVRP-T), where the uncertainty in
energy consumption is considered, and a recourse policy is employed to ensure
that EVs recharge at Charging Stations (CSs) whenever their State of Charge
(SoC) falls below a specified threshold. We formulate the SEVRP-T as a
two-stage stochastic mixed-integer second-order cone model, where the first
stage determines the sequences of customers to be visited, and the second stage
incorporates charging activities. The objective is to minimize the expected
total duration of the routes, composed by travel times and recharging
operations. To cope with the computational complexity of the model, we propose
a heuristic based on an Iterated Local Search (ILS) procedure coupled with a
Set Partitioning problem. To further speed up the heuristic, we develop two
lower bounds on the corresponding first-stage customer sequences. Furthermore,
to handle a large number of energy consumption scenarios, we employ a scenario
reduction technique. Extensive computational experiments are conducted to
validate the effectiveness of the proposed solution strategy and to assess the
importance of considering the stochastic nature of the energy consumption. The
research presented in this paper contributes to the growing body of literature
on EVRP and provides insights into managing the operational deployment of EVs
in logistics activities under uncertainty.

</details>


### [10] [Fast convergence of a primal-dual dynamical system with implicit Hessian damping and Tikhonov regularization](https://arxiv.org/abs/2506.19545)
*Hong-lu Li,Xin He,Yi-bin Xiao*

Main category: math.OC

TL;DR: 本文提出了两种解决线性等式约束凸优化问题的原始-对偶动力系统，分析了其快速收敛特性，并首次在仅要求凸性和L-光滑性的条件下，证明了原始-对偶间隙的$o(\frac{1}{t^2})$收敛速度和速度$\dot{x}(t)$的$o(\frac{1}{t})$收敛速度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发无需额外假设（仅需凸性和L-光滑性）即可实现快速收敛的原始-对偶动力系统，填补理论分析空白，并验证Tikhonov正则化系统轨迹强收敛于最小范数解的特性。

Method: 提出两种动力系统：1) 仅含隐式Hessian阻尼；2) 结合Tikhonov正则化。通过理论分析比较收敛速率，并进行数值实验验证。

Result: 两种系统均实现原始-对偶间隙$o(\frac{1}{t^2})$和速度$o(\frac{1}{t})$的收敛速率。Tikhonov正则化系统轨迹强收敛于最小范数解，且数值实验显示即使目标函数仅连续可微，轨迹仍保持平滑。

Conclusion: 该工作为原始-对偶动力系统提供了首个无需强假设的快速收敛理论框架，Tikhonov正则化可确保强收敛性，数值结果与理论一致，拓展了非光滑优化问题的应用潜力。

Abstract: This paper proposes two primal-dual dynamical systems for solving linear
equality constrained convex optimization problems: one with implicit Hessian
damping only, and the other further incorporating Tikhonov regularization. We
analyze the fast convergence properties of both dynamical systems and show that
they achieve the same convergence rates. To the best of our knowledge, this
work provides the first theoretical analysis establishing a convergence rate
$o(\frac{1}{t^2})$ for the primal-dual gap and a convergence rate
$o(\frac{1}{t})$ for the velocity $\dot{x}(t)$, without imposing additional
assumptions on the objective function beyond convexity and L-smoothness.
Moreover, we show that the trajectory generated by the dynamical system with
Tikhonov regularization converges strongly to the minimum-norm solution of the
underlying problem. Finally, numerical experiments are conducted to validate
the theoretical findings. Interestingly, the trajectories exhibit smooth
behavior even when the objective function is only continuously differentiable.

</details>


### [11] [Integrated Balanced and Staggered Routing in Autonomous Mobility-on-Demand Systems](https://arxiv.org/abs/2506.19722)
*Antonio Coppola,Gerhard Hiermann,Dario Paccagnan,Michel Gendreau,Maximilian Schiffer*

Main category: math.OC

TL;DR: 本文提出了一种联合优化路径选择和出发时间的统一框架，用于自主按需出行（AMoD）系统，通过平衡和错峰策略显著减少交通延误和拥堵。


<details>
  <summary>Details</summary>
Motivation: 自主按需出行（AMoD）系统通过集中控制提供比传统叫车服务更优的交通流和运营成本，但需解决路径选择和出发时间优化的联合问题。

Method: 研究构建了一个优化模型，基于Vickrey瓶颈模型的离散版本估计行程时间，并开发了一种基于大邻域搜索框架的元启发式算法来解决大规模问题。

Result: 在曼哈顿街网的案例研究中，该方法在纯AMoD车辆场景下减少了25%的交通延误和35%的网络拥堵；在混合交通场景中，无论运营商目标如何，均实现了双赢效果。

Conclusion: 平衡和错峰策略的实施使AMoD和传统交通均受益，验证了集中控制在优化城市交通系统中的潜力。

Abstract: Autonomous mobility-on-demand (AMoD) systems, centrally coordinated fleets of
self-driving vehicles, offer a promising alternative to traditional
ride-hailing by improving traffic flow and reducing operating costs.
Centralized control in AMoD systems enables two complementary routing
strategies: balanced routing, which distributes traffic across alternative
routes to ease congestion, and staggered routing, which delays departures to
smooth peak demand over time. In this work, we introduce a unified framework
that jointly optimizes both route choices and departure times to minimize
system travel times. We formulate the problem as an optimization model and show
that our congestion model yields an unbiased estimate of travel times derived
from a discretized version of Vickrey's bottleneck model. To solve large-scale
instances, we develop a custom metaheuristic based on a large neighborhood
search framework. We assess our method through a case study on the Manhattan
street network using real-world taxi data. In a setting with exclusively
centrally controlled AMoD vehicles, our approach reduces total traffic delay by
up to 25 percent and mitigates network congestion by up to 35 percent compared
to selfish routing. We also consider mixed-traffic settings with both AMoD and
conventional vehicles, comparing a welfare-oriented operator that minimizes
total system travel time with a profit-oriented one that optimizes only the
fleet's travel time. Independent of the operator's objective, the analysis
reveals a win-win outcome: across all control levels, both autonomous and
non-autonomous traffic benefit from the implementation of balancing and
staggering strategies.

</details>


### [12] [On the computation of the cosine measure in high dimensions](https://arxiv.org/abs/2506.19723)
*Warren Hare,Scholar Sun*

Main category: math.OC

TL;DR: 本文针对高维无导数优化中的余弦测度计算难题，提出新问题表述与启发式算法，并与现有方法对比，同时给出构建特定余弦测度集合的新结果以建立测试基准。


<details>
  <summary>Details</summary>
Motivation: 随着高维无导数优化问题日益受关注，余弦测度作为直接搜索法收敛分析的关键指标，其计算被证明是NP难问题，亟需高效解决方案。

Method: 提出新的问题表述及启发式算法处理高维场景，并与文献现有算法进行系统性比较。

Result: 展示了构建特定余弦测度集合的新结果，为算法性能评估创建了标准化测试集。

Conclusion: 研究为高维无导数优化提供了实用的余弦测度计算工具，并通过可复现的测试框架推动领域算法发展。

Abstract: In derivative-free optimization, the cosine measure is a value that often
arises in the convergence analysis of direct search methods. Given the
increasing interest in high-dimensional derivative-free optimization problems,
it is valuable to compute the cosine measure in this setting; however, it has
recently been shown to be NP-hard. We propose a new formulation of the problem
and heuristic to tackle this problem in higher dimensions and compare it with
existing algorithms in the literature. In addition, new results are presented
to facilitate the construction of sets with specific cosine measures, allowing
for the creation of a test-set to benchmark the algorithms with.

</details>


### [13] [An approach to control design for two-level quantum ensemble systems](https://arxiv.org/abs/2506.19740)
*Ruikang Liang,Gong Cheng*

Main category: math.OC

TL;DR: 本文提出了一种针对无漂移双能级量子系统的可实施控制策略，填补了量子系综系统控制输入构建的研究空白。


<details>
  <summary>Details</summary>
Motivation: 量子系综系统在核磁共振光谱学和鲁棒量子控制等领域有广泛应用，但其控制输入的具体构建方法尚未得到充分研究。

Method: 作者提出了一种完全可实施的控制策略，适用于单参数族的无漂移双能级量子系统，并通过严格分析保证了对SU(2)上目标分布的精确逼近。

Result: 研究通过解析方法建立了收敛性，并提供了数值模拟验证该方法的有效性。

Conclusion: 该控制策略为量子系综系统的精确控制提供了理论保障和实现路径，具有重要的应用价值。

Abstract: Quantum ensemble systems arise in a variety of applications, including NMR
spectroscopy and robust quantum control. While their theoretical properties
have been extensively studied, relatively little attention has been given to
the explicit construction of control inputs. In this paper, we address this gap
by presenting a fully implementable control strategy for a one-parameter family
of driftless two-level quantum systems. The proposed method is supported by
rigorous analysis that guarantees accurate approximation of target
distributions on SU(2). Convergence properties are established analytically,
and numerical simulations are provided to demonstrate the effectiveness of the
approach.

</details>


### [14] [Exact Matrix Seriation through Mathematical Optimization: Stress and Effectiveness-Based Models](https://arxiv.org/abs/2506.19821)
*Víctor Blanco,Alfredo Marín,Justo Puerto*

Main category: math.OC

TL;DR: 本文提出了一种基于数学优化的统一框架来解决矩阵排序问题，通过组合和混合整数优化方法，提高了解决方案的质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 矩阵排序是数据科学中的基础技术，用于揭示关系数据中的潜在结构，但传统启发式方法与精确解法之间存在差距。本文旨在通过优化方法弥合这一差距。

Method: 提出了基于邻域的应力准则的数学规划模型，包括非线性公式及其线性化版本，并针对特定结构（如Moore和von Neumann邻域）开发了基于哈密顿路径的新颖重构方法。

Result: 在合成和真实数据集上的广泛实验表明，优化模型不仅提高了解决方案的质量和可解释性，还为矩阵排序在数据科学中的新应用提供了灵活的基础。

Conclusion: 本文的优化框架为矩阵排序问题提供了严格且通用的解决方案，显著提升了传统方法的性能，并扩展了其应用范围。

Abstract: Matrix seriation, the problem of permuting the rows and columns of a matrix
to uncover latent structure, is a fundamental technique in data science,
particularly in the visualization and analysis of relational data. Applications
span clustering, anomaly detection, and beyond. In this work, we present a
unified framework grounded in mathematical optimization to address matrix
seriation from a rigorous, model-based perspective. Our approach leverages
combinatorial and mixed-integer optimization to represent seriation objectives
and constraints with high fidelity, bridging the gap between traditional
heuristic methods and exact solution techniques.
  We introduce new mathematical programming models for neighborhood-based
stress criteria, including nonlinear formulations and their linearized
counterparts. For structured settings such as Moore and von Neumann
neighborhoods, we develop a novel Hamiltonian path-based reformulation that
enables effective control over spatial arrangement and interpretability in the
reordered matrix.
  To assess the practical impact of our models, we carry out an extensive set
of experiments on synthetic and real-world datasets, as well as on a newly
curated benchmark based on a coauthorship network from the matrix seriation
literature. Our results show that these optimization-based formulations not
only enhance solution quality and interpretability but also provide a versatile
foundation for extending matrix seriation to new domains in data science.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [15] [An Analytic Prime Indicator Based on the Fejer Kernel](https://arxiv.org/abs/2506.18933)
*Sebastian Fuchs*

Main category: math.NT

TL;DR: 本文提出了一种解析素数指示函数，通过平滑三角模拟试除法构建，其零点对应奇数素数，并展示了如何构造全局光滑函数$P_\phi$来保持这一性质，最终应用于素数计数函数$\pi(x)$的解析逼近。


<details>
  <summary>Details</summary>
Motivation: 研究旨在构建一种解析方法，通过光滑函数精确标识素数，为素数分布提供新的分析工具。

Method: 首先构造$C^1$类函数$P$，其零点对应奇数素数；随后通过无穷级数修正为全局光滑的$C^\infty$类函数$P_\phi$，保留素数零点特性。

Result: 成功构建了$P_\phi$函数，其零点严格匹配素数，并基于此推导出$\pi(x)$的解析逼近公式，且误差可控。

Conclusion: 该解析素数指示器为素数理论研究提供了新途径，尤其在素数计数函数的近似计算中展现出实用价值。

Abstract: This note introduces an analytic prime indicator, constructed by smoothing a
trigonometric analogue of trial division. First, a function P: R -> R of class
C1 is presented, whose zeros for x > 2 correspond precisely to the odd primes.
Its second derivative exhibits jump discontinuities at integer squares.
Subsequently, it is shown how this construction can be modified via an infinite
series to yield a globally smooth function, P_phi, of class C-inf that
preserves this prime-zero property. As a primary application, it is
demonstrated how this indicator can be used to construct an analytic
approximation for the prime-counting function pi(x) with a provably controlled
error.

</details>


### [16] [Diophantine approximation on abelian varieties; a conjecture of M. Waldschmidt](https://arxiv.org/abs/2506.19060)
*Lior Fishman,David Lambert,Keith Merrill,David Simmons*

Main category: math.NT

TL;DR: 本文研究了阿贝尔簇上的Diophantine逼近问题，证明了Waldschmidt猜想与特定矩阵的Diophantine条件等价，提出了一个较弱猜想并证明了其上界方向，对于数域$K \subset \mathbb{R}$上的秩1椭圆曲线，得到了弱型Dirichlet定理并证明了猜想的成立。


<details>
  <summary>Details</summary>
Motivation: 研究阿贝尔簇上的Diophantine逼近问题，探索Waldschmidt猜想及其相关矩阵条件的等价性，并提出一个较弱猜想以进一步理解这一领域的数学结构。

Method: 通过分析简单阿贝尔簇上的Diophantine条件，建立猜想与矩阵条件的等价性；提出较弱猜想并证明其上界方向；针对数域$K \subset \mathbb{R}$上的秩1椭圆曲线，推导弱型Dirichlet定理并验证猜想。

Result: 证明了Waldschmidt猜想与特定矩阵的Diophantine条件等价；提出了较弱猜想并确立了其上界方向；对于数域$K \subset \mathbb{R}$上的秩1椭圆曲线，得到了弱型Dirichlet定理并证明了猜想的最优性。

Conclusion: 本文在阿贝尔簇的Diophantine逼近问题上取得了重要进展，不仅验证了Waldschmidt猜想的等价条件，还通过较弱猜想和具体椭圆曲线的分析，深化了对这一领域的理解。

Abstract: Following the work of Waldschmidt, we investigate problems in Diophantine
approximation on abelian varieties. First we show that a conjecture of
Waldschmidt for a given simple abelian variety is equivalent to a well-known
Diophantine condition holding for a certain matrix related to that variety. We
then posit a related but weaker conjecture, and establish the upper bound
direction of that conjecture in full generality. For rank 1 elliptic curves
defined over a number field $K \subset \mathbb{R}$, we then obtain a weak-type
Dirichlet theorem in this setting, establish the optimality of this statement,
and prove our conjecture in this case.

</details>


### [17] [New zero-free regions for Dedekind zeta-functions at small and large ordinates](https://arxiv.org/abs/2506.19319)
*Sourabhashis Das,Swati Gaba,Ethan Simpson Lee,Aditi Savalia,Peng-Jie Wong*

Main category: math.NT

TL;DR: 本文为任意数域$L\neq \mathbb{Q}$的Dedekind zeta函数建立了新的显式无零点区域，改进了Ahn--Kwon、Kadiri和Lee的先前结果，尤其对低零点情形推广了Kadiri的结论并优化了主常数。


<details>
  <summary>Details</summary>
Motivation: 研究数域$L\neq \mathbb{Q}$的Dedekind zeta函数的零点分布，旨在改进现有无零点区域的精度和适用范围，特别是针对低零点情形。

Method: 通过改进分析方法，扩展并优化了Kadiri等人的技术，适用于所有数域，并显著提升了关键常数。

Result: 获得了比前人更优的显式无零点区域，将Kadiri的结果推广至所有数域，且主常数得到改进。

Conclusion: 该研究显著推进了Dedekind zeta函数零点分布的理论，为所有数域提供了更精确的无零点区域，尤其在低零点方面取得突破。

Abstract: Given a number field $L\neq \mathbb{Q}$, we obtain new and explicit zero-free
regions for Dedekind zeta-functions of $L$, which refine the previous works of
Ahn--Kwon, Kadiri, and Lee. In particular, for low-lying zeros, we extend
Kadiri's result to all number fields while improving the main constant.

</details>


### [18] [Counting rational points on transcendental curves in valued fields](https://arxiv.org/abs/2506.19411)
*Floris Vermeulen*

Main category: math.NT

TL;DR: 本文证明了任意1-h极小域上超越曲线的有理点数量上界，扩展了Cluckers-Comte-Loeser在p进域的结果，方法结合了参数化和行列式法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将Pila-Wilkie计数定理从o极小情形推广到混合特征值域，特别是超越曲线的有理点计数问题。

Method: 采用避免使用r次幂映射的参数化方法，结合行列式法进行证明。

Result: 得到了混合特征值域上超越曲线有理点数量的上界，类似o极小情形下的Pila-Wilkie定理。

Conclusion: 该结果将p进域的计数定理推广到更一般的混合特征值域，为超越几何中的有理点研究提供了新工具。

Abstract: We prove upper bounds on the number of rational points on transcendental
curves in arbitrary $1$-h-minimal fields, similar to the Pila--Wilkie counting
theorem in the o-minimal setting. These results extend results due to
Cluckers--Comte--Loeser from $p$-adic fields to arbitrary valued fields of
mixed characteristic. Our methods rely on parametrizations, where we avoid the
usage of $r$-th power maps, combined with the determinant method.

</details>


### [19] [Determining explicitly the Mordell-Weil group of certain rational elliptic surfaces](https://arxiv.org/abs/2506.19423)
*Remke Kloosterman*

Main category: math.NT

TL;DR: 本文针对椭圆曲线$E_{A,B}/\mathbb{Q}(t)$的秩计算问题，提出了比Desjardins和Naskrecki更简洁的几何证明方法，并讨论了该方法的适用场景。


<details>
  <summary>Details</summary>
Motivation: 研究椭圆曲线$E_{A,B}/\mathbb{Q}(t)$的秩计算问题，验证已有算法的正确性，并探索更高效的证明方法。

Method: 采用几何方法重新证明Desjardins和Naskrecki提出的算法，该方法比原证明更简洁直观。

Result: 成功验证了原算法的正确性，并通过几何方法简化了证明过程。

Conclusion: 几何方法在特定类别的椭圆曲线秩计算问题中具有优势，未来可进一步探索其应用范围。

Abstract: Let $A,B$ be nonzero rational numbers. Consider the elliptic curve
$E_{A,B}/\mathbb{Q}(t)$ with Weierstrass equation $y^2=x^3+At^6+B$.
  An algorithm to determine $\mathrm{rank } E_{A,B}(\mathbb{Q}(t))$ as a
function of $(A,B)$ was presented in a recent paper by Desjardins and
Naskrecki. We will give a different and shorter proof for the correctness of
that algorithm, using a more geometric approach and discuss for which classes
of examples this approach might be useful.

</details>


### [20] [Rational isolated $j$-invariants from $X_1(\ell^n)$ and $X_0(\ell^n)$](https://arxiv.org/abs/2506.19560)
*Abbey Bourdon,Özlem Ejder*

Main category: math.NT

TL;DR: 该论文研究了模曲线$X_1(\ell^n)$和$X_0(\ell^n)$上的孤立点，确定了15个有理$j$-不变量和19个对应孤立点的有理$j$-不变量。


<details>
  <summary>Details</summary>
Motivation: 研究模曲线$X_1(\ell^n)$和$X_0(\ell^n)$上的孤立点，以理解这些曲线上不属于无限参数化家族的点。

Method: 通过分析模曲线$X_1(\ell^n)$和$X_0(\ell^n)$的结构，以及自然映射$j:X_1(\ell^n) \rightarrow X_1(1)$，分类孤立点对应的$j$-不变量。

Result: 确定了15个有理$j$-不变量对应于$X_1(\ell^n)$上的孤立点，以及19个有理$j$-不变量对应于$X_0(\ell^n)$上的孤立点。

Conclusion: 该研究完成了Ejder之前的部分分类工作，为模曲线上孤立点的研究提供了完整的分类结果。

Abstract: Let $\ell$ and $n$ be positive integers with $\ell$ prime. The modular curves
$X_1(\ell^n)$ and $X_0(\ell^n)$ are algebraic curves over $\mathbb{Q}$ whose
non-cuspidal points parameterize elliptic curves with a distinguished point of
order $\ell^n$ or a distinguished cyclic subgroup of order $\ell^n$,
respectively. We wish to understand isolated points on these curves, which are
roughly those not belonging to an infinite parameterized family of points
having the same degree. Our first main result is that there are precisely 15
$j$-invariants in $\mathbb{Q}$ which arise as the image of an isolated point
$x\in X_1(\ell^n)$ under the natural map $j:X_1(\ell^n) \rightarrow X_1(1)$.
This completes a prior partial classification of Ejder. We also identify the 19
rational $j$-invariants which correspond to isolated points on $X_0(\ell^n)$.

</details>


### [21] [The Asai--Flach Euler system in $p$-adic families](https://arxiv.org/abs/2506.19673)
*David Loeffler,Arshay Sheth*

Main category: math.NT

TL;DR: 本文证明了Lei、Loeffler和Zerbes(2018)构建的实二次域上Hilbert模特征形式对应的Asai表示的Euler系统可在Hida族中随Hilbert模形式变化而进行$p$进插值。该成果被Grossi、Loeffler和Zerbes(2025)用作证明解析秩为零时Asai表示Bloch-Kato猜想的重要依据。


<details>
  <summary>Details</summary>
Motivation: 研究Hilbert模特征形式对应的Asai表示Euler系统的$p$进插值性质，为后续证明Bloch-Kato猜想提供理论基础。

Method: 通过Hida族理论，将实二次域上Hilbert模特征形式的Euler系统进行$p$进插值分析。

Result: 成功实现了Asai表示对应Euler系统在Hida族变化下的$p$进插值，支持了Bloch-Kato猜想在解析秩为零情况的证明。

Conclusion: 该工作不仅拓展了Euler系统的插值理论，还为Asai表示相关猜想的证明提供了关键工具。

Abstract: We show that the Euler system for the Asai representation corresponding to a
Hilbert modular eigenform over a real quadratic field, constructed by Lei,
Loeffler and Zerbes (2018), can be interpolated $p$-adically as the Hilbert
modular form varies in a Hida family. This work is used as an important input
in recent work of Grossi, Loeffler and Zerbes (2025) on the proof of the
Bloch--Kato conjecture in analytic rank zero for the Asai representation.

</details>


### [22] [On the Asymptotic Density of a GCD-based Map](https://arxiv.org/abs/2506.19812)
*Thang Pang Ern,Malcolm Tan Jun Xi*

Main category: math.NT

TL;DR: 论文研究了函数$f(a,b)=\frac{\operatorname{gcd}(ab,a+b)}{\operatorname{gcd}(a,b)}$的对称性及其解的结构，揭示了其与$\operatorname{SL}_2(\mathbb{Z})$作用的关系，并给出了解的密度估计。


<details>
  <summary>Details</summary>
Motivation: 探索函数$f(a,b)$的对称性来源及其解的普遍结构，理解其在数论中的意义。

Method: 利用$\operatorname{SL}_2(\mathbb{Z})$在原始对上的作用，分析$f(a,b)=n$的解，并应用中国剩余定理。

Result: 发现$f(a,b)=1$的密度趋近于$\prod_p(1-p^{-2}(p+1)^{-1})\approx0.88151$，且高阶类似函数$f_r$在$r\ge2$时的极限密度为$6/\pi^2$。

Conclusion: 研究揭示了函数$f(a,b)$的深刻对称性和解的普遍结构，为相关数论问题提供了新的视角。

Abstract: We show that the symmetry of
\[f\left(a,b\right)=\frac{\operatorname{gcd}\left(ab,a+b\right)}{\operatorname{gcd}\left(a,b\right)}\]
stems from an $\operatorname{SL}_2\left(\mathbb{Z}\right)$ action on primitive
pairs and that all solutions to $f\left(a,b\right)=n$ admit a uniform
three-parameter description -- recovering arithmetic-progression families via
the Chinese remainder theorem when $n$ is squarefree. It shows that the density
of pairs with $f\left(a,b\right)=1$ tends to
$\prod_p\left(1-p^{-2}(p+1)^{-1}\right)\approx0.88151$, and that its
higher-order analogue $f_r$ has a limiting density $6/\pi^2$ for $r\ge2$.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [23] [Indiscernible extraction at small large cardinals from a higher-arity stability notion](https://arxiv.org/abs/2506.19147)
*James E. Hanson*

Main category: math.LO

TL;DR: 该论文引入基于$k$-分裂的高阶稳定性概念，证明了有界$k$-分裂理论在$k$-不可言说基数上具有更好的不可辨提取性，并构建了强反例表明NIP理论可能对所有$k$无界$k$-分裂。


<details>
  <summary>Details</summary>
Motivation: 研究高阶稳定性概念及其在模型论中的应用，特别是探索$k$-分裂与不可辨序列、NIP理论之间的深层关系。

Method: 1. 定义$k$-分裂及其有界性 2. 结合Kaplan-Shelah构造法 3. 使用基数特性分析与反例构建

Result: 1. 有界$k$-分裂理论提升$k$-不可言说基数上的不可辨提取 2. 发现奇数$k>1$时存在有界$k$-分裂但无界$(k-1)$-分裂的理论 3. 证明有界$k$-分裂蕴含$\mathrm{NFOP}_k$但逆命题不成立 4. 无树性可强化为$\mathrm{NFOP}_2$

Conclusion: 高阶分裂概念为稳定性理论提供新视角，其与NIP的复杂关系揭示了模型论中尚未探索的深层结构特性，无树性结果则完善了现有理论框架。

Abstract: We introduce a higher-arity stability notion defined in terms of
$k$-splitting, a higher-arity generalization of splitting. We show that
theories with bounded $k$-splitting have improved indiscernible extraction at
$k$-ineffable cardinals, and we give a non-trivial example of a theory with
bounded $k$-splitting but unbounded $(k-1)$-splitting for each odd $k > 1$. We
also show that bounded $k$-splitting implies $\mathrm{NFOP}_k$, a higher arity
stability notion introduced by Terry and Wolf. We then use our indiscernible
extraction result together with a construction of Kaplan and Shelah to give a
strong counterexample to the converse: an $\mathrm{NIP}$ theory with unbounded
$k$-splitting for every $k$. Finally, as a thematically related but technically
independent result, we show that treelessness implies $\mathrm{NFOP}_2$,
sharpening a result of Kaplan, Ramsey, and Simon.

</details>


### [24] [Fields with Lie-commuting and iterative operators](https://arxiv.org/abs/2506.19489)
*Jan Dobrowolski,Omar Leon Sanchez*

Main category: math.LO

TL;DR: 本文提出了一种研究带有算子的域的一般框架$\mathcal{D}^{\Gamma}$-fields，证明了其核的主实现存在性，并建立了$\mathcal{D}^{\Gamma}$-CF伴随理论。在特征零情况下，该理论具有稳定性并满足CBP与Zilber二分性。此外，还提出了$\mathcal{D}^{\Gamma}$-大域概念，并证明PAC子结构在$\mathcal{D}^{\Gamma}$-DCF中是初等的。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个统一框架来研究具有特定兼容性条件（如导子的Lie交换性）的算子域结构，并探索其模型论性质。

Method: 通过定义$\mathcal{D}^{\Gamma}$-域及核的主实现，采用伴随理论方法，在特征零与正特征场景下分别分析理论性质，并引入$\mathcal{D}^{\Gamma}$-大域概念。

Result: 1) 证明了$\mathcal{D}^{\Gamma}$-核主实现的存在性；2) 建立了伴随理论$\mathcal{D}^{\Gamma}$-CF；3) 特征零下该理论稳定且满足CBP与Zilber二分性；4) 提出$\mathcal{D}^{\Gamma}$-大域并证明PAC子结构的初等性。

Conclusion: 该框架为算子域研究提供了普适工具，$\mathcal{D}^{\Gamma}$-CF的稳定性与模型论性质为后续研究奠定基础，$\mathcal{D}^{\Gamma}$-大域概念扩展了应用场景。

Abstract: We introduce a general framework for studying fields equipped with operators,
given as co-ordinate functions of homomorphisms into a local algebra
$\mathcal{D}$, satisfying various compatibility conditions that we denote by
$\Gamma$ and call such structures $\mathcal{D}^{\Gamma}$-fields. These include
Lie-commutativity of derivations and $\mathfrak g$-iterativity of (truncated)
Hasse-Schmidt derivations. Our main result is about the existence of principal
realisations of $\mathcal{D}^{\Gamma}$-kernels. As an application, we prove
companionability of the theory of $\mathcal{D}^{\Gamma}$-fields and denote the
companion by $\mathcal{D}^{\Gamma}$-CF. In characteristic zero, we prove that
$\mathcal{D}^{\Gamma}$-CF is a stable theory that satisfies the CBP and
Zilber's dichotomy for finite-dimensional types. We also prove that there is a
uniform companion for model-complete theories of large
$\mathcal{D}^{\Gamma}$-fields, which leads to the notion of
$\mathcal{D}^{\Gamma}$-large fields and we further use this to show that PAC
substructures of $\mathcal{D}^{\Gamma}$-DCF are elementary.

</details>


<div id='math.GM'></div>

# math.GM [[Back]](#toc)

### [25] [A Two-Operator Calculus for Arithmetic-Progression Paths in the Collatz Graph](https://arxiv.org/abs/2506.19115)
*Sebastian Angermund*

Main category: math.GM

TL;DR: 论文通过两个基本算子重构了3x+1问题的余类分析，提出了能一次性分割奇偶子序列、计算特定奇偶模式种子集、禁止无限奇移动轨迹的封闭公式，并将非平凡环问题简化为线性同余组。


<details>
  <summary>Details</summary>
Motivation: 研究旨在简化3x+1问题的分析框架，通过算术级数上的算子统一处理奇偶性分类与轨迹约束问题。

Method: 采用两个基本算子重构余类分析，建立封闭公式描述奇偶模式种子集，并推导仿射不变量约束轨迹行为。

Result: 获得四项核心成果：(i)奇偶子序列单步分割；(ii)特定奇偶模式的种子集封闭解；(iii)禁止无限奇移动的单行判据；(iv)非平凡环问题转化为线性同余组。

Conclusion: 该算子演算为3x+1问题提供了更简洁的分析工具，尤其在线性约束和轨迹分类方面展现出高效性。

Abstract: A recast of the standard residue-class analysis of the 3x+1 (Collatz) map in
terms of two elementary operators on arithmetic progressions. The resulting
calculus (i) splits any progression into its even and odd subsequences in a
single step, (ii) gives a closed formula for every set of seeds that realises a
prescribed parity word, (iii) yields a one line affine invariant that forbids
trajectories consisting of infinitely many odd moves, and (iv) reduces the
non-trivial-cycle problem to a pair of linear congruences.

</details>


### [26] [A Closed-Form Symbolic Generator: $A^n + B^n = C^n + D^n$, for $n = 2,3$](https://arxiv.org/abs/2506.19173)
*Jamal Agbanwa*

Main category: math.GM

TL;DR: 本文提出一个统一框架，用于构造满足$A^{n} + B^{n} = C^{n} + D^{n}$（$n=2,3$）的整数解。对于$n=2$，通过平方差推导显式公式；对于$n=3$，引入包含哈代-拉马努金数1729的通用公式，并开发符号生成器无限生成整数解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为二项式展开与丢番图约束之间的结构联系提供新视角，特别是针对高次幂问题的解决方案。

Method: 对于$n=2$利用平方差法；对于$n=3$设计符号生成器，基于代数数$\alpha, \beta$的嵌套根式和指数，通过单变量参数$c_1$的闭式代数参数化表达解。

Result: 成功构造$n=2,3$的显式解公式，并开发首个符号递归生成器，可无限生成$A^3 + B^3 = C^3 + D^3$的整数解，虽未覆盖所有可能解，但提供结构化路径。

Conclusion: 该框架为高次幂丢番图方程提供了新方法，通过连接二项式与丢番图约束的结构，奠定了向更高次幂扩展的基础。

Abstract: We present a unified framework for constructing integer solutions to $A^{n} +
B^{n} = C^{n} + D^{n}$ for $n=2,3$. For $n=2$, we derive explicit formulas for
any solutions via differences of squares. For $n=3$, we introduce general
formulas that include the Hardy-Ramanujan number 1729 for instance, we also
construct a symbolic generator that produces infinitely many integer solutions
to the Diophantine equation A^3 + B^3 = C^3 + D^3 . While the resulting
formulas for $A,B,C,D$ from the symbolic generator developed do not span every
single number expressible as a sum of two positive cubes in at least two
distinct ways, our method provides a closed-form, algebraic parametrization in
terms of a single variable, expressing each term as a radical-exponential
function of an integer parameter $c_1$. The generator leverages nested radicals
and exponents of algebraic numbers, $\alpha, \beta$ derived from the recurrence
structure of the Diophantine constraint. This work represents the first
symbolic, recursive generator of its kind and offers a pathway toward
approaching higher powers of this problem from a different lens. These methods
exploit structural links between binomial expansions and Diophantine
constraints, offering a foundation for extensions to higher powers.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [27] [Low complexity binary words avoiding $(5/2)^+$-powers](https://arxiv.org/abs/2506.19050)
*Narad Rampersad,James Currie*

Main category: math.CO

TL;DR: 本文证明了避免$(5/2)^+$-幂的Rote词的结构定理，证实了Ollinger和Shallit的猜想。


<details>
  <summary>Details</summary>
Motivation: 研究Rote词的性质及其避免特定幂的能力，特别是在Shallit、Shur、Ollinger和Shallit之前的工作基础上，进一步探索其结构。

Method: 通过结构分析的方法，对避免$(5/2)^+$-幂的Rote词进行深入研究，验证了相关猜想。

Result: 证明了存在避免$(5/2)^+$-幂的Rote词，并且给出了这类词的结构定理，证实了Ollinger和Shallit的猜想。

Conclusion: 该研究不仅确认了避免$(5/2)^+$-幂的Rote词的存在性，还提供了其结构定理，为无限词的研究提供了新的理论支持。

Abstract: Rote words are infinite words that contain $2n$ factors of length $n$ for
every $n \geq 1$. Shallit and Shur, as well as Ollinger and Shallit, showed
that there are Rote words that avoid $(5/2)^+$-powers and that this is best
possible. In this note we give a structure theorem for the Rote words that
avoid $(5/2)^+$-powers, confirming a conjecture of Ollinger and Shallit.

</details>


### [28] [Rational Exponents for General Graphs](https://arxiv.org/abs/2506.19061)
*Sean English,Sam Spiro*

Main category: math.CO

TL;DR: 该论文首次针对任意图$H$证明了可实现指数的存在性，并给出了具体区间，同时对树的广义Turán数稳定性进行了研究。


<details>
  <summary>Details</summary>
Motivation: 目前关于可实现指数的研究仅限于星图或团图，Bukh和Conlon关于$H=K_2$的突破性工作激发了更一般的图$H$的可实现指数研究需求。

Method: 通过分析图$H$的最大度$\Delta$，构建特定图族$\mathcal{F}$，并利用改进的Helly定理对树结构进行稳定性证明。

Result: 证明了对任意最大度$\Delta \ge 1$的图$H$，区间$\left[v(H)-\frac{e(H)}{2\Delta^2},\ v(H)\right]$内的所有有理数都是可实现指数；且非$K_2$的树在$[0,\ell]\setminus \mathbb{Z}$区间无可实现指数。

Conclusion: 该研究将可实现指数理论推广到一般图类，提出的树结构稳定性证明方法及改进的Helly定理具有独立的理论价值。

Abstract: A rational number $r$ is a \textbf{realizable exponent} for a graph $H$ if
there exists a finite family of graphs $\mathcal{F}$ such that
$\mathrm{ex}(n,H,\mathcal{F})=\Theta(n^r)$, where
$\mathrm{ex}(n,H,\mathcal{F})$ denotes the maximum number of copies of $H$ that
an $n$-vertex $\mathcal{F}$-free graph can have. Results for realizable
exponents are currently known only when $H$ is either a star or a clique, with
the full resolution of the $H=K_2$ case being a major breakthrough of Bukh and
Conlon.
  In this paper, we establish the first set of results for realizable exponents
which hold for arbitrary graphs $H$ by showing that for any graph $H$ with
maximum degree $\Delta \ge 1$, every rational in the interval
$\left[v(H)-\frac{e(H)}{2\Delta^2},\ v(H)\right]$ is realizable for $H$. We
also prove a ``stability'' result for generalized Tur\'an numbers of trees
which implies that if $T\ne K_2$ is a tree with $\ell$ leaves, then $T$ has no
realizable exponents in $[0,\ell]\setminus \mathbb{Z}$. Our proof of this
latter result uses a new variant of the classical Helly theorem for trees,
which may be of independent interest.

</details>


### [29] [On Gyárfás' Path-Colour Problem](https://arxiv.org/abs/2506.19100)
*Ben Cameron,Alexander Clow*

Main category: math.CO

TL;DR: 该论文研究了Gy\'{a}rf\'{a}s提出的关于图着色问题的猜想，通过构造性证明展示了对于所有自然数r，存在满足条件的图G，并探讨了在禁止特定诱导子图情况下的着色问题。


<details>
  <summary>Details</summary>
Motivation: 研究源于Gy\'{a}rf\'{a}s在1997年提出的猜想：若图中每条路径生成的子图是3-可着色的，则整个图是k-可着色的。论文旨在验证或反驳这一猜想，并探索在特定图类中的着色性质。

Method: 通过构造性证明，展示了对于任意自然数r，存在图G满足r(G)≤r且χ(G)≥⌊3r/2⌋−1。此外，研究了在禁止特定诱导子图（如K_{1,t}或爪形图）的图中，着色数与r(G)的关系。

Result: 证明了对于所有常数k，存在图满足χ−r>k。在禁止特定诱导子图的图中，如K_{1,t}-free图（t≥4）中，χ(G)≤(t−1)(r(G)+\binom{t−1}{2}−3)；在爪形图-free图中，χ(G)≤2r(G)。还定义了路径完美图，并证明了一些特定条件下的路径完美性质。

Conclusion: 论文不仅反驳了Gy\'{a}rf\'{a}s猜想中的常数k的存在性，还扩展了对特定图类着色性质的理解，特别是在禁止诱导子图的条件下，为图的着色理论提供了新的见解。

Abstract: In their 1997 paper titled ``Fruit Salad", Gy\'{a}rf\'{a}s posed the
following conjecture: there exists a constant $k$ such that if each path of a
graph spans a $3$-colourable subgraph, then the graph is $k$-colourable. It is
noted that $k=4$ might suffice. Let $r(G)$ be the maximum chromatic number of
any subgraph $H$ of $G$ where $H$ is spanned by a path. The only progress on
this conjecture comes from Randerath and Schiermeyer in 2002, who proved that
if $G$ is an $n$ vertex graph, then $\chi(G) \leq r(G)\log_{\frac{8}{7}}(n)$.
  We prove that for all natural numbers $r$, there exists a graph $G$ with
$r(G)\leq r$ and $\chi(G)\geq \lfloor\frac{3r}{2}\rfloor -1$. Hence, for all
constants $k$ there exists a graph with $\chi - r > k$. Our proof is
constructive.
  We also study this problem in graphs with a forbidden induced subgraph. We
show that if $G$ is $K_{1,t}$-free, for $t\geq 4$, then $\chi(G) \leq
(t-1)(r(G)+\binom{t-1}{2}-3)$. If $G$ is claw-free, then we prove $\chi(G) \leq
2r(G)$. Additionally, the graphs $G$ where every induced subgraph $G'$ of $G$
satisfy $\chi(G') = r(G')$ are considered. We call such graphs path-perfect, as
this class generalizes perfect graphs. We prove that if $H$ is a forest with at
most $4$ vertices other than the claw, then every $H$-free graph $G$ has
$\chi(G) \leq r(G)+1$. We also prove that if $H$ is additionally not isomorphic
to $2K_2$ or $K_2+2K_1$, then all $H$-free graphs are path-perfect.

</details>


### [30] [Solution to a problem on isolation of $3$-vertex paths](https://arxiv.org/abs/2506.19149)
*Karl Bartolo,Peter Borg,Dayle Scicluna*

Main category: math.CO

TL;DR: 本文研究了无诱导6-环的连通图中3-路径隔离数的上界，证明了$f(n) = \left \lfloor (n+1)/4 \right \rfloor$，并验证了$\limsup_{n \to\infty}\frac{f(n)}{n} = \frac{1}{4}$。


<details>
  <summary>Details</summary>
Motivation: 研究无诱导6-环的连通图中3-路径隔离数的渐近行为，解决Huang等人提出的问题。

Method: 通过证明若图$G$满足$\iota(G, P_3) = (n+1)/4$，则对每个顶点$v$有$\iota(G-v, P_3) < \iota(G, P_3)$，并分析最大度数的影响。

Result: 证明了$f(n) = \left \lfloor (n+1)/4 \right \rfloor$，且当图的最大度数至少为5时，$\iota(G,P_3) \leq n/4$。

Conclusion: 该结果验证了Huang等人的猜想，并为无诱导6-环图的3-路径隔离数提供了精确上界，新方法有望进一步应用。

Abstract: The $3$-path isolation number of a connected $n$-vertex graph $G$, denoted by
$\iota(G,P_3)$, is the size of a smallest subset $D$ of the vertex set of $G$
such that the closed neighbourhood $N[D]$ of $D$ in $G$ intersects each
$3$-vertex path of $G$, meaning that no two edges of $G-N[D]$ intersect. Zhang
and Wu proved that $\iota(G,P_3) \leq 2n/7$ unless $G$ is a $3$-path or a
$3$-cycle or a $6$-cycle. The bound is attained by infinitely many graphs
having induced $6$-cycles. Huang, Zhang and Jin proved that if $G$ has no
$6$-cycles, or $G$ has no induced $5$-cycles and no induced $6$-cycles, then
$\iota(G, P_3) \leq n/4$ unless $G$ is a $3$-path or a $3$-cycle or a $7$-cycle
or an $11$-cycle. They asked if the bound still holds asymptotically for
connected graphs having no induced $6$-cycles. More precisely, taking $f(n)$ to
be the maximum value of $\iota(G,P_3)$ over all connected $n$-vertex graphs $G$
having no induced $6$-cycles, their question is whether $\limsup_{n
\to\infty}\frac{f(n)}{n} = \frac{1}{4}$. We verify this by proving that $f(n) =
\left \lfloor (n+1)/4 \right \rfloor$. The proof hinges on further proving that
if $G$ is such a graph and $\iota(G, P_3) = (n+1)/4$, then $\iota(G-v, P_3) <
\iota(G, P_3)$ for each vertex $v$ of $G$. This new idea promises to be of
further use. We also prove that if the maximum degree of such a graph $G$ is at
least $5$, then $\iota(G,P_3) \leq n/4$.

</details>


### [31] [Upper Chromatic Numbers: An Update](https://arxiv.org/abs/2506.19126)
*Aaron Abrams*

Main category: math.CO

TL;DR: 2000年关于上色数的综述论文


<details>
  <summary>Details</summary>
Motivation: 探讨图论中上色数的研究进展及其重要性

Method: 综述性研究，整理和分析已有文献

Result: 总结了上色数的定义、性质及相关研究成果

Conclusion: 该综述为上色数的研究提供了系统的参考

Abstract: This is a survey written in 2000 about upper chromatic numbers

</details>


### [32] [Complete polyhedral description of chemical graphs of maximum degree at most 3](https://arxiv.org/abs/2506.19768)
*Valentin Dusollier,Sébastien Bonte,Gauvain Devillez,Alain Hertz,Hadrien Mélot,David Schindl*

Main category: math.CO

TL;DR: 该研究探讨了化学图中基于度的拓扑指数的极值问题，通过将问题转化为多面体极值点求解，证明了在最大度数不超过3的情况下，极值图类别不超过16种。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决化学图中基于度的拓扑指数在给定顶点数$n$和边数$m$条件下的极值问题，为分子物理化学性质研究提供理论支持。

Method: 将化学图的极值问题转化为包含最多10个面的多面体极值点求解，并限制化学图的最大度数不超过3。

Result: 证明了极值点数量不超过16个，表明无论选择何种基于度的拓扑指数，极值图的类别都非常有限。

Conclusion: 该研究为化学图拓扑指数的极值问题提供了简洁的解决方案，揭示了极值图类别的有限性，对分子描述符研究具有重要意义。

Abstract: Chemical graphs are simple undirected connected graphs, where vertices
represent atoms in a molecule and edges represent chemical bonds. A
degree-based topological index is a molecular descriptor used to study specific
physicochemical properties of molecules. Such an index is computed from the sum
of the weights of the edges of a chemical graph, each edge having a weight
defined by a formula that depends only on the degrees of its endpoints. Given
any degree-based topological index and given two integers $n$ and $m$, we are
interested in determining chemical graphs of order $n$ and size $m$ that
maximize or minimize the index. Focusing on chemical graphs with maximum degree
at most 3, we show that this reduces to determining the extreme points of a
polytope that contains at most 10 facets. We also show that the number of
extreme points is at most 16, which means that for any given $n$ and $m$, there
are very few different classes of extremal graphs, independently of the chosen
degree-based topological index.

</details>


### [33] [Yet Another Species of Forbidden-distances Chromatic Number](https://arxiv.org/abs/2506.19151)
*Aaron Abrams,Peter Johnson*

Main category: math.CO

TL;DR: 2001年论文提出了一种新的点集色数类型


<details>
  <summary>Details</summary>
Motivation: 研究点集着色问题的新视角

Method: 引入新型色数定义方法

Result: 建立了点集色数的新理论框架

Conclusion: 该新型色数为组合几何研究提供了新工具

Abstract: This 2001 paper introduces a new type of chromatic number for point sets.

</details>


### [34] [Finding congruences with the WZ method](https://arxiv.org/abs/2506.19221)
*Li-Quan Feng,Qing-Hu Hou*

Main category: math.CO

TL;DR: 使用Wilf-Zeilberger方法建立截断Ramanujan型级数的同余式，验证了Sun的部分猜想并发现新同余关系。


<details>
  <summary>Details</summary>
Motivation: 研究截断Ramanujan型级数的同余性质，验证数学猜想并探索新的同余关系。

Method: 通过构造超几何项$f(k, a, b, \ldots)$并选择合适参数，利用Gosper可和差分性质推导同余式。

Result: 证明了对于素数$p > 2$，多个截断级数满足模$p$或$p^2$的同余关系，例如$\sum_{n=0}^{p-1} \frac{10n+3}{2^{3n}}\binom{3n}{n}\binom{2n}{n}^2 \equiv 0 \pmod{p}$。

Conclusion: 该方法有效建立了新型同余式，部分证实了Sun的猜想，为相关领域提供了新的理论工具。

Abstract: We utilize the Wilf-Zeilberger (WZ) method to establish congruences related
to truncated Ramanujan-type series. By constructing hypergeometric terms $f(k,
a, b, \ldots)$ with Gosper-summable differences and selecting appropriate
parameters, we derive several congruences modulo $p$ and $p^2$ for primes $p >
2$. For instance, we prove that for any prime $p > 2$, \[ \sum_{n=0}^{p-1}
\frac{10n+3}{2^{3n}}\binom{3n}{n}\binom{2n}{n}^2 \equiv 0 \pmod{p},\] and \[
\sum_{n=0}^{p-1} \frac{(-1)^n(20n^2+8n+1)}{2^{12n}}\binom{2n}{n}^5 \equiv 0
\pmod{p^2}. \] These results partially confirm conjectures by Sun and provide
some novel congruences.

</details>


### [35] [Inequalities related to the coefficients of the $j$-function](https://arxiv.org/abs/2506.19292)
*Zhongjie Li*

Main category: math.CO

TL;DR: 本文研究了$j$-函数的傅里叶系数$c(n)$的上下界，并建立了相关不等式。


<details>
  <summary>Details</summary>
Motivation: 近年来，组合序列及其根序列的对数凹凸性、高阶Tur{\'a}n不等式和二阶Laguerre不等式被广泛研究，但$j$-函数的傅里叶系数$c(n)$的研究仅限于其渐近形式。

Method: 通过给出$c(n)$的适当上下界，建立了与之相关的不等式。

Result: 确定了$c(n)$的上下界，并建立了相关不等式。

Conclusion: 本文填补了$j$-函数傅里叶系数$c(n)$研究的空白，为其建立了具体的上下界和相关不等式。

Abstract: In recent years, the log-concavity or log-convexity of combinatorial
sequences and their root sequences, higher order Tur{\'a}n inequalities, and
Laguerre inequalities of order two have been widely studied. However, the
research of the Fourier coefficient $c(n)$ of the $j$-function is limited to
its asymptotic form. In this paper, we give the appropriate upper and lower
bounds of $c(n)$ to establish the inequalities associated with it.

</details>


### [36] [Undecidability of Translational Tiling of the Plane with Four Tiles](https://arxiv.org/abs/2506.19295)
*Chao Yang,Zhujun Zhang*

Main category: math.CO

TL;DR: 本文证明了平面平移拼接问题在仅使用4个（不连通）多边形块时仍为不可判定问题，进一步降低了此前已知的5块下限。


<details>
  <summary>Details</summary>
Motivation: 平移拼接问题是离散几何与组合数学中最具代表性的不可判定问题之一。自Ollinger 2009年首次证明11块多边形不可判定性以来，学界持续探索最小块数下限。

Method: 通过改进前人关于多联骨牌（polyominoes）的构造方法，将不可判定性证明所需的拼图块数从5个优化至4个。

Result: 最终证明：使用4个不连通的多联骨牌进行平面平移拼接的问题属于不可判定问题。

Conclusion: 该研究将平移拼接不可判定性的最小块数纪录推进至4块，为计算几何领域的基础理论提供了新的边界值。

Abstract: The translational tiling problem, dated back to Wang's domino problem in the
1960s, is one of the most representative undecidable problems in the field of
discrete geometry and combinatorics. Ollinger initiated the study of the
undecidability of translational tiling with a fixed number of tiles in 2009,
and proved that translational tiling of the plane with a set of $11$
polyominoes is undecidable. The number of polyominoes needed to obtain
undecidability was reduced from $11$ to $7$ by Yang and Zhang, and then to $5$
by Kim. We show that translational tiling of the plane with a set of $4$
(disconnected) polyominoes is undecidable in this paper.

</details>


### [37] [Sturmian lattices and Aperiodic tile sets](https://arxiv.org/abs/2506.19362)
*Shigeki Akiyama,Tadahisa Hamada,Katsuki Ito*

Main category: math.CO

TL;DR: 基于二次斜率Sturmian词的非周期瓦片集构造方法，适用于任何二次无理斜率，并能生成无限多非周期瓦片集，其缩放常数为实二次域的单位。


<details>
  <summary>Details</summary>
Motivation: 探索利用Sturmian词和Delone集的有界位移等价性，构建具有特定数学性质的非周期瓦片集。

Method: 结合Sturmian格子（由Sturmian词生成的网格结构）和Delone集的有界位移等价性，构造非周期瓦片集。

Result: 成功构建了基于二次斜率Sturmian词的非周期瓦片集，且缩放常数为实二次域的单位。

Conclusion: 该方法不仅扩展了非周期瓦片集的构造范围，还揭示了Sturmian格子与Delone集在非周期结构中的重要作用。

Abstract: We give aperiodic tile sets based on Sturmian words of quadratic slopes. The
method works for any quadratic irrational slope and we can produce infinitely
many aperiodic tile sets whose underlying scaling constant is a unit of any
real quadratic field. There are two key ingredients in our construction. The
first one is ``Sturmian lattices'', an interesting grid structure generated by
Sturmian words that emerged in an aperiodic monotile called Smith Turtle.The
second is the bounded displacement equivalence of Delone sets, which plays a
central role in this construction.

</details>


### [38] [Closed-Form Decomposition for Simplicial Cones and PDBarv Algorithm for Lattice Point Counting](https://arxiv.org/abs/2506.19322)
*Sihao Tao,Guoce Xin,Zihao Zhang*

Main category: math.CO

TL;DR: 本文提出了一种新的混合算法PDBarv，通过结合原始和对偶Barvinok算法，并采用新的加速策略，显著提高了计算性能。


<details>
  <summary>Details</summary>
Motivation: 计算有理多面体内的格点数量是一个基础性问题，但现有方法在处理锥体边界点时存在困难，或对某些锥体类型效率低下。

Method: 首先推导了生成函数的闭式表达式，支持更有效的原始空间分解；其次通过从较小索引的一侧分解锥体及其对偶锥体，提出了混合算法PDBarv。

Result: PDBarv算法在5维情况下平均计算性能提升超过20%，在更高维度中表现更优。

Conclusion: PDBarv算法通过结合原始和对偶方法的优势，并引入新的加速策略，显著提升了格点计数问题的计算效率。

Abstract: Counting lattice points within a rational polytope is a foundational problem
with applications across mathematics and computer science. A key approach is
Barvinok's algorithm, which decomposes the lattice point generating function of
cones to that of unimodular cones. However, standard implementations face
difficulties: the original primal method struggles with points on cone
boundaries, while the alternative dual method can be slow for certain cone
types.
  This paper introduces two main contributions. First, We derive a closed-form
expression for these generating functions using arbitrary lattice point
decompositions, enabling more effective primal space decomposition. Second, by
decomposing both the cone and its dual cone starting from the side with a
smaller index, we develop a novel algorithm called \textup{PDBarv}. This hybrid
approach integrates the primal and dual Barvinok algorithms with a novel
acceleration strategy, achieving an average computational performance
improvement of over 20\% in dimension 5 and even better in higher dimensions.

</details>


### [39] [Polytopality criteria for the mix of polytopes and maniplexes](https://arxiv.org/abs/2506.19334)
*Gabe Cunningham,Isabel Hubard*

Main category: math.CO

TL;DR: 本文研究两个maniplex的混合是否仍为多面体，提出了一个普适性判定准则，推广了多个已知的多面体性判定标准。


<details>
  <summary>Details</summary>
Motivation: 探索两个maniplex混合后的多面体性质，特别是当其中一个为抽象多面体时，混合结果是否保持多面体结构。该问题在寻找maniplex的最小正则覆盖等应用中具有重要意义。

Method: 通过构建最小覆盖两个maniplex的混合maniplex，建立了一个通用判定框架，将多个已知的多面体性判定条件纳入统一理论体系。

Result: 提出了一个普适性准则，可判定混合maniplex的多面体性质。该准则推广了先前已知的多个特例判定标准，具有更广泛的适用性。

Conclusion: 研究成果为maniplex混合结构的多面体性判定提供了统一的理论工具，扩展了相关领域的认知边界，对组合几何与多面体理论具有重要推进作用。

Abstract: The mix of two maniplexes is the minimal maniplex that covers both. This
construction has many important applications, such as finding the smallest
regular cover of a maniplex. If one of the maniplexes is an abstract polytope,
a natural question to ask is whether the mix is also a polytope. We describe
here a general criterion for the polytopality of the mix which generalizes
several previously-known polytopality criteria.

</details>


### [40] [From dual canonical bases to positroidal subdivisions](https://arxiv.org/abs/2506.19443)
*Jian-Rong Li,Ayush Kumar Tewari*

Main category: math.CO

TL;DR: 该研究探讨了Grassmannian簇代数$\mathbb{C}[\text{Gr}(k, n)]$的双典范基与超单纯形$\Delta(k,n)$的正向拟阵细分之间的深刻联系，特别关注了$\text{Gr}(2,n)$的情况，并提出了关于$\Delta(k,n)$分裂正向拟阵细分数量的猜想公式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解Grassmannian簇代数的双典范基与超单纯形$\Delta(k,n)$的几何结构之间的联系，特别是通过半标准Young表与正向拟阵细分的对应关系，揭示两者之间的深层组合性质。

Method: 研究方法包括利用Speyer和Williams引入的映射，将半标准Young表与正向拟阵细分联系起来，并通过计算实验验证$k>2$时的结果。此外，还提出了一个关于$\Delta(k,n)$分裂正向拟阵细分数量的猜想公式。

Result: 研究结果表明，对于$\text{Gr}(2,n)$，非冻结素表恰好对应于$\Delta(2,n)$的最粗正向拟阵细分。计算证据进一步支持这些结果可以推广到$k>2$的情况。

Conclusion: 结论强调了$\Delta(k,n)$的多面体组合学与$\mathbb{C}[\text{Gr}(k, n)]$的双典范基之间的深刻联系，并提出了未来研究的方向，特别是关于分裂正向拟阵细分数量的猜想。

Abstract: The Grassmannian cluster algebra $\mathbb{C}[\text{Gr}(k, n)]$ admits a
distinguished basis known as the dual canonical basis, whose elements
correspond to rectangular semi-standard Young tableaux with $k$ rows and with
entries in $[n]$. We establish that each such tableau induces a positroidal
subdivision of the hypersimplex $\Delta(k,n)$ via a map introduced by Speyer
and Williams. For $\text{Gr}(2,n)$, we prove that non-frozen prime tableaux
correspond precisely to the coarsest positroidal subdivisions of $\Delta(2,n)$.
Furthermore, we present computational evidence extending these results to
$k>2$. In the process, we formulate a conjectural formula for the number of
split positroidal subdivisions of $\Delta(k,n)$ for any $k \ge 2$ and explore
the deep connections between the polyhedral combinatorics of $\Delta(k,n)$ and
the dual canonical basis of $\mathbb{C}[\text{Gr}(k, n)]$.

</details>


### [41] [Study of higher-order interactions in unweighted, undirected networks using persistent homology](https://arxiv.org/abs/2506.19448)
*Udit Raj,Slobodan Maletić,Sudeepto Bhattacharya*

Main category: math.CO

TL;DR: 该研究通过构建加权单纯邻接矩阵和广义加权介数中心性，提出了计算持续同调的新方法，并建立了最大广义度中心性以分析网络拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 研究高阶相互作用（HoIs）能更全面地理解复杂系统（如生态系统和生物系统）的结构特性，持续同调能揭示网络中非成对相互作用形成的高阶结构信息。

Method: 利用无向无权网络中的团复形构建加权单纯邻接矩阵，计算广义加权介数中心性，并建立最大广义度中心性，提出三种基于全局和局部度量的过滤方案。

Result: 通过实际网络计算了二维以内的Betti数，验证了所提方法的有效性，并比较了不同顶点相互作用形成的高阶结构拓扑。

Conclusion: 该方法可推广至图论情形（一维单纯复形），为分析复杂系统的高阶相互作用提供了新的拓扑视角和计算工具。

Abstract: Persistent homology has been studied to better understand the structural
properties and topology features of weighted networks. It can reveal hidden
layers of information about the higher-order structures formed by non-pairwise
interactions in a network. Studying of higher-order interactions (HoIs) of a
system provides a more comprehensive understanding of the complex system;
moreover, it is a more precise depiction of the system as many complex systems,
such as ecological systems and biological systems, etc., demonstrate HoIs. In
this study, the weighted simplicial adjacency matrix has been constructed using
the concept of adjacency strength of simplices in a clique complex obtained
from an unweighted, undirected network. This weighted simplicial adjacency
matrix is thus used to calculate the global measure, which is called
generalised weighted betweenness centrality, which further helps us in
calculating the persistent homology on the given simplicial complex by
constructing a filtration on it. Moreover, a local measure called maximal
generalised degree centrality has also been established for better
understanding of the network topology of the studied simplicial complex. All
the generalizations given in this work can be reduced to the graph-theoretic
case. i.e., for a simplicial complex of dimension 1. Three different filtration
schemes for constructing the sequence of simplicial complexes have been given
with the help of both global and local measures, and by using these measures,
the topology of higher-order structures of the studied network due to the
interactions of their vertices has been compared. Further, the illustration of
established definitions has been given using a real-life network by calculating
Betti numbers up to dimension two.

</details>


### [42] [Word-Representable Graphs and Locality of Words](https://arxiv.org/abs/2506.19493)
*Philipp Böll,Pamela Fleischmann,Annika Huch,Jana Kreiß,Tim Löck,Kajus Park,Max Wiedenhöft*

Main category: math.CO

TL;DR: 本文研究了$k$-可表示图与$k$-局部词可表示图的关系，证明了后者是$(k+1)$-可表示的，并探讨了这两类图的遗传性质与速度度量。


<details>
  <summary>Details</summary>
Motivation: 研究$k$-可表示图与$k$-局部词可表示图之间的关系，以深化对图表示理论的理解，并探索其遗传性质与计算复杂性。

Method: 通过理论分析，重新审视了$1$-局部词可表示图的结果，并研究了这两类图在遗传性质和速度度量方面的特性。

Result: 证明了$k$-局部词可表示图是$(k+1)$-可表示的，且这些图属于阶乘层并具有有界的团宽度。

Conclusion: 本研究扩展了对图表示理论的认识，证明了$k$-局部词可表示图的$(k+1)$-可表示性，并揭示了其在遗传性质和计算复杂性方面的重要特性。

Abstract: In this work, we investigate the relationship between $k$-repre\-sentable
graphs and graphs representable by $k$-local words. In particular, we show that
every graph representable by a $k$-local word is $(k+1)$-representable. A
previous result about graphs represented by $1$-local words is revisited with
new insights. Moreover, we investigate both classes of graphs w.r.t. hereditary
and in particular the speed as a measure. We prove that the latter ones belong
to the factorial layer and that the graphs in this classes have bounded
clique-width.

</details>


### [43] [The optimal binding function for (cap, even hole)-free graphs](https://arxiv.org/abs/2506.19580)
*Ran Chen,Baogang Xu,Yian Xu*

Main category: math.CO

TL;DR: 该论文研究了特定图类的着色问题，证明了在特定条件下，图的色数与其团数之间存在明确关系，并解决了Cameron等人的一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 研究图的着色性质，特别是针对不含特定子图（如cap、偶洞）的图类，解决Cameron等人提出的关于色数上界的问题。

Method: 通过定义洞、偶洞和cap等图结构，引入团膨胀的概念，并利用数学归纳和不等式推导，证明了色数与团数之间的关系。

Result: 证明了对于不含cap和偶洞的图，色数满足$\chi(G)\leq\lceil\frac{5}{4}\omega(G)\rceil$；对于不含cap、偶洞和5-洞的图，色数满足$\chi(G)\leq\lceil\frac{7}{6}\omega(G)\rceil$，且该界限可达。

Conclusion: 该研究不仅解决了Cameron等人的问题，还为特定图类的着色性质提供了新的理论结果，展示了团膨胀方法在图论中的有效性。

Abstract: A {\em hole} is an induced cycle of length at least 4, an {\em even hole} is
a hole of even length, and a {\em cap} is a graph obtained from a hole by
adding an additional vertex which is adjacent exactly to two adjacent vertices
of the hole. A graph $G$ obtained from a graph $H$ by blowing up all the
vertices into cliques is said to be a clique blowup of $H$. Let $p, q$ be two
positive integers with $p>2q$, let $F$ be a triangle-free graph, and let $G'$
be a clique blowup of $F$ with $\omega(G')\leq\max\{\frac{2q(p-q-2)}{p-2q},
2q\}$. In this paper, we prove that for any clique blowup $G$ of $F$,
$\chi(G)\leq\lceil\frac{p}{2q}\omega(G)\rceil$ if and only if
$\chi(G')\leq\lceil\frac{p}{2q}\omega(G')\rceil$. As its consequences, we show
that every (cap, even hole)-free graph $G$ satisfies
$\chi(G)\leq\lceil\frac{5}{4}\omega(G)\rceil$, which affirmatively answers a
question of Cameron {\em et al.} \cite{CdHV2018}, we also show that every (cap,
even hole, 5-hole)-free graph $G$ satisfies
$\chi(G)\leq\lceil\frac{7}{6}\omega(G)\rceil$, and the bound is reachable.

</details>


### [44] [De Bruijn Tori Without Zeros: A Field-Theoretic Perspective](https://arxiv.org/abs/2506.19605)
*Ming Hsuan Kang,Yu Hsuan Hsieh*

Main category: math.CO

TL;DR: 本文提出了一种基于有限域上迹函数的非零De Bruijn环面代数构造方法，通过乘法独立生成元排列非零域元素，并建立了采样模式与$\mathbb{F}_p$-基的关联性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在构建省略全零模式的非零De Bruijn环面，为有限域上的结构化排列提供理论框架与应用基础。

Method: 采用两个乘法独立生成元在环面网格上排列非零域元素，利用固定线性映射（通常为域迹）获取值，并通过基于乘法平移的递归更新规则实现高效计算。

Result: 证明了采样模式对应的域元素构成$\mathbb{F}_p$-基，且列结构由子域上不可约多项式确定的De Bruijn序列循环移位产生。

Conclusion: 该代数构造不仅揭示了De Bruijn环面与域论的内在联系，其递归计算机制也为实际应用提供了高效实现途径。

Abstract: We present an algebraic construction of trace-based De Bruijn tori over
finite fields, focusing on the nonzero variant that omits the all-zero pattern.
The construction arranges nonzero field elements on a toroidal grid using two
multiplicatively independent generators, with values obtained by applying a
fixed linear map, typically the field trace.
  We characterize sampling patterns as subsets whose associated field elements
form an \( \mathbb{F}_p \)-basis, and show that column structures correspond to
cyclic shifts of De Bruijn sequences determined by irreducible polynomials over
subfields. Recursive update rules based on multiplicative translations enable
efficient computation.

</details>


### [45] [Elliptic arrangements of complex multiplication type](https://arxiv.org/abs/2506.19638)
*Luca Moci,Roberto Pagaria,Maddalena Pismataro,Alejandro Vargas*

Main category: math.CO

TL;DR: 本文提出了一种椭圆排列的自然定义，将其扩展到具有复乘的椭圆曲线E上，分析了排列元素的交集及其连通分量作为End(E)-模的性质，证明了椭圆排列的组合数据同时定义了算术拟阵和环End(E)上的拟阵，获得了一类不同于环面排列实现的算术拟阵，并证明了补集的欧拉示性数是算术Tutte多项式的求值。


<details>
  <summary>Details</summary>
Motivation: 扩展经典椭圆排列框架至具有复乘的椭圆曲线，研究其组合与代数结构，探索新型算术拟阵的构造。

Method: 通过分析椭圆排列元素的交集连通性（作为End(E)-模），建立与算术拟阵及环上拟阵的关联，并计算补集欧拉示性数。

Result: 证明了椭圆排列数据可同时定义算术拟阵和End(E)-环拟阵，其补集欧拉示性数等于算术Tutte多项式特定求值。

Conclusion: 该研究构建了一类新型算术拟阵（区别于环面排列实现类），揭示了椭圆排列组合与算术Tutte多项式之间的深刻联系。

Abstract: We provide a natural definition of an elliptic arrangement, extending the
classical framework to an elliptic curve E with complex multiplication. We
analyse the intersections of elements of the arrangement and their connected
components as End(E)-modules. Furthermore, we prove that the combinatorial data
of elliptic arrangements define both an arithmetic matroid and a matroid over
the ring End(E). In this way, we obtain a class of arithmetic matroids that is
different from the class of arithmetic matroids realizable via toric
arrangements. Finally, we show that the Euler characteristic of the complement
is an evaluation of the arithmetic Tutte polynomial.

</details>


### [46] [Infinite polynomial patterns in large subsets of the rational numbers](https://arxiv.org/abs/2506.19667)
*Ethan Ackelsberg*

Main category: math.CO

TL;DR: 该论文受Kra等人的问题启发，证明了有理数大子集中无限多项式配置的两个新结果：有限着色下存在无限集$B$使$\{b_i, b_i^2 + b_j : i < j\}$单色，且正密度子集包含此类配置的平移。方法结合组合动力学与多项式遍历平均，并引入新工具。


<details>
  <summary>Details</summary>
Motivation: 研究受Kra、Moreira、Richter和Robertson关于有理数无限多项式配置问题的启发，旨在扩展对有理数子集结构的理解。

Method: 通过将组合问题转化为动力系统问题，利用多项式遍历平均的Wiener-Wintner定理和Abramov $\mathbb{Q}$-系统的结构定理，结合Kra等人的系列方法。

Result: 证明了两个主要结果：有限着色下存在无限单色多项式配置，以及正密度子集必然包含此类配置的平移。

Conclusion: 论文通过动力学方法解决了有理数中的多项式配置问题，并讨论了整数中相关问题的可能扩展。

Abstract: Inspired by a question of Kra, Moreira, Richter, and Robertson, we prove two
new results about infinite polynomial configurations in large subsets of the
rational numbers. First, given a finite coloring of $\mathbb{Q}$, we show that
there exists an infinite set $B = \{b_n : n \in \mathbb{N}\} \subseteq
\mathbb{Q}$ such that $\{b_i, b_i^2 + b_j : i < j\}$ is monochromatic. Second,
we prove that every subset of positive density in the rational numbers contains
a translate of such an infinite configuration.
  The proofs build upon methods developed in a series of papers by Kra,
Moreira, Richter, and Robertson to translate from combinatorics into dynamics,
where the core of the argument reduces to understanding the behavior of certain
polynomial ergodic averages. The new dynamical tools required for this analysis
are a Wiener--Wintner theorem for polynomially-twisted ergodic averages in
$\mathbb{Q}$-systems and a structure theorem for Abramov $\mathbb{Q}$-systems.
  The end of the paper includes a discussion of related problems in the
integers.

</details>


### [47] [The Origami flip graph of the $2\times n$ Miura-ori](https://arxiv.org/abs/2506.19700)
*Lumi Christensen,Thomas C. Hull,Emma O'Neil,Valentina Pappano,Natalya Ter-Saakov,Kacey Yang*

Main category: math.CO

TL;DR: 本文研究了Miura-ori折纸图案$M_{2,n}$的局部有效山-谷分配及其翻转图$OFG(M_{2,n})$的性质，包括顶点和边的数量、顶点度数的多项式描述以及图的直径。


<details>
  <summary>Details</summary>
Motivation: Miura-ori折纸图案在工程中有广泛应用，研究其局部有效的山-谷分配及其翻转图的性质有助于理解其折叠行为的数学基础。

Method: 通过建立折纸翻转图$OFG(M_{2,n})$，分析其顶点和边的数量，并利用递推关系和3-着色重构图技术研究顶点度数和图的直径。

Result: 证明了$OFG(M_{2,n})$中特定度数顶点的数量由多项式描述，且图的直径为$\lceil \frac{n^2}{2}\rceil$。

Conclusion: 研究揭示了Miura-ori折纸图案$M_{2,n}$的局部有效山-谷分配及其翻转图的数学结构，为折纸工程应用提供了理论支持。

Abstract: Given an origami crease pattern $C=(V,E)$, a straight-line planar graph
embedded in a region of $\mathbb{R}^2$, we assign each crease to be either a
mountain crease (which bends convexly) or a valley crease (which bends
concavely), creating a mountain-valley (MV) assignment $\mu:E\to\{-1,1\}$. An
MV assignment $\mu$ is locally valid if the faces around each vertex in $C$ can
be folded flat under $\mu$. In this paper, we investigate locally valid MV
assignments of the Miura-ori, $M_{m,n}$, an $m\times n$ parallelogram
tessellation used in numerous engineering applications. The origami flip graph
$OFG(C)$ of $C$ is a graph whose vertices are locally valid MV assignments of
$C$, and two vertices are adjacent if they differ by a face flip, an operation
that swaps the MV-parity of every crease bordering a given face of $C$. We
enumerate the number of vertices and edges in $OFG(M_{2,n})$ and prove several
facts about the degrees of vertices in $OFG(M_{2,n})$. By finding recurrence
relations, we show that the number of vertices of degree $d$ and $2n-a$ (for
$0\leq a$) are both described by polynomials of particular degrees. We then
prove that the diameter of $OFG(M_{2,n})$ is $\lceil \frac{n^2}{2}\rceil$ using
techniques from 3-coloring reconfiguration graphs.

</details>


### [48] [The Hamilton cycle space of random graphs](https://arxiv.org/abs/2506.19731)
*Dan Hefetz,Michael Krivelevich*

Main category: math.CO

TL;DR: 该论文证明了对于奇数个顶点的随机图$G \sim \mathbb{G}(n,p)$，当最小度数$\delta(G) \geq 3$时，几乎必然其Hamilton圈能生成整个圈空间$C_n(G) = C(G)$。


<details>
  <summary>Details</summary>
Motivation: 研究随机图中Hamilton圈与圈空间的关系，解决Christoph等人提出的问题，验证Heinig观察到的必要条件$\delta(G) \geq 3$的充分性。

Method: 采用随机图理论的分析方法，结合概率工具和代数图论中的圈空间概念，研究$G \sim \mathbb{G}(n,p)$的渐进性质。

Result: 对于奇数$n$的随机图，几乎必然$\delta(G) \geq 3$时，Hamilton圈的生成向量能张成整个圈空间$C(G)$。

Conclusion: 该结果不仅扩展了随机图Hamilton性的经典结论，还揭示了Hamilton圈在生成圈空间中的重要作用。

Abstract: The cycle space of a graph $G$, denoted $C(G)$, is a vector space over
${\mathbb F}_2$, spanned by all incidence vectors of edge-sets of cycles of
$G$. If $G$ has $n$ vertices, then $C_n(G)$ denotes the subspace of $C(G)$,
spanned by the incidence vectors of Hamilton cycles of $G$. A classical result
in the theory of random graphs asserts that for $G \sim \mathbb{G}(n,p)$,
asymptotically almost surely the necessary condition $\delta(G) \geq 2$ is also
sufficient to ensure Hamiltonicity. Resolving a problem of Christoph, Nenadov,
and Petrova, we augment this result by proving that for $G \sim
\mathbb{G}(n,p)$, with $n$ being odd, asymptotically almost surely the
condition $\delta(G) \geq 3$ (observed to be necessary by Heinig) is also
sufficient for ensuring $C_n(G) = C(G)$. That is, not only does $G$ typically
have a Hamilton cycle, but its Hamilton cycles are typically rich enough to
span its cycle space.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [Trustworthy Artificial Intelligence for Cyber Threat Analysis](https://arxiv.org/abs/2506.19052)
*Shuangbao Paul Wang,Paul Mullin*

Main category: cs.CR

TL;DR: 本研究开发了一种基于机器学习的网络威胁检测与评估工具，结合无监督与监督学习两阶段分析AWS云服务器日志数据，能高置信度识别网络威胁。


<details>
  <summary>Details</summary>
Motivation: 人工智能虽带来创新，但算法中的偏见与伦理问题降低了应用可信度。机器学习威胁狩猎算法相比传统方法优势显著，强化学习模型能更精准识别基于签名和行为的威胁。量子力学为分类速度带来指数级提升。

Method: 采用两阶段（无监督+监督）机器学习方法，分析AWS云服务器记录的日志数据。

Result: 算法展现出高置信度识别网络威胁的能力。

Conclusion: 该研究证明了机器学习方法在网络威胁检测中的有效性，尤其通过结合量子力学可进一步提升分类速度。

Abstract: Artificial Intelligence brings innovations into the society. However, bias
and unethical exist in many algorithms that make the applications less
trustworthy. Threats hunting algorithms based on machine learning have shown
great advantage over classical methods. Reinforcement learning models are
getting more accurate for identifying not only signature-based but also
behavior-based threats. Quantum mechanics brings a new dimension in improving
classification speed with exponential advantage. In this research, we developed
a machine learning based cyber threat detection and assessment tool. It uses
two stage, unsupervised and supervised learning, analyzing method on log data
recorded from a web server on AWS cloud. The results show the algorithm has the
ability to identify cyber threats with high confidence.

</details>


### [50] [PolyGuard: Massive Multi-Domain Safety Policy-Grounded Guardrail Dataset](https://arxiv.org/abs/2506.19054)
*Mintong Kang,Zhaorun Chen,Chejian Xu,Jiawei Zhang,Chengquan Guo,Minzhou Pan,Ivan Revilla,Yu Sun,Bo Li*

Main category: cs.CR

TL;DR: 本文介绍了PolyGuard，首个基于多领域安全政策的大规模护栏数据集，用于评估和改进大语言模型(LLM)的安全防护能力。该数据集覆盖8个关键领域，包含政策依据的风险构建、多样化交互格式及对抗性攻击样本，并通过对19个先进护栏模型的基准测试揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM广泛应用，其交互安全引发担忧。现有护栏基准缺乏标准化政策依据且忽视领域特异性风险，难以满足实际需求。PolyGuard旨在填补这一空白，提供政策对齐的综合性评估框架。

Method: 构建PolyGuard数据集包含：(1)覆盖金融、法律等8个安全关键领域；(2)基于真实领域安全指南的风险分类；(3)声明、问答等多样化交互形式；(4)通过去毒提示生成良性数据；(5)设计对抗性攻击样本。随后对19个护栏模型进行多维度基准测试。

Result: 实验发现：(1)所有模型F1分数差异显著，对领域特异性风险处理不足；(2)模型迭代会扩大风险覆盖范围，但可能降低常见风险处理性能；(3)所有模型均对优化后的对抗攻击表现脆弱。

Conclusion: PolyGuard作为首个政策依据的多领域护栏数据集，揭示了现有模型的局限性，其评估结果为开发更具韧性和政策对齐的护栏系统提供了重要参考。

Abstract: As LLMs become widespread across diverse applications, concerns about the
security and safety of LLM interactions have intensified. Numerous guardrail
models and benchmarks have been developed to ensure LLM content safety.
However, existing guardrail benchmarks are often built upon ad hoc risk
taxonomies that lack a principled grounding in standardized safety policies,
limiting their alignment with real-world operational requirements. Moreover,
they tend to overlook domain-specific risks, while the same risk category can
carry different implications across different domains. To bridge these gaps, we
introduce PolyGuard, the first massive multi-domain safety policy-grounded
guardrail dataset. PolyGuard offers: (1) broad domain coverage across eight
safety-critical domains, such as finance, law, and codeGen; (2) policy-grounded
risk construction based on authentic, domain-specific safety guidelines; (3)
diverse interaction formats, encompassing declarative statements, questions,
instructions, and multi-turn conversations; (4) advanced benign data curation
via detoxification prompting to challenge over-refusal behaviors; and (5)
\textbf{attack-enhanced instances} that simulate adversarial inputs designed to
bypass guardrails. Based on PolyGuard, we benchmark 19 advanced guardrail
models and uncover a series of findings, such as: (1) All models achieve varied
F1 scores, with many demonstrating high variance across risk categories,
highlighting their limited domain coverage and insufficient handling of
domain-specific safety concerns; (2) As models evolve, their coverage of safety
risks broadens, but performance on common risk categories may decrease; (3) All
models remain vulnerable to optimized adversarial attacks. We believe that
\dataset and the unique insights derived from our evaluations will advance the
development of policy-aligned and resilient guardrail systems.

</details>


### [51] [Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems](https://arxiv.org/abs/2506.19109)
*Valerii Gakh,Hayretdin Bahsi*

Main category: cs.CR

TL;DR: 论文研究了针对LLM的提示注入攻击，特别是提示泄露攻击的检测技术，比较了LLM Guard、Vigil和Rebuff三种开源解决方案的性能，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的软件应用日益普及和多样化，提示注入攻击威胁其安全性，现有防御措施不足，需研究有效的检测技术。

Method: 研究分析了多种提示泄露检测技术，并对LLM Guard、Vigil和Rebuff三种开源解决方案进行了性能比较，评估其检测效果和配置优化。

Result: 发现Vigil和Rebuff中的金丝雀词检查效果不佳，Rebuff的基于次级模型的技术存在规避漏洞；Vigil在最低误报率要求下最优，Rebuff适合一般需求。

Conclusion: 研究揭示了现有提示泄露检测技术的优缺点，提出了改进方案，并推荐了不同场景下的最优解决方案。

Abstract: Prompt injection threatens novel applications that emerge from adapting LLMs
for various user tasks. The newly developed LLM-based software applications
become more ubiquitous and diverse. However, the threat of prompt injection
attacks undermines the security of these systems as the mitigation and defenses
against them, proposed so far, are insufficient. We investigated the
capabilities of early prompt injection detection systems, focusing specifically
on the detection performance of techniques implemented in various open-source
solutions. These solutions are supposed to detect certain types of prompt
injection attacks, including the prompt leak. In prompt leakage attacks, an
attacker maliciously manipulates the LLM into outputting its system
instructions, violating the system's confidentiality. Our study presents
analyzes of distinct prompt leakage detection techniques, and a comparative
analysis of several detection solutions, which implement those techniques. We
identify the strengths and weaknesses of these techniques and elaborate on
their optimal configuration and usage in high-stake deployments. In one of the
first studies on existing prompt leak detection solutions, we compared the
performances of LLM Guard, Vigil, and Rebuff. We concluded that the
implementations of canary word checks in Vigil and Rebuff were not effective at
detecting prompt leak attacks, and we proposed improvements for them. We also
found an evasion weakness in Rebuff's secondary model-based technique and
proposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil,
and Rebuff at their peak performance revealed that Vigil is optimal for cases
when minimal false positive rate is required, and Rebuff is the most optimal
for average needs.

</details>


### [52] [Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning](https://arxiv.org/abs/2506.19260)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.CR

TL;DR: 联邦学习系统中网络拓扑结构可能引发隐私泄露，研究发现不同知识背景的攻击者能推断敏感数据分布模式，并提出结构噪声注入作为有效防御手段。


<details>
  <summary>Details</summary>
Motivation: 现有隐私研究主要关注基于梯度的攻击，而网络拓扑知识对隐私的影响尚未充分研究。本文首次全面分析了基于拓扑的隐私泄露问题。

Method: 通过系统评估4,720个攻击实例，分析六种对抗知识场景（包括完整拓扑知识和五种部分知识配置），提出三种攻击向量：通信模式分析、参数幅度分析和结构位置相关性分析。

Result: 在完整知识条件下，三种攻击成功率分别为84.1%、65.0%和47.2%。80%的部分知识场景攻击效果仍高于安全阈值。结构噪声注入防御机制可额外减少51.4%的攻击。

Conclusion: 网络拓扑是联邦学习系统的根本隐私漏洞，但通过拓扑感知防御机制（如结构噪声注入）可有效缓解这一风险。

Abstract: Federated learning systems increasingly rely on diverse network topologies to
address scalability and organizational constraints. While existing privacy
research focuses on gradient-based attacks, the privacy implications of network
topology knowledge remain critically understudied. We conduct the first
comprehensive analysis of topology-based privacy leakage across realistic
adversarial knowledge scenarios, demonstrating that adversaries with varying
degrees of structural knowledge can infer sensitive data distribution patterns
even under strong differential privacy guarantees. Through systematic
evaluation of 4,720 attack instances, we analyze six distinct adversarial
knowledge scenarios: complete topology knowledge and five partial knowledge
configurations reflecting real-world deployment constraints. We propose three
complementary attack vectors: communication pattern analysis, parameter
magnitude profiling, and structural position correlation, achieving success
rates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.
Critically, we find that 80% of realistic partial knowledge scenarios maintain
attack effectiveness above security thresholds, with certain partial knowledge
configurations achieving performance superior to the baseline complete
knowledge scenario. To address these vulnerabilities, we propose and
empirically validate structural noise injection as a complementary defense
mechanism across 808 configurations, demonstrating up to 51.4% additional
attack reduction when properly layered with existing privacy techniques. These
results establish that network topology represents a fundamental privacy
vulnerability in federated learning systems while providing practical pathways
for mitigation through topology-aware defense mechanisms.

</details>


### [53] [WebGuard++:Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT](https://arxiv.org/abs/2506.19356)
*Ye Tian,Zhang Yumin,Yifan Jia,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: WebGuard++提出了一种融合URL与HTML特征的双向耦合恶意URL检测框架，通过跨尺度编码、子图感知、双向对齐和投票机制解决现有方法的四大缺陷，实验显示检测性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有恶意URL检测方法存在四大不足：URL建模不完整、HTML图稀疏导致威胁信号稀释、URL-HTML特征单向分析、决策过程不透明。WebGuard++旨在解决这些问题。

Method: 1) 跨尺度URL编码器：基于Transformer和动态卷积分层学习URL特征；2) 子图感知HTML编码器：分解DOM图并增强稀疏威胁信号；3) 双向耦合模块：通过跨模态对比学习对齐特征；4) 投票模块：通过恶意子图预测共识定位威胁区域。

Result: 实验表明WebGuard++在TPR指标上比现有最优方法提升1.1-7.9倍（FPR固定为0.001和0.0001），在两个数据集上均表现优异。

Conclusion: 该框架通过多组件协同解决了URL检测的关键挑战，双向特征融合和可解释子图分析为恶意网页检测提供了新方向。

Abstract: URL+HTML feature fusion shows promise for robust malicious URL detection,
since attacker artifacts persist in DOM structures. However, prior work suffers
from four critical shortcomings: (1) incomplete URL modeling, failing to
jointly capture lexical patterns and semantic context; (2) HTML graph sparsity,
where threat-indicative nodes (e.g., obfuscated scripts) are isolated amid
benign content, causing signal dilution during graph aggregation; (3)
unidirectional analysis, ignoring URL-HTML feature bidirectional interaction;
and (4) opaque decisions, lacking attribution to malicious DOM components. To
address these challenges, we present WebGuard++, a detection framework with 4
novel components: 1) Cross-scale URL Encoder: Hierarchically learns
local-to-global and coarse to fine URL features based on Transformer network
with dynamic convolution. 2) Subgraph-aware HTML Encoder: Decomposes DOM graphs
into interpretable substructures, amplifying sparse threat signals via
Hierarchical feature fusion. 3) Bidirectional Coupling Module: Aligns URL and
HTML embeddings through cross-modal contrastive learning, optimizing
inter-modal consistency and intra-modal specificity. 4) Voting Module:
Localizes malicious regions through consensus voting on malicious subgraph
predictions. Experiments show WebGuard++ achieves significant improvements over
state-of-the-art baselines, achieving 1.1x-7.9x higher TPR at fixed FPR of
0.001 and 0.0001 across both datasets.

</details>


### [54] [SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation](https://arxiv.org/abs/2506.19360)
*Yunsung Chung,Yunbei Zhang,Nassir Marrouche,Jihun Hamm*

Main category: cs.CR

TL;DR: 本文系统综述了隐私保护数据合成(PPDS)中的合成图像生成方法，通过生成-采样-分类流程对现有技术进行分类，并利用基准测试和成员推理攻击(MIA)评估隐私风险，旨在回答合成数据能否替代真实数据等关键问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型的进步推动了隐私保护数据合成(PPDS)领域的发展，但缺乏对不同场景下合成图像生成方法的全面调查与比较。研究旨在系统评估合成数据在分类器训练中的效用-隐私权衡。

Method: 研究沿生成-采样-分类流程对现有图像合成方法、隐私攻击及缓解措施进行分类，采用代表性生成方法构建基准测试，并以模型无关的成员推理攻击(MIA)作为隐私风险度量指标。

Result: 通过系统评估多种方法，研究揭示了合成数据生成方法的效用-隐私权衡特性，为不同场景下的最优数据发布策略提供了实证依据。

Conclusion: 该研究为合成数据生成方法的实际应用提供了可操作的见解，指导现实场景中平衡效用与隐私的最优数据发布策略决策。

Abstract: Advances in generative models have transformed the field of synthetic image
generation for privacy-preserving data synthesis (PPDS). However, the field
lacks a comprehensive survey and comparison of synthetic image generation
methods across diverse settings. In particular, when we generate synthetic
images for the purpose of training a classifier, there is a pipeline of
generation-sampling-classification which takes private training as input and
outputs the final classifier of interest. In this survey, we systematically
categorize existing image synthesis methods, privacy attacks, and mitigations
along this generation-sampling-classification pipeline. To empirically compare
diverse synthesis approaches, we provide a benchmark with representative
generative methods and use model-agnostic membership inference attacks (MIAs)
as a measure of privacy risk. Through this study, we seek to answer critical
questions in PPDS: Can synthetic data effectively replace real data? Which
release strategy balances utility and privacy? Do mitigations improve the
utility-privacy tradeoff? Which generative models perform best across different
scenarios? With a systematic evaluation of diverse methods, our study provides
actionable insights into the utility-privacy tradeoffs of synthetic data
generation methods and guides the decision on optimal data releasing strategies
for real-world applications.

</details>


### [55] [Yotta: A Large-Scale Trustless Data Trading Scheme for Blockchain System](https://arxiv.org/abs/2506.19368)
*Xiang Liu,Zhanpeng Guo,Liangxi Liu,Mengyao Zheng,Yiming Qiu,Linshan Jiang*

Main category: cs.CR

TL;DR: 本文首次形式化了Web 3.0中大规模数据交易应满足的属性要求，并提出了基于区块链的完整批量数据交易方案Yotta，该方案结合创新密码学工作流、IPFS和zk-SNARK技术，性能超越基线方法130倍且具备卓越可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于区块链智能合约的数据交换方法无法在保障安全性的同时支持大规模数据交易，且缺乏对Web 3.0数据交易核心属性的系统讨论。

Method: 提出Yotta方案：通过形式化属性需求，设计融合IPFS存储与zk-SNARK零知识证明的创新密码学工作流，实现区块链批量数据交易。

Result: 仿真表明Yotta性能达基线方法的130倍，完美满足所有形式化属性并展现极佳可扩展性。

Conclusion: Yotta是首个符合Web 3.0精神的大规模数据交易解决方案，为区块链数据交易领域建立了理论框架与实践范例。

Abstract: Data trading is one of the key focuses of Web 3.0. However, all the current
methods that rely on blockchain-based smart contracts for data exchange cannot
support large-scale data trading while ensuring data security, which falls
short of fulfilling the spirit of Web 3.0. Even worse, there is currently a
lack of discussion on the essential properties that large-scale data trading
should satisfy. In this work, we are the first to formalize the property
requirements for enabling data trading in Web 3.0. Based on these requirements,
we are the first to propose Yotta, a complete batch data trading scheme for
blockchain, which features a data trading design that leverages our innovative
cryptographic workflow with IPFS and zk-SNARK. Our simulation results
demonstrate that Yotta outperforms baseline approaches up to 130 times and
exhibits excellent scalability to satisfy all the properties.

</details>


### [56] [ZK-SERIES: Privacy-Preserving Authentication using Temporal Biometric Data](https://arxiv.org/abs/2506.19393)
*Daniel Reijsbergen,Eyasu Getahun Chekole,Howard Halim,Jianying Zhou*

Main category: cs.CR

TL;DR: 本文提出ZK-SERIES，一种保护隐私且高效的时间序列认证协议，适用于低性能设备如智能手机，实验显示在旧设备上认证可在1.3秒内完成。


<details>
  <summary>Details</summary>
Motivation: 生物识别认证依赖难以丢失或伪造的生理或行为特征，但现有时间序列比对方法（如DTW、TWED）缺乏隐私保护，且生物凭证重置复杂，亟需兼顾隐私与效率的解决方案。

Method: ZK-SERIES通过零知识乘法证明和批量范围证明构建通用框架，兼容多种时间序列认证协议，并针对智能手机等低容量设备优化，以摇动和吹气认证为案例验证实用性。

Result: 在5年前低配智能手机上的真实数据实验表明，隐私保护认证协议可在1.3秒内完成，人工数据的扩展性评估进一步验证了方案的可行性。

Conclusion: ZK-SERIES为时间序列生物认证提供了隐私保护与效率的平衡，其轻量级设计使老旧设备也能快速完成认证，具有实际应用价值。

Abstract: Biometric authentication relies on physiological or behavioral traits that
are inherent to a user, making them difficult to lose, forge or forget.
Biometric data with a temporal component enable the following authentication
protocol: recent readings of the underlying biometrics are encoded as time
series and compared to a set of base readings. If the distance between the new
readings and the base readings falls within an acceptable threshold, then the
user is successfully authenticated. Various methods exist for comparing time
series data, such as Dynamic Time Warping (DTW) and the Time Warp Edit Distance
(TWED), each offering advantages and drawbacks depending on the context.
Moreover, many of these techniques do not inherently preserve privacy, which is
a critical consideration in biometric authentication due to the complexity of
resetting biometric credentials.
  In this work, we propose ZK-SERIES to provide privacy and efficiency to a
broad spectrum of time series-based authentication protocols. ZK-SERIES uses
the same building blocks, i.e., zero-knowledge multiplication proofs and
efficiently batched range proofs, to ensure consistency across all protocols.
Furthermore, it is optimized for compatibility with low-capacity devices such
as smartphones. To assess the effectiveness of our proposed technique, we
primarily focus on two case studies for biometric authentication: shake-based
and blow-based authentication. To demonstrate ZK-SERIES's practical
applicability even in older and less powerful smartphones, we conduct
experiments on a 5-year-old low-spec smartphone using real data for two case
studies alongside scalability assessments using artificial data. Our
experimental results indicate that the privacy-preserving authentication
protocol can be completed within 1.3 seconds on older devices.

</details>


### [57] [An ETSI GS QKD compliant TLS implementation](https://arxiv.org/abs/2506.19409)
*Thomas Prévost,Bruno Martin,Olivier Alibart*

Main category: cs.CR

TL;DR: 本文提出了一种基于量子密钥分发（QKD）的TLS协议修改方案，保持了客户端和服务端的向后兼容性，并测试了其性能。


<details>
  <summary>Details</summary>
Motivation: 旨在推动量子密钥分发（QKD）在互联网中的广泛应用，通过修改TLS协议以实现更安全的通信。

Method: 基于Rustls库实现ETSI GS QKD 014 v1.1.1标准，修改TLS协议并保持向后兼容性，用于加密视频会议通话。

Result: 分析了协议性能，比较了其与TLS 1.3在建立握手时间上的差异。

Conclusion: 该协议为QKD在互联网中的推广提供了可行方案，同时保持了与传统TLS的兼容性。

Abstract: A modification of the TLS protocol is presented, using our implementation of
the Quantum Key Distribution (QKD) standard ETSI GS QKD 014 v1.1.1. We rely on
the Rustls library for this. The TLS protocol is modified while maintaining
backward compatibility on the client and server side. We thus wish to
participate in the effort to generalize the use of QKD on the Internet. We used
our protocol for a video conference call encrypted by QKD. Finally, we analyze
the performance of our protocol, comparing the time needed to establish a
handshake to that of TLS 1.3.

</details>


### [58] [FuncVul: An Effective Function Level Vulnerability Detection Model using LLM and Code Chunk](https://arxiv.org/abs/2506.19453)
*Sajal Halder,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: 本文提出FuncVul，一种基于代码块的函数级漏洞检测模型，专注于识别C/C++和Python中关键代码段的多种漏洞，显著提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注包或库级别的漏洞，忽视具体漏洞函数定位。识别漏洞函数可大幅降低开源软件使用风险，但开发者提交的非漏洞修复代码增加了识别难度。

Method: 采用两种方法构建六种数据集：(1)结合补丁信息与大型语言模型标记漏洞样本；(2)单独利用大型语言模型检测函数级代码漏洞。基于GraphCodeBERT微调模型捕获代码语法和语义特征。

Result: FuncVul在全部数据集上平均准确率达87-92%，F1分数86-92%，较基于完整函数的预测模型准确率提升53.9%，F1分数提升42.0%。

Conclusion: 基于代码块的FuncVul模型在函数级漏洞检测中表现优异，代码与数据集已开源。该方法为精准定位软件供应链漏洞提供了有效解决方案。

Abstract: Software supply chain vulnerabilities arise when attackers exploit weaknesses
by injecting vulnerable code into widely used packages or libraries within
software repositories. While most existing approaches focus on identifying
vulnerable packages or libraries, they often overlook the specific functions
responsible for these vulnerabilities. Pinpointing vulnerable functions within
packages or libraries is critical, as it can significantly reduce the risks
associated with using open-source software. Identifying vulnerable patches is
challenging because developers often submit code changes that are unrelated to
vulnerability fixes. To address this issue, this paper introduces FuncVul, an
innovative code chunk-based model for function-level vulnerability detection in
C/C++ and Python, designed to identify multiple vulnerabilities within a
function by focusing on smaller, critical code segments. To assess the model's
effectiveness, we construct six code and generic code chunk based datasets
using two approaches: (1) integrating patch information with large language
models to label vulnerable samples and (2) leveraging large language models
alone to detect vulnerabilities in function-level code. To design FuncVul
vulnerability model, we utilise GraphCodeBERT fine tune model that captures
both the syntactic and semantic aspects of code. Experimental results show that
FuncVul outperforms existing state-of-the-art models, achieving an average
accuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore,
we have demonstrated that our code-chunk-based FuncVul model improves 53.9%
accuracy and 42.0% F1-score than the full function-based vulnerability
prediction. The FuncVul code and datasets are publicly available on GitHub at
https://github.com/sajalhalder/FuncVul.

</details>


### [59] [PhishingHook: Catching Phishing Ethereum Smart Contracts leveraging EVM Opcodes](https://arxiv.org/abs/2506.19480)
*Pasquale De Rosa,Simon Queyrut,Yérom-David Bromberg,Pascal Felber,Valerio Schiavoni*

Main category: cs.CR

TL;DR: 本文提出PhishingHook框架，通过机器学习直接分析智能合约字节码与操作码，实现钓鱼活动检测，在7000个真实恶意合约上验证了90%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 以太坊虚拟机(EVM)钓鱼攻击日益猖獗，现有基于交易回溯的检测方法存在用户数据暴露风险，亟需新型检测方案。

Method: 采用16种机器学习技术（分属直方图相似度分类器、视觉模型、语言模型和漏洞检测模型四类），直接分析未部署合约的字节码模式与异常行为特征。

Result: 在7000个真实恶意合约测试中，各模型平均准确率达90%，显著优于传统交易追踪方法。

Conclusion: PhishingHook通过静态代码分析有效识别钓鱼合约，研究成果已开源以促进社区发展，为EVM生态安全提供新范式。

Abstract: The Ethereum Virtual Machine (EVM) is a decentralized computing engine. It
enables the Ethereum blockchain to execute smart contracts and decentralized
applications (dApps). The increasing adoption of Ethereum sparked the rise of
phishing activities. Phishing attacks often target users through deceptive
means, e.g., fake websites, wallet scams, or malicious smart contracts, aiming
to steal sensitive information or funds. A timely detection of phishing
activities in the EVM is therefore crucial to preserve the user trust and
network integrity. Some state-of-the art approaches to phishing detection in
smart contracts rely on the online analysis of transactions and their traces.
However, replaying transactions often exposes sensitive user data and
interactions, with several security concerns. In this work, we present
PhishingHook, a framework that applies machine learning techniques to detect
phishing activities in smart contracts by directly analyzing the contract's
bytecode and its constituent opcodes. We evaluate the efficacy of such
techniques in identifying malicious patterns, suspicious function calls, or
anomalous behaviors within the contract's code itself before it is deployed or
interacted with. We experimentally compare 16 techniques, belonging to four
main categories (Histogram Similarity Classifiers, Vision Models, Language
Models and Vulnerability Detection Models), using 7,000 real-world malware
smart contracts. Our results demonstrate the efficiency of PhishingHook in
performing phishing classification systems, with about 90% average accuracy
among all the models. We support experimental reproducibility, and we release
our code and datasets to the research community.

</details>


### [60] [PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty](https://arxiv.org/abs/2506.19563)
*Jinwen He,Yiyang Lu,Zijin Lin,Kai Chen,Yue Zhao*

Main category: cs.CR

TL;DR: 本文提出PrivacyXray框架，通过分析大语言模型(LLM)内部状态检测隐私泄露，无需依赖外部验证数据，在五大LLM上平均准确率达92.69%，较现有方法提升20.06%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗、金融等敏感领域应用时存在隐私泄露风险，现有攻击方法无法验证泄露信息的准确性，且缺乏公开数据集进行交叉验证。

Method: PrivacyXray通过合成真实隐私数据，利用四类指标检测隐私泄露：层内/层间语义相似度、词级/句级概率分布，分析LLM生成正确隐私信息时表现出的高语义连贯性和概率确定性。

Result: 实验表明PrivacyXray在五大LLM上平均准确率达92.69%，较现有最优方法平均提升20.06%，展现出稳定性和实际应用价值。

Conclusion: PrivacyXray通过LLM内部状态分析解决了隐私检测中开源数据集缺失和外部验证依赖问题，为实际应用提供了有效的隐私泄露检测方案。

Abstract: Large Language Models (LLMs) are widely used in sensitive domains, including
healthcare, finance, and legal services, raising concerns about potential
private information leaks during inference. Privacy extraction attacks, such as
jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the
models to output sensitive information. However, these attacks cannot verify
whether the extracted private information is accurate, as no public datasets
exist for cross-validation, leaving a critical gap in private information
detection during inference. To address this, we propose PrivacyXray, a novel
framework detecting privacy breaches by analyzing LLM inner states. Our
analysis reveals that LLMs exhibit higher semantic coherence and probabilistic
certainty when generating correct private outputs. Based on this, PrivacyXray
detects privacy breaches using four metrics: intra-layer and inter-layer
semantic similarity, token-level and sentence-level probability distributions.
PrivacyXray addresses critical challenges in private information detection by
overcoming the lack of open-source private datasets and eliminating reliance on
external data for validation. It achieves this through the synthesis of
realistic private data and a detection mechanism based on the inner states of
LLMs. Experiments show that PrivacyXray achieves consistent performance, with
an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art
methods, PrivacyXray achieves significant improvements, with an average
accuracy increase of 20.06%, highlighting its stability and practical utility
in real-world applications.

</details>


### [61] [Decompiling Smart Contracts with a Large Language Model](https://arxiv.org/abs/2506.19624)
*Isaac David,Liyi Zhou,Dawn Song,Arthur Gervais,Kaihua Qin*

Main category: cs.CR

TL;DR: 本文提出了一种创新的反编译管道，首次成功利用大型语言模型（LLM）将以太坊虚拟机（EVM）字节码转换为可读且语义准确的Solidity代码，显著提升了区块链安全分析的效率。


<details>
  <summary>Details</summary>
Motivation: 当前区块链浏览器（如Etherscan）上绝大多数智能合约未开源（<1%），导致安全审计困难。传统反编译器生成的代码可读性差，严重阻碍漏洞分析和功能理解。

Method: 采用静态程序分析将字节码转换为结构化三地址码（TAC），再通过微调的Llama-3.2-3B模型（训练于238,446个TAC-Solidity函数对数据集）生成高质量Solidity代码，恢复变量名、控制流和函数签名。

Result: 实验表明该方法显著优于传统反编译器，平均语义相似度达0.82（与原代码），并实现卓越可读性。已部署于公开系统evmdecompiler.com。

Conclusion: 该研究为区块链安全提供了突破性解决方案，通过LLM驱动的精准反编译技术，有效解决了字节码可读性与语义保真度的核心挑战。

Abstract: The widespread lack of broad source code verification on blockchain explorers
such as Etherscan, where despite 78,047,845 smart contracts deployed on
Ethereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents
a severe impediment to blockchain security. This opacity necessitates the
automated semantic analysis of on-chain smart contract bytecode, a fundamental
research challenge with direct implications for identifying vulnerabilities and
understanding malicious behavior. Prevailing decompilers struggle to reverse
bytecode in a readable manner, often yielding convoluted code that critically
hampers vulnerability analysis and thwarts efforts to dissect contract
functionalities for security auditing.
  This paper addresses this challenge by introducing a pioneering decompilation
pipeline that, for the first time, successfully leverages Large Language Models
(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable
and semantically faithful Solidity code. Our novel methodology first employs
rigorous static program analysis to convert bytecode into a structured
three-address code (TAC) representation. This intermediate representation then
guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset
of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.
This approach uniquely recovers meaningful variable names, intricate control
flow, and precise function signatures. Our extensive empirical evaluation
demonstrates a significant leap beyond traditional decompilers, achieving an
average semantic similarity of 0.82 with original source and markedly superior
readability. The practical viability and effectiveness of our research are
demonstrated through its implementation in a publicly accessible system,
available at https://evmdecompiler.com.

</details>


### [62] [A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures](https://arxiv.org/abs/2506.19676)
*Dezhang Kong,Shi Lin,Zhenhua Xu,Zhebo Wang,Minghao Li,Yufeng Li,Yilun Zhang,Zeyang Sha,Yuyuan Li,Changting Lin,Xun Wang,Xuan Liu,Muhammad Khurram Khan,Ningyu Zhang,Chaochao Chen,Meng Han*

Main category: cs.CR

TL;DR: 本文综述了AI代理通信安全的研究现状，将代理通信生命周期分为三个阶段，分析了各阶段的安全风险及防御措施，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的AI代理展现出前所未有的智能和灵活性，代理间通信成为未来AI生态的基础支柱，但这一新兴领域存在重大安全隐患，亟需系统研究。

Method: 作者首先明确定义了代理通信，将其生命周期分为用户-代理交互、代理-代理通信和代理-环境通信三个阶段，随后分析各阶段协议特性及安全风险，并总结可能的防御对策。

Result: 研究揭示了不同通信阶段（如Anthropic的MCP和Google的A2A协议）存在的安全隐患，并针对性地提出了防御措施的分类框架。

Conclusion: 本文系统梳理了代理通信安全的研究脉络，指出了当前开放性问题，为未来构建安全的AI代理通信生态系统提供了重要参考方向。

Abstract: In recent years, Large-Language-Model-driven AI agents have exhibited
unprecedented intelligence, flexibility, and adaptability, and are rapidly
changing human production and lifestyle. Nowadays, agents are undergoing a new
round of evolution. They no longer act as an isolated island like LLMs.
Instead, they start to communicate with diverse external entities, such as
other agents and tools, to collectively perform more complex tasks. Under this
trend, agent communication is regarded as a foundational pillar of the future
AI ecosystem, and many organizations intensively begin to design related
communication protocols (e.g., Anthropic's MCP and Google's A2A) within the
recent few months. However, this new field exposes significant security hazard,
which can cause severe damage to real-world scenarios. To help researchers to
quickly figure out this promising topic and benefit the future agent
communication development, this paper presents a comprehensive survey of agent
communication security. More precisely, we first present a clear definition of
agent communication and categorize the entire lifecyle of agent communication
into three stages: user-agent interaction, agent-agent communication, and
agent-environment communication. Next, for each communication phase, we dissect
related protocols and analyze its security risks according to the communication
characteristics. Then, we summarize and outlook on the possible defense
countermeasures for each risk. Finally, we discuss open issues and future
directions in this promising research field.

</details>


### [63] [KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs](https://arxiv.org/abs/2506.19802)
*Xin Fan Guo,Albert Merono Penuela,Sergio Maffeis,Fabio Pierazzi*

Main category: cs.CR

TL;DR: 提出KnowML框架，通过知识图谱和LLM增强机器学习入侵检测系统，显著提升对多样化攻击变体的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有ML-NIDS依赖同质数据集导致性能虚高，且过度依赖人类专家经验，难以泛化检测多样化攻击变体。

Method: 利用LLM自动分析攻击实现，构建攻击策略知识图谱，通过符号推理生成知识增强输入嵌入ML-NIDS设计。

Result: 在28种攻击变体测试中，基线模型F1最低为0%，而KnowML达到99% F1且误报率低于0.1%。

Conclusion: 知识引导的机器学习框架能有效解决攻击变体检测难题，为ML-NIDS设计提供自动化领域知识注入路径。

Abstract: Despite extensive research on Machine Learning-based Network Intrusion
Detection Systems (ML-NIDS), their capability to detect diverse attack variants
remains uncertain. Prior studies have largely relied on homogeneous datasets,
which artificially inflate performance scores and offer a false sense of
security. Designing systems that can effectively detect a wide range of attack
variants remains a significant challenge. The progress of ML-NIDS continues to
depend heavily on human expertise, which can embed subjective judgments of
system designers into the model, potentially hindering its ability to
generalize across diverse attack types.
  To address this gap, we propose KnowML, a framework for knowledge-guided
machine learning that integrates attack knowledge into ML-NIDS. KnowML
systematically explores the threat landscape by leveraging Large Language
Models (LLMs) to perform automated analysis of attack implementations. It
constructs a unified Knowledge Graph (KG) of attack strategies, on which it
applies symbolic reasoning to generate KG-Augmented Input, embedding domain
knowledge directly into the design process of ML-NIDS.
  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly
collected for this study. Our findings reveal that baseline ML-NIDS models fail
to detect several variants entirely, achieving F1 scores as low as 0 %. In
contrast, our knowledge-guided approach achieves up to 99 % F1 score while
maintaining a False Positive Rate below 0.1 %.

</details>


### [64] [Machine Learning with Privacy for Protected Attributes](https://arxiv.org/abs/2506.19836)
*Saeed Mahloujifar,Chuan Guo,G. Edward Suh,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: 本文提出特征差分隐私(FDP)框架，针对特定保护属性优化隐私分析，显著提升模型效用。通过改进DP-SGD算法并在AFHQ数据集验证，FID从286.7降至101.9（$\epsilon=8$）。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私(DP)对所有特征实施统一保护，导致非敏感特征的效用损失。当仅需保护特定属性时，需要更灵活的隐私框架。

Method: 1) 提出基于模拟的特征差分隐私(FDP)定义\n2) 支持增删/替换两种隐私变体\n3) 改进DP-SGD算法实现FDP\n4) 利用子采样进行隐私放大

Result: 在AFHQ动物面部数据集上：\n- 标准DP的FID=286.7\n- FDP框架下FID降至101.9（$\epsilon=8$）\n- 有效抵抗属性推断攻击

Conclusion: FDP框架在保证强隐私的前提下，通过区分保护/非保护特征显著提升模型效用，为隐私数据分析提供新范式。

Abstract: Differential privacy (DP) has become the standard for private data analysis.
Certain machine learning applications only require privacy protection for
specific protected attributes. Using naive variants of differential privacy in
such use cases can result in unnecessary degradation of utility. In this work,
we refine the definition of DP to create a more general and flexible framework
that we call feature differential privacy (FDP). Our definition is
simulation-based and allows for both addition/removal and replacement variants
of privacy, and can handle arbitrary and adaptive separation of protected and
non-protected features. We prove the properties of FDP, such as adaptive
composition, and demonstrate its implications for limiting attribute inference
attacks. We also propose a modification of the standard DP-SGD algorithm that
satisfies FDP while leveraging desirable properties such as amplification via
sub-sampling. We apply our framework to various machine learning tasks and show
that it can significantly improve the utility of DP-trained models when public
features are available. For example, we train diffusion models on the AFHQ
dataset of animal faces and observe a drastic improvement in FID compared to
DP, from 286.7 to 101.9 at $\epsilon=8$, assuming that the blurred version of a
training image is available as a public feature. Overall, our work provides a
new approach to private data analysis that can help reduce the utility cost of
DP while still providing strong privacy guarantees.

</details>


### [65] [On the efficacy of old features for the detection of new bots](https://arxiv.org/abs/2506.19635)
*Rocco De Nicola,Marinella Petrocchi,Manuel Pratelli*

Main category: cs.CR

TL;DR: 本研究比较了四种先进特征集在Twitter新型机器人检测中的性能，发现通用分类器和低成本计算特征可有效识别进化机器人。


<details>
  <summary>Details</summary>
Motivation: 恶意机器人通过传播垃圾邮件和操纵舆论危害网络生态，现有检测方法多依赖复杂特征，本研究探索低成本高效检测方案。

Method: 以Twitter为基准平台，对比分析Botometer评分、账户资料/时间线特征集及推文客户端信息四类特征在六个最新数据集上的表现。

Result: 实验结果表明，通用分类器结合易提取的账户特征能够有效识别新型进化机器人。

Conclusion: 该研究为对抗持续进化的社交机器人提供了经济高效的检测思路，具有实际应用价值。

Abstract: For more than a decade now, academicians and online platform administrators
have been studying solutions to the problem of bot detection. Bots are computer
algorithms whose use is far from being benign: malicious bots are purposely
created to distribute spam, sponsor public characters and, ultimately, induce a
bias within the public opinion. To fight the bot invasion on our online
ecosystem, several approaches have been implemented, mostly based on
(supervised and unsupervised) classifiers, which adopt the most varied account
features, from the simplest to the most expensive ones to be extracted from the
raw data obtainable through the Twitter public APIs. In this exploratory study,
using Twitter as a benchmark, we compare the performances of four state-of-art
feature sets in detecting novel bots: one of the output scores of the popular
bot detector Botometer, which considers more than 1,000 features of an account
to take a decision; two feature sets based on the account profile and timeline;
and the information about the Twitter client from which the user tweets. The
results of our analysis, conducted on six recently released datasets of Twitter
accounts, hint at the possible use of general-purpose classifiers and
cheap-to-compute account features for the detection of evolved bots.

</details>


### [66] [Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases](https://arxiv.org/abs/2506.17336)
*Yubeen Bae,Minchan Kim,Jaejin Lee,Sangbum Kim,Jaehyung Kim,Yejin Choi,Niloofar Mireshghallah*

Main category: cs.CR

TL;DR: 提出了一种结合强大但不信任的LLM与本地弱模型的方法，通过苏格拉底式思维链推理和同态加密向量数据库保护用户隐私，在LoCoMo基准上性能提升7.1%。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM处理敏感数据时面临隐私风险，需在强大但不信任的云端模型与隐私安全但性能较弱的本地模型之间做出选择。

Method: 首先由不信任的LLM生成通用思维链提示和子查询，然后通过同态加密向量数据库对用户百万级私有数据进行加密语义搜索，最后由本地模型结合提示和解密数据生成最终响应。

Result: 在LoCoMo长上下文QA基准测试中，GPT-4o与本地Llama-3.2-1B混合框架比单独使用GPT-4o性能提升最高达7.1个百分点。

Conclusion: 该系统首次实现了任务在不信任强LLM与本地弱模型间的分解与协作，在提升性能的同时有效保护了用户隐私。

Abstract: Large language models (LLMs) are increasingly used as personal agents,
accessing sensitive user data such as calendars, emails, and medical records.
Users currently face a trade-off: They can send private records, many of which
are stored in remote databases, to powerful but untrusted LLM providers,
increasing their exposure risk. Alternatively, they can run less powerful
models locally on trusted devices. We bridge this gap. Our Socratic
Chain-of-Thought Reasoning first sends a generic, non-private user query to a
powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and
detailed sub-queries without accessing user data. Next, we embed these
sub-queries and perform encrypted sub-second semantic search using our
Homomorphically Encrypted Vector Database across one million entries of a
single user's private data. This represents a realistic scale of personal
documents, emails, and records accumulated over years of digital activity.
Finally, we feed the CoT prompt and the decrypted records to a local language
model and generate the final response. On the LoCoMo long-context QA benchmark,
our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,
outperforms using GPT-4o alone by up to 7.1 percentage points. This
demonstrates a first step toward systems where tasks are decomposed and split
between untrusted strong LLMs and weak local ones, preserving user privacy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [Signal Use and Emergent Cooperation](https://arxiv.org/abs/2506.18920)
*Michael Williams*

Main category: cs.AI

TL;DR: 本研究探讨了自治代理如何通过通信信号协调活动并提升集体效率，使用NEC-DAC系统展示了代理如何通过学习与信号发展出类似文化的共享行为系统。


<details>
  <summary>Details</summary>
Motivation: 研究自治代理如何通过通信信号自我组织文化，并探索不同通信策略对代理群体适应性与合作的影响。

Method: 采用NEC-DAC系统，每个代理配备独立神经网络进行决策，分析不同社会结构（如权威层级）下文化的自组织过程。

Result: 合作文化显著影响代理群体的表现，信号不仅促进文化涌现，还支持文化在代理世代间传递。

Conclusion: 协调行为与信号在代理神经网络中的协同作用，为自治代理群体的文化演化与高效合作提供了新见解。

Abstract: In this work, we investigate how autonomous agents, organized into tribes,
learn to use communication signals to coordinate their activities and enhance
their collective efficiency. Using the NEC-DAC (Neurally Encoded Culture -
Distributed Autonomous Communicators) system, where each agent is equipped with
its own neural network for decision-making, we demonstrate how these agents
develop a shared behavioral system -- akin to a culture -- through learning and
signalling. Our research focuses on the self-organization of culture within
these tribes of agents and how varying communication strategies impact their
fitness and cooperation. By analyzing different social structures, such as
authority hierarchies, we show that the culture of cooperation significantly
influences the tribe's performance. Furthermore, we explore how signals not
only facilitate the emergence of culture but also enable its transmission
across generations of agents. Additionally, we examine the benefits of
coordinating behavior and signaling within individual agents' neural networks.

</details>


### [68] [Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience](https://arxiv.org/abs/2506.18928)
*Lingyu Yang*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLMs）在策略随机化方面的表现，通过设计一个受田忌赛马启发的零和游戏，评估了不同LLMs在多种提示风格下的随机化决策能力，揭示了模型间战略推理能力的差异。


<details>
  <summary>Details</summary>
Motivation: 策略随机化是博弈论的核心原则，但在大语言模型中的研究不足。先前工作常将随机化的认知决策与随机性生成混为一谈，导致评估不完整。本文旨在填补这一空白。

Method: 研究设计了一个零和游戏，其纳什均衡对应最大熵策略。通过三种提示风格（框架化、中性、暗示性）评估五个LLMs，使用系统提供的随机选择来隔离随机化决策，并进行竞争性多轮比赛。

Result: 结果显示，较弱模型无论提示如何均保持确定性，而较强模型在明确提示下增加随机化。面对弱模型时，强LLMs采用确定性策略利用偏差，但在与同类对抗时趋向均衡策略。胜负结果和贝叶斯因子分析揭示了LLMs战略推理能力的显著差异。

Conclusion: 研究揭示了LLMs在抽象推理和自适应学习方面的改进空间，并通过公开实现确保可重复性。结果强调了不同模型在战略随机化能力上的分化，为未来研究提供了方向。

Abstract: Strategic randomization is a key principle in game theory, yet it remains
underexplored in large language models (LLMs). Prior work often conflates the
cognitive decision to randomize with the mechanical generation of randomness,
leading to incomplete evaluations. To address this, we propose a novel zero-sum
game inspired by the Tian Ji Horse Race, where the Nash equilibrium corresponds
to a maximal entropy strategy. The game's complexity masks this property from
untrained humans and underdeveloped LLMs. We evaluate five LLMs across prompt
styles -- framed, neutral, and hinted -- using competitive multi-tournament
gameplay with system-provided random choices, isolating the decision to
randomize. Results show that weaker models remain deterministic regardless of
prompts, while stronger models exhibit increased randomization under explicit
hints. When facing weaker models, strong LLMs adopt deterministic strategies to
exploit biases, but converge toward equilibrium play when facing peers. Through
win/loss outcomes and Bayes factor analysis, we demonstrate meaningful
variation in LLMs' strategic reasoning capabilities, highlighting opportunities
for improvement in abstract reasoning and adaptive learning. We make our
implementation publicly available at
https://github.com/ocelopus/llm-when-to-throw-coin to ensure full
reproducibility.

</details>


### [69] [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
*Sheraz Khan,Subha Madhavan,Kannan Natarajan*

Main category: cs.AI

TL;DR: 本文反驳了Shojaee等人(2025)关于大模型推理能力存在固有局限的结论，指出其观察到的'推理悬崖'现象实为实验范式限制所致。通过工具赋能，模型能突破原有复杂度阈值，展现层级化的智能推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对Shojaee等提出的'推理悬崖'理论，作者认为该结论受限于静态文本评估范式的系统约束，旨在证明模型失败源于执行接口限制而非根本性认知缺陷。

Method: 采用工具赋能策略进行对比实验：先复现文本受限条件下的模型失败案例，再展示同模型在工具支持下的卓越表现，并分析不同智能体（o4-mini/GPT-4o）的元认知层级。

Result: 工具赋能模型成功解决原研究中'不可能'的难题，其复杂度远超原阈值；揭示智能体推理存在从程序执行到自我修正的元认知层级，重定义了机器智能的评估维度。

Conclusion: '思维幻觉'源于行动工具缺失而非推理缺陷。当前评估范式严重低估模型潜力，应建立包含工具使用的动态评估体系，机器智能的边界取决于'行动赋能'而非静态认知测试。

Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models via the Lens of
Problem Complexity, presents a compelling empirical finding, a reasoning cliff,
where the performance of Large Reasoning Models (LRMs) collapses beyond a
specific complexity threshold, which the authors posit as an intrinsic scaling
limitation of Chain-of-Thought (CoT) reasoning. This commentary, while
acknowledging the study's methodological rigor, contends that this conclusion
is confounded by experimental artifacts. We argue that the observed failure is
not evidence of a fundamental cognitive boundary, but rather a predictable
outcome of system-level constraints in the static, text-only evaluation
paradigm, including tool use restrictions, context window recall issues, the
absence of crucial cognitive baselines, inadequate statistical reporting, and
output generation limits. We reframe this performance collapse through the lens
of an agentic gap, asserting that the models are not failing at reasoning, but
at execution within a profoundly restrictive interface. We empirically
substantiate this critique by demonstrating a striking reversal. A model,
initially declaring a puzzle impossible when confined to text-only generation,
now employs agentic tools to not only solve it but also master variations of
complexity far beyond the reasoning cliff it previously failed to surmount.
Additionally, our empirical analysis of tool-enabled models like o4-mini and
GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural
execution to complex meta-cognitive self-correction, which has significant
implications for how we define and measure machine intelligence. The illusion
of thinking attributed to LRMs is less a reasoning deficit and more a
consequence of an otherwise capable mind lacking the tools for action.

</details>


### [70] [Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition](https://arxiv.org/abs/2506.19191)
*Craig Steven Wright*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯推理、测度论和群体动力学的概率智能体框架，通过结构化竞争和信念修正实现进化，证明真理可作为进化吸引子从对抗性认知压力中涌现。


<details>
  <summary>Details</summary>
Motivation: 旨在构建数学严谨的AI系统，通过竞争性进化机制使智能体信念向外部真理标准对齐，解决可验证知识在动态环境中的涌现问题。

Method: 采用贝叶斯推理框架，定义基于真理对齐度的适应度函数；通过离散时间环境中的配对效用比较实现种群更新，引入哈希加密身份承诺和do-演算因果推理算子。

Result: 形式化定理证明系统具有收敛性、鲁棒性和进化稳定性，验证了真理作为进化吸引子的特性，实现可计算的自调节群体智能。

Conclusion: 该框架为对抗性认知压力下产生可验证知识提供了理论基础，表明结构化竞争能使群体信念向真理标准自发演化。

Abstract: We introduce a mathematically rigorous framework for an artificial
intelligence system composed of probabilistic agents evolving through
structured competition and belief revision. The architecture, grounded in
Bayesian inference, measure theory, and population dynamics, defines agent
fitness as a function of alignment with a fixed external oracle representing
ground truth. Agents compete in a discrete-time environment, adjusting
posterior beliefs through observed outcomes, with higher-rated agents
reproducing and lower-rated agents undergoing extinction. Ratings are updated
via pairwise truth-aligned utility comparisons, and belief updates preserve
measurable consistency and stochastic convergence. We introduce hash-based
cryptographic identity commitments to ensure traceability, alongside causal
inference operators using do-calculus. Formal theorems on convergence,
robustness, and evolutionary stability are provided. The system establishes
truth as an evolutionary attractor, demonstrating that verifiable knowledge
arises from adversarial epistemic pressure within a computable, self-regulating
swarm.

</details>


### [71] [From Rows to Yields: How Foundation Models for Tabular Data Simplify Crop Yield Prediction](https://arxiv.org/abs/2506.19046)
*Filip Sabo,Michele Meroni,Maria Piles,Martin Claverie,Fanie Ferreira,Elna Van Den Berg,Francesco Collivignarelli,Felix Rembold*

Main category: cs.AI

TL;DR: 研究将基础模型TabPFN应用于南非次国家级作物产量预测，与传统机器学习模型相比，TabPFN在保持精度的同时显著提升了调参效率和工程便捷性。


<details>
  <summary>Details</summary>
Motivation: TabPFN在小中型表格数据任务中表现优异，本研究旨在验证其在农业产量预测领域的实用价值，特别是针对数据效率与实施便捷性的需求。

Method: 使用23年8个省份的夏季作物产量数据，结合月度聚合的FAPAR、土壤湿度等EO数据及气温、降水等网格气象数据，采用留一年份交叉验证对比TabPFN与6种机器学习模型及3种基线模型。

Result: TabPFN与机器学习模型精度相当且均优于基线，但其调参速度更快、特征工程需求更低，展现出更强的实际应用优势。

Conclusion: TabPFN因其高效性和易用性，成为现实世界产量预测的更优选择，尤其适用于对操作效率要求严格的场景。

Abstract: We present an application of a foundation model for small- to medium-sized
tabular data (TabPFN), to sub-national yield forecasting task in South Africa.
TabPFN has recently demonstrated superior performance compared to traditional
machine learning (ML) models in various regression and classification tasks. We
used the dekadal (10-days) time series of Earth Observation (EO; FAPAR and soil
moisture) and gridded weather data (air temperature, precipitation and
radiation) to forecast the yield of summer crops at the sub-national level. The
crop yield data was available for 23 years and for up to 8 provinces. Covariate
variables for TabPFN (i.e., EO and weather) were extracted by region and
aggregated at a monthly scale. We benchmarked the results of the TabPFN against
six ML models and three baseline models. Leave-one-year-out cross-validation
experiment setting was used in order to ensure the assessment of the models
capacity to forecast an unseen year. Results showed that TabPFN and ML models
exhibit comparable accuracy, outperforming the baselines. Nonetheless, TabPFN
demonstrated superior practical utility due to its significantly faster tuning
time and reduced requirement for feature engineering. This renders TabPFN a
more viable option for real-world operation yield forecasting applications,
where efficiency and ease of implementation are paramount.

</details>


### [72] [Baba is LLM: Reasoning in a Game with Dynamic Rules](https://arxiv.org/abs/2506.19095)
*Fien van Wetten,Aske Plaat,Max van Duijn*

Main category: cs.AI

TL;DR: 研究评估了六种大语言模型（LLM）在2D解谜游戏《Baba is You》中的表现，发现即使微调后，模型对动态规则变化的推理仍存在困难，突显了此类游戏对测试LLM推理能力的适用性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在语言任务上表现优异，但在需要复杂推理的任务（如动态规则解谜游戏）中表现欠佳。本研究通过《Baba is You》游戏探索LLM结合语言能力与规则推理的潜力。

Method: 评估六种LLM（包括GPT-4o、Mistral和OLMo），使用三种提示类型（基础提示、规则扩展提示和动作扩展提示），并对其中两个模型进行游戏文本与结构数据的微调。

Result: 大型模型（如GPT-4o）在推理和解谜中表现更优，但未微调的小模型难以识别游戏机制或应用规则变化。微调提升了游戏关卡分析能力，但对解决方案生成的改进有限。

Conclusion: 即使最先进的微调LLM也难以理解动态规则变化（特别是使用-提及的区分），表明此类游戏适合作为测试LLM复杂推理与反思能力的工具。

Abstract: Large language models (LLMs) are known to perform well on language tasks, but
struggle with reasoning tasks. This paper explores the ability of LLMs to play
the 2D puzzle game Baba is You, in which players manipulate rules by
rearranging text blocks that define object properties. Given that this
rule-manipulation relies on language abilities and reasoning, it is a
compelling challenge for LLMs. Six LLMs are evaluated using different prompt
types, including (1) simple, (2) rule-extended and (3) action-extended prompts.
In addition, two models (Mistral, OLMo) are finetuned using textual and
structural data from the game. Results show that while larger models
(particularly GPT-4o) perform better in reasoning and puzzle solving, smaller
unadapted models struggle to recognize game mechanics or apply rule changes.
Finetuning improves the ability to analyze the game levels, but does not
significantly improve solution formulation. We conclude that even for
state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is
difficult (specifically, understanding the use-mention distinction). The
results provide insights into the applicability of LLMs to complex
problem-solving tasks and highlight the suitability of games with dynamically
changing rules for testing reasoning and reflection by LLMs.

</details>


### [73] [Spiritual-LLM : Gita Inspired Mental Health Therapy In the Era of LLMs](https://arxiv.org/abs/2506.19185)
*Janak Kapuriya,Aman Singh,Jainendra Shukla,Rajiv Ratn Shah*

Main category: cs.AI

TL;DR: 本研究提出了一种结合《薄伽梵歌》精神智慧与GPT-4o大语言模型的新型框架GITes，用于提升情感支持效果。通过构建包含10,729条精神引导回复的数据集，并在12个先进LLM上测试，结果显示Phi3-Mini模型在各项指标上显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康支持系统仅基于用户当前情绪和情境生成回复，导致干预流于表面。本研究旨在通过整合精神智慧与AI技术，解决深层情感需求。

Method: 研究构建了GITes数据集（基于ExTES增强），包含GPT-4o生成的精神引导回复；提出"精神洞察力"新指标，并采用LLM陪审团框架进行自动化评估；在12个LLM上进行了基准测试。

Result: 最佳模型Phi3-Mini在ROUGE（提升122.71%）、METEOR（126.53%）、BERT分数（8.15%）、精神洞察力（15.92%）、充分性（18.61%）和相关性（13.22%）等指标上均显著优于零样本版本。

Conclusion: 精神智慧增强的AI系统在自动共情和灵性指标上表现优异，具有提升用户满意度的潜力。但需在真实患者群体中进一步验证。代码和数据集将公开以促进该新兴领域研究。

Abstract: Traditional mental health support systems often generate responses based
solely on the user's current emotion and situations, resulting in superficial
interventions that fail to address deeper emotional needs. This study
introduces a novel framework by integrating spiritual wisdom from the Bhagavad
Gita with advanced large language model GPT-4o to enhance emotional well-being.
We present the GITes (Gita Integrated Therapy for Emotional Support) dataset,
which enhances the existing ExTES mental health dataset by including 10,729
spiritually guided responses generated by GPT-4o and evaluated by domain
experts. We benchmark GITes against 12 state-of-the-art LLMs, including both
mental health specific and general purpose models. To evaluate spiritual
relevance in generated responses beyond what conventional n-gram based metrics
capture, we propose a novel Spiritual Insight metric and automate assessment
via an LLM as jury framework using chain-of-thought prompting. Integrating
spiritual guidance into AI driven support enhances both NLP and spiritual
metrics for the best performing LLM Phi3-Mini 3.2B Instruct, achieving
improvements of 122.71% in ROUGE, 126.53% in METEOR, 8.15% in BERT score,
15.92% in Spiritual Insight, 18.61% in Sufficiency and 13.22% in Relevance
compared to its zero-shot counterpart. While these results reflect substantial
improvements across automated empathy and spirituality metrics, further
validation in real world patient populations remains a necessary step. Our
findings indicate a strong potential for AI systems enriched with spiritual
guidance to enhance user satisfaction and perceived support outcomes. The code
and dataset will be publicly available to advance further research in this
emerging area.

</details>


### [74] [GBGC: Efficient and Adaptive Graph Coarsening via Granular-ball Computing](https://arxiv.org/abs/2506.19224)
*Shuyin Xia,Guan Wang,Gaojie Xu,Sen Zhao,Guoyin Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于多粒度球（GBGC）的高效自适应图粗化方法，通过结合图结构的多粒度特性，显著提升了粗化效果与效率。


<details>
  <summary>Details</summary>
Motivation: 传统图粗化方法主要基于谱保持视角，忽视了原图由不同粒度子区域组成的事实。本文旨在通过多粒度球计算，优化粗化粒度并提升处理效率。

Method: GBGC引入自适应多粒度球图细化机制，将原图从粗到细自适应分割为不同大小的最优粒度球，并以这些球作为超节点构建粗化图。

Result: 相比现有方法，GBGC处理速度提升数十至数百倍，时间复杂度更低；且因多粒度球的鲁棒性，其精度通常高于原图。

Conclusion: GBGC凭借高效性、自适应性和高精度，有望成为图数据预处理的标准方法。

Abstract: The objective of graph coarsening is to generate smaller, more manageable
graphs while preserving key information of the original graph. Previous work
were mainly based on the perspective of spectrum-preserving, using some
predefined coarsening rules to make the eigenvalues of the Laplacian matrix of
the original graph and the coarsened graph match as much as possible. However,
they largely overlooked the fact that the original graph is composed of
subregions at different levels of granularity, where highly connected and
similar nodes should be more inclined to be aggregated together as nodes in the
coarsened graph. By combining the multi-granularity characteristics of the
graph structure, we can generate coarsened graph at the optimal granularity. To
this end, inspired by the application of granular-ball computing in
multi-granularity, we propose a new multi-granularity, efficient, and adaptive
coarsening method via granular-ball (GBGC), which significantly improves the
coarsening results and efficiency. Specifically, GBGC introduces an adaptive
granular-ball graph refinement mechanism, which adaptively splits the original
graph from coarse to fine into granular-balls of different sizes and optimal
granularity, and constructs the coarsened graph using these granular-balls as
supernodes. In addition, compared with other state-of-the-art graph coarsening
methods, the processing speed of this method can be increased by tens to
hundreds of times and has lower time complexity. The accuracy of GBGC is almost
always higher than that of the original graph due to the good robustness and
generalization of the granular-ball computing, so it has the potential to
become a standard graph data preprocessing method.

</details>


### [75] [RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1](https://arxiv.org/abs/2506.19235)
*Yu Xie,Xingkai Ren,Ying Qi,Yao Hu,Lianlei Shan*

Main category: cs.AI

TL;DR: 本文提出RecLLM-R1框架，利用大语言模型(LLM)解决推荐系统中的过滤气泡、外部知识利用不足及模型优化与业务策略迭代脱节问题。通过两阶段训练（监督微调+强化学习）和思维链机制，显著提升推荐准确性、多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统存在三大局限：过滤气泡效应、外部知识利用不足、模型优化与业务策略迭代脱节。这些缺陷制约了推荐系统的综合性能与业务适配性。

Method: 1) 将用户画像、历史交互和多维物品属性转化为LLM可理解的自然语言提示\n2) 两阶段训练：监督微调(SFT)打基础→强化学习(GRPO)结合思维链(CoT)进行多步推理\n3) 通过自定义奖励函数同步优化准确性、多样性等业务目标

Result: 在大型社交媒体真实用户行为数据集上，RecLLM-R1在准确性、多样性、新颖性等指标全面超越基线方法，有效缓解过滤气泡效应。

Conclusion: 该框架为复杂业务目标下推荐模型与策略的协同优化提供了可行路径，证实LLM在解决推荐系统核心挑战方面的巨大潜力。

Abstract: Traditional recommendation systems often grapple with "filter bubbles",
underutilization of external knowledge, and a disconnect between model
optimization and business policy iteration. To address these limitations, this
paper introduces RecLLM-R1, a novel recommendation framework leveraging Large
Language Models (LLMs) and drawing inspiration from the DeepSeek R1
methodology. The framework initiates by transforming user profiles, historical
interactions, and multi-faceted item attributes into LLM-interpretable natural
language prompts through a carefully engineered data construction process.
Subsequently, a two-stage training paradigm is employed: the initial stage
involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental
recommendation capabilities. The subsequent stage utilizes Group Relative
Policy Optimization (GRPO), a reinforcement learning technique, augmented with
a Chain-of-Thought (CoT) mechanism. This stage guides the model through
multi-step reasoning and holistic decision-making via a flexibly defined reward
function, aiming to concurrently optimize recommendation accuracy, diversity,
and other bespoke business objectives. Empirical evaluations on a real-world
user behavior dataset from a large-scale social media platform demonstrate that
RecLLM-R1 significantly surpasses existing baseline methods across a spectrum
of evaluation metrics, including accuracy, diversity, and novelty. It
effectively mitigates the filter bubble effect and presents a promising avenue
for the integrated optimization of recommendation models and policies under
intricate business goals.

</details>


### [76] [Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach](https://arxiv.org/abs/2506.19280)
*Feiting Yang,Antoine Moevus,Steve Lévesque*

Main category: cs.AI

TL;DR: 本文探讨了将情绪检测技术集成到日历应用中的两种方法：基于生物特征的心率数据分析和基于行为的计算机活动分析，结果显示后者在准确性和一致性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 人机交互（HCI）通过情绪识别能力的发展，为自适应和个性化用户体验提供了新机遇。本研究旨在通过情绪检测技术提升日历应用的用户界面动态响应能力，从而提高用户的生产力和参与度。

Method: 研究提出了两种情绪检测方法：1）基于生物特征的方法，利用LSTM和GRU神经网络处理心电图（ECG）信号中的心率（HR）数据，预测情绪维度（效价、唤醒度和支配度）；2）基于行为的方法，通过机器学习模型分析用户的计算机活动（如鼠标移动、点击和键盘输入模式）来分类情绪。

Result: 实验结果表明，两种方法均有效，但基于计算机活动的方法在一致性和准确性上表现更优，尤其是鼠标相关交互的准确率约为90%。在生物特征方法中，GRU网络优于LSTM模型，效价预测准确率达到84.38%。

Conclusion: 基于行为的情绪检测方法在日历应用中表现出更高的实用性和准确性，为未来人机交互系统的情绪感知功能设计提供了重要参考。

Abstract: Human-Computer Interaction (HCI) has evolved significantly to incorporate
emotion recognition capabilities, creating unprecedented opportunities for
adaptive and personalized user experiences. This paper explores the integration
of emotion detection into calendar applications, enabling user interfaces to
dynamically respond to users' emotional states and stress levels, thereby
enhancing both productivity and engagement. We present and evaluate two
complementary approaches to emotion detection: a biometric-based method
utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals
processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
neural networks to predict the emotional dimensions of Valence, Arousal, and
Dominance; and a behavioral method analyzing computer activity through multiple
machine learning models to classify emotions based on fine-grained user
interactions such as mouse movements, clicks, and keystroke patterns. Our
comparative analysis, from real-world datasets, reveals that while both
approaches demonstrate effectiveness, the computer activity-based method
delivers superior consistency and accuracy, particularly for mouse-related
interactions, which achieved approximately 90\% accuracy. Furthermore, GRU
networks outperformed LSTM models in the biometric approach, with Valence
prediction reaching 84.38\% accuracy.

</details>


### [77] [Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize Framework for Predictive Maintenance](https://arxiv.org/abs/2506.19698)
*Zhuojun Xie,Adam Abdin,Yiping Fang*

Main category: cs.AI

TL;DR: 本文提出了一种集成估计-优化（IEO）框架，通过联合调整预测模型并直接优化维护结果，解决了预测性维护（PdM）中模型错误设定导致的决策不一致问题，实证显示平均维护后悔值降低达22%。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在预测性维护中的应用日益增多，但模型错误设定带来的不确定性限制了工业界的广泛采用。研究旨在解决预测误差如何影响维护决策，并提出更稳健的解决方案。

Method: 研究对比了传统估计后优化（ETO）框架与提出的集成估计-优化（IEO）框架。IEO通过联合优化预测模型和维护目标，采用随机扰动梯度下降算法，适用于小样本失效数据集。

Result: 在涡扇发动机维护案例中，IEO框架比ETO平均减少22%的维护后悔值。理论分析证明了在标准假设下决策一致性的有限样本保证。

Conclusion: IEO框架通过将预测模型训练与维护目标对齐，显著提升了模型错误设定下的决策质量，尤其在决策策略与目标不一致时效果更明显，为不确定环境下的可靠维护规划提供了理论支持。

Abstract: Recent research increasingly integrates machine learning (ML) into predictive
maintenance (PdM) to reduce operational and maintenance costs in data-rich
operational settings. However, uncertainty due to model misspecification
continues to limit widespread industrial adoption. This paper proposes a PdM
framework in which sensor-driven prognostics inform decision-making under
economic trade-offs within a finite decision space. We investigate two key
questions: (1) Does higher predictive accuracy necessarily lead to better
maintenance decisions? (2) If not, how can the impact of prediction errors on
downstream maintenance decisions be mitigated? We first demonstrate that in the
traditional estimate-then-optimize (ETO) framework, errors in probabilistic
prediction can result in inconsistent and suboptimal maintenance decisions. To
address this, we propose an integrated estimate-optimize (IEO) framework that
jointly tunes predictive models while directly optimizing for maintenance
outcomes. We establish theoretical finite-sample guarantees on decision
consistency under standard assumptions. Specifically, we develop a stochastic
perturbation gradient descent algorithm suitable for small run-to-failure
datasets. Empirical evaluations on a turbofan maintenance case study show that
the IEO framework reduces average maintenance regret up to 22% compared to ETO.
This study provides a principled approach to managing prediction errors in
data-driven PdM. By aligning prognostic model training with maintenance
objectives, the IEO framework improves robustness under model misspecification
and improves decision quality. The improvement is particularly pronounced when
the decision-making policy is misaligned with the decision-maker's target.
These findings support more reliable maintenance planning in uncertain
operational environments.

</details>


### [78] [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs](https://arxiv.org/abs/2506.19290)
*Liang Zeng,Yongcong Li,Yuzhen Xiao,Changshi Li,Chris Yuhao Liu,Rui Yan,Tianwen Wei,Jujie He,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种自动化数据整理流程，用于扩展软件工程(SWE)数据集，并训练出性能持续提升的Skywork-SWE模型，在SWE-bench基准测试中创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 当前SWE领域的数据集构建依赖人工标注和运行环境配置，导致规模受限（通常仅数千个实例），无法满足LLM智能体在持续迭代问题解决和长上下文依赖处理方面的需求。

Method: 开发自动化数据整理管道，从2,531个GitHub仓库收集10,169个Python任务实例，每个实例包含自然语言任务描述和专用运行时环境镜像，并基于8,000条已验证的训练轨迹微调Skywork-SWE模型。

Result: 模型性能随数据量增加持续提升（未出现饱和），在SWE-bench基准测试中达到38.0% pass@1准确率（不使用验证器或多轮测试）。结合测试时扩展技术后，性能进一步提升至47.0%，刷新了32B参数以下模型的SOTA记录。

Conclusion: 自动化数据整理能有效扩大SWE数据集规模，基于此训练的Skywork-SWE-32B模型展现出卓越性能，其公开将推动未来研究。实验证实LLM的软件工程能力会随训练数据量持续提升。

Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.

</details>


### [79] [FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring](https://arxiv.org/abs/2506.19325)
*Hyein Seo,Taewook Hwang,Yohan Lee,sangkeun Jung*

Main category: cs.AI

TL;DR: 本研究提出FEAT框架，通过三种互补数据集（DM、DG、DA）高效生成英语教育辅导中的教师反馈，实验表明少量高质量数据（DM）可显著提升生成反馈质量。


<details>
  <summary>Details</summary>
Motivation: AI辅导系统需要高质量教师反馈数据，但人工生成成本高昂。研究旨在开发一种成本效益高的反馈生成框架。

Method: 构建三种数据集：1) DM（人工+LLM协作生成高质量数据）；2) DG（纯LLM生成低成本数据）；3) DA（DG为主，混入少量DM提升质量）。

Result: 实验显示：DG中混入5-10%的DM数据后，性能优于使用100%纯DM数据。

Conclusion: FEAT框架通过混合少量高质量数据，能以低成本生成优质教师反馈，为AI教育系统提供可行解决方案。

Abstract: In English education tutoring, teacher feedback is essential for guiding
students. Recently, AI-based tutoring systems have emerged to assist teachers;
however, these systems require high-quality and large-scale teacher feedback
data, which is both time-consuming and costly to generate manually. In this
study, we propose FEAT, a cost-effective framework for generating teacher
feedback, and have constructed three complementary datasets: (1) DIRECT-Manual
(DM), where both humans and large language models (LLMs) collaboratively
generate high-quality teacher feedback, albeit at a higher cost; (2)
DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower
quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small
portion of DM added to enhance quality while maintaining cost-efficiency.
Experimental results showed that incorporating a small portion of DM (5-10%)
into DG leads to superior performance compared to using 100% DM alone.

</details>


### [80] [Evolutionary Level Repair](https://arxiv.org/abs/2506.19359)
*Debosmita Bhaumik,Julian Togelius,Georgios N. Yannakakis,Ahmed Khalifa*

Main category: cs.AI

TL;DR: 本文提出了一种基于搜索的游戏关卡修复方法，结合进化算法和多样性质量算法，有效修复由机器学习生成的风格一致但常存在缺陷的关卡。


<details>
  <summary>Details</summary>
Motivation: 解决游戏关卡设计中的功能性问题，如关卡完整性、对象可达性等，同时限制修复过程中的改动次数。

Method: 采用基于搜索的解决方案，特别是进化算法和多样性质量算法，对机器学习生成的关卡进行修复。

Result: 该方法在修复由机器学习生成的风格一致但常存在缺陷的关卡方面表现出色。

Conclusion: 结合机器学习生成和基于搜索的修复方法，展现出作为混合程序化内容生成方法的巨大潜力。

Abstract: We address the problem of game level repair, which consists of taking a
designed but non-functional game level and making it functional. This might
consist of ensuring the completeness of the level, reachability of objects, or
other performance characteristics. The repair problem may also be constrained
in that it can only make a small number of changes to the level. We investigate
search-based solutions to the level repair problem, particularly using
evolutionary and quality-diversity algorithms, with good results. This level
repair method is applied to levels generated using a machine learning-based
procedural content generation (PCGML) method that generates stylistically
appropriate but frequently broken levels. This combination of PCGML for
generation and search-based methods for repair shows great promise as a hybrid
procedural content generation (PCG) method.

</details>


### [81] [Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics](https://arxiv.org/abs/2506.19385)
*Ziqi Zhu,Tao Hu,Honglong Zhang,Dan Yang,HanGeng Chen,Mengran Zhang,Xilun Chen*

Main category: cs.AI

TL;DR: CID-GraphRAG是一种新型对话系统框架，通过动态意图转移图和双检索机制，显著提升了多轮客服对话的上下文连贯性和目标导向性。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统在多轮客服对话中难以同时保持上下文连贯和目标导向，CID-GraphRAG旨在解决这一局限性。

Method: CID-GraphRAG构建动态意图转移图，并采用双检索机制，结合意图图遍历和语义搜索，以平衡意图流模式和上下文语义。

Result: 实验表明，CID-GraphRAG在自动指标和LLM评估中均显著优于传统方法，BLEU提升11%，ROUGE-L提升5%，METEOR提升6%，响应质量提升58%。

Conclusion: CID-GraphRAG通过整合意图转移结构和语义检索，实现了协同效应，有效解决了知识密集型多轮对话中的挑战。

Abstract: We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval
Augmented Generation), a novel framework that addresses the limitations of
existing dialogue systems in maintaining both contextual coherence and
goal-oriented progression in multi-turn customer service conversations. Unlike
traditional RAG systems that rely solely on semantic similarity (Conversation
RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic
intent transition graphs from goal achieved historical dialogues and implements
a dual-retrieval mechanism that adaptively balances intent-based graph
traversal with semantic search. This approach enables the system to
simultaneously leverage both conversional intent flow patterns and contextual
semantics, significantly improving retrieval quality and response quality. In
extensive experiments on real-world customer service dialogues, we employ both
automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG
significantly outperforms both semantic-based Conversation RAG and intent-based
GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG
demonstrates substantial improvements over Conversation RAG across automatic
metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and
most notably, a 58% improvement in response quality according to LLM-as-judge
evaluations. These results demonstrate that the integration of intent
transition structures with semantic retrieval creates a synergistic effect that
neither approach achieves independently, establishing CID-GraphRAG as an
effective framework for addressing the challenges of maintaining contextual
coherence and goal-oriented progression in knowledge-intensive multi-turn
dialogues.

</details>


### [82] [Is an object-centric representation beneficial for robotic manipulation ?](https://arxiv.org/abs/2506.19408)
*Alexandre Chapin,Emmanuel Dellandrea,Liming Chen*

Main category: cs.AI

TL;DR: 本文探讨了目标中心表示（OCR）在机器人操作任务中的潜力，通过模拟环境中的多对象交互场景，比较了OCR方法与整体表示方法的性能差异。


<details>
  <summary>Details</summary>
Motivation: 目标中心表示（OCR）被认为能提升数据效率和泛化能力，但现有研究多局限于场景分解，缺乏对学习表示推理能力的评估。机器人操作任务因涉及多对象交互，成为验证OCR潜力的理想场景。

Method: 研究创建了多个模拟机器人操作任务，包含多对象环境和高随机化（位置、颜色、形状等），并评估一种经典OCR方法在多种泛化场景下的表现，与先进整体表示方法进行对比。

Result: 实验结果表明，现有方法在复杂场景结构中容易失败，而OCR方法能有效克服这些挑战。

Conclusion: 目标中心表示在复杂机器人操作任务中展现出优于整体表示的泛化能力，为未来研究提供了重要方向。

Abstract: Object-centric representation (OCR) has recently become a subject of interest
in the computer vision community for learning a structured representation of
images and videos. It has been several times presented as a potential way to
improve data-efficiency and generalization capabilities to learn an agent on
downstream tasks. However, most existing work only evaluates such models on
scene decomposition, without any notion of reasoning over the learned
representation. Robotic manipulation tasks generally involve multi-object
environments with potential inter-object interaction. We thus argue that they
are a very interesting playground to really evaluate the potential of existing
object-centric work. To do so, we create several robotic manipulation tasks in
simulated environments involving multiple objects (several distractors, the
robot, etc.) and a high-level of randomization (object positions, colors,
shapes, background, initial positions, etc.). We then evaluate one classical
object-centric method across several generalization scenarios and compare its
results against several state-of-the-art hollistic representations. Our results
exhibit that existing methods are prone to failure in difficult scenarios
involving complex scene structures, whereas object-centric methods help
overcome these challenges.

</details>


### [83] [Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification](https://arxiv.org/abs/2506.19410)
*Anas Hattay,Mayara Ayat,Fred Ngole Mboula*

Main category: cs.AI

TL;DR: 本文提出了一种名为U-DaDiL的无监督学习方法，用于坐姿识别的鲁棒聚类，通过Wasserstein重心表示解决数据集间的分布对齐问题，在Office31数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多样数据集上适应性差且存在域偏移问题，U-DaDiL旨在解决这些挑战，提升无监督坐姿识别的鲁棒性。

Method: 采用基于Wasserstein重心的表示方法，实现不同数据集间的分布对齐，进行完全无监督的鲁棒聚类。

Result: 在Office31数据集上的实验表明，该方法显著提高了聚类对齐的准确性。

Conclusion: U-DaDiL为无监督坐姿识别中的域偏移和鲁棒聚类问题提供了有效的解决方案，展现了良好的应用前景。

Abstract: This paper introduces a novel approach, Unsupervised Dataset Dictionary
Learning (U-DaDiL), for totally unsupervised robust clustering applied to
sitting posture identification. Traditional methods often lack adaptability to
diverse datasets and suffer from domain shift issues. U-DaDiL addresses these
challenges by aligning distributions from different datasets using Wasserstein
barycenter based representation. Experimental evaluations on the Office31
dataset demonstrate significant improvements in cluster alignment accuracy.
This work also presents a promising step for addressing domain shift and robust
clustering for unsupervised sitting posture identification

</details>


### [84] [Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.19420)
*Yazhou Zhang,Chunwang Zou,Bo Wang,Jing Qin*

Main category: cs.AI

TL;DR: 本文提出Commander-GPT框架，通过军事指挥理论启发的模块化决策路由机制，协调多个专用LLM代理完成多模态讽刺理解任务，在MMSD基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型(LLM)在多数NLP任务表现优异，但现有证据表明其难以理解讽刺这种高阶认知任务。

Method: 采用军事指挥理论框架：1) 部署专注子任务的特化代理(如上下文建模、情感分析)；2) 设计三类中央指挥官(轻量编码器、自回归小模型、大模型零样本路由)；3) 通过信息整合实现最终讽刺判断。

Result: 在MMSD和MMSD 2.0基准测试中，相比现有最优模型平均提升F1分数4.4%和11.7%，验证了五类提示策略的有效性。

Conclusion: 模块化任务路由框架能有效弥补单一LLM的认知局限，为复杂多模态语义理解提供新范式。

Abstract: Multimodal sarcasm understanding is a high-order cognitive task. Although
large language models (LLMs) have shown impressive performance on many
downstream NLP tasks, growing evidence suggests that they struggle with sarcasm
understanding. In this paper, we propose Commander-GPT, a modular decision
routing framework inspired by military command theory. Rather than relying on a
single LLM's capability, Commander-GPT orchestrates a team of specialized LLM
agents where each agent will be selectively assigned to a focused sub-task such
as context modeling, sentiment analysis, etc. Their outputs are then routed
back to the commander, which integrates the information and performs the final
sarcasm judgment. To coordinate these agents, we introduce three types of
centralized commanders: (1) a trained lightweight encoder-based commander
(e.g., multi-modal BERT); (2) four small autoregressive language models,
serving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large
LLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output
aggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate
Commander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting
strategies. Experimental results show that our framework achieves 4.4% and
11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on
average, demonstrating its effectiveness.

</details>


### [85] [KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models](https://arxiv.org/abs/2506.19466)
*Cheng Li,Jiexiong Liu,Yixuan Chen,Qihang Zhou,KunLun Meta*

Main category: cs.AI

TL;DR: 本文提出KunLunBaizeRAG框架，通过强化学习增强大语言模型在复杂多跳问答任务中的推理能力，解决了传统RAG的检索漂移、信息冗余和策略僵化等问题。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)在复杂推理任务中存在检索漂移、信息冗余和策略僵化等局限性，需要新的方法提升大语言模型的推理性能。

Method: 框架包含四大创新机制：RAG驱动的推理对齐(RDRA)、搜索-思考迭代增强(STIE)、网络-本地智能路由(NLR)，以及渐进式混合训练策略。

Result: 实验表明，该框架在四个基准测试中的精确匹配(EM)和LLM评判得分(LJ)均有显著提升，验证了其在复杂推理场景中的鲁棒性和有效性。

Conclusion: KunLunBaizeRAG框架通过创新机制有效提升了LLM的复杂推理能力，为多跳问答任务提供了可靠的解决方案。

Abstract: This paper introduces KunLunBaizeRAG, a reinforcement learning-driven
reasoning framework designed to enhance the reasoning capabilities of large
language models (LLMs) in complex multi-hop question-answering tasks. The
framework addresses key limitations of traditional RAG, such as retrieval
drift, information redundancy, and strategy rigidity. Key innovations include
the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative
Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)
mechanism, and a progressive hybrid training strategy. Experimental results
demonstrate significant improvements in exact match (EM) and LLM-judged score
(LJ) across four benchmarks, highlighting the framework's robustness and
effectiveness in complex reasoning scenarios.

</details>


### [86] [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
*Yan Jiang,Hao Zhou,LiZhong GU,Ai Han,TianLong Li*

Main category: cs.AI

TL;DR: NaviAgent是一种基于图导航的双层规划架构，通过多路径决策器和图编码导航器提升大模型在复杂工具链调用中的鲁棒性和效率，显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖静态知识且工具调用脆弱，难以协调复杂异构工具链，现有方法存在单路径执行僵化、错误恢复差及搜索空间爆炸问题。

Method: 提出双层架构：1)多路径决策器通过四维决策空间动态选择最优动作；2)图编码导航器构建工具依赖异构图(TDHG)，融合API模式与历史调用行为，采用启发式搜索策略指导工具链组合。

Result: 在Qwen2.5-14B/32B和Deepseek-V3上任务成功率(TSR)分别超越基线13.5%/16.4%/19.0%，优化后的14B模型(49.5%TSR)优于32B模型(44.9%)，图导航器平均提升2.4个TSR点(复杂任务最高+9点)。

Conclusion: NaviAgent通过动态决策与图结构知识融合，实现了质量与效率的平衡，其架构设计对提升大模型工具链编排能力具有普适性价值。

Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

</details>


### [87] [NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons](https://arxiv.org/abs/2506.19530)
*Carlo Romeo,Andrew D. Bagdanov*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的D&D战斗遭遇生成方法NTRL，通过动态难度调整自动化设计战斗，显著提升战斗时长与策略深度，同时保持高胜率与游戏公平性。


<details>
  <summary>Details</summary>
Motivation: 传统D&D战斗中，地下城主(DM)需手动平衡队伍实力、敌人配置与动态互动，易打断叙事流程。NTRL旨在通过自动化动态难度调整解决这一复杂问题。

Method: 将问题建模为上下文赌博机，基于实时队伍属性生成战斗遭遇。相比传统DM启发式方法，NTRL通过迭代优化实现动态难度调整。

Result: NTRL使战斗时长延长200%，队伍成员承受伤害增加（战后生命值降低16.67%），玩家死亡次数上升但保持低团灭率。生成遭遇战在保证70%胜率的同时，显著提升战术深度。

Conclusion: NTRL在战略深度与难度提升方面优于人工设计的遭遇战，通过强化学习实现了既具挑战性又公平的D&D战斗体验。

Abstract: Balancing combat encounters in Dungeons & Dragons (D&D) is a complex task
that requires Dungeon Masters (DM) to manually assess party strength, enemy
composition, and dynamic player interactions while avoiding interruption of the
narrative flow. In this paper, we propose Encounter Generation via
Reinforcement Learning (NTRL), a novel approach that automates Dynamic
Difficulty Adjustment (DDA) in D&D via combat encounter design. By framing the
problem as a contextual bandit, NTRL generates encounters based on real-time
party members attributes. In comparison with classic DM heuristics, NTRL
iteratively optimizes encounters to extend combat longevity (+200%), increases
damage dealt to party members, reducing post-combat hit points (-16.67%), and
raises the number of player deaths while maintaining low total party kills
(TPK). The intensification of combat forces players to act wisely and engage in
tactical maneuvers, even though the generated encounters guarantee high win
rates (70%). Even in comparison with encounters designed by human Dungeon
Masters, NTRL demonstrates superior performance by enhancing the strategic
depth of combat while increasing difficulty in a manner that preserves overall
game fairness.

</details>


### [88] [Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming](https://arxiv.org/abs/2506.19573)
*Sanne Wielinga,Jesse Heyninck*

Main category: cs.AI

TL;DR: 本文提出了一种结合符号推理与机器学习的混合方法，通过FOLD-R++算法生成可解释规则来修正黑盒模型的不确定预测，在医疗数据集上显著提升了准确率和F1值。


<details>
  <summary>Details</summary>
Motivation: 高性能机器学习模型（如神经网络）缺乏可解释性限制了其在医疗等高风险领域的应用，而纯符号方法（如ASP）的预测性能又不足。需要兼具可解释性与准确性的解决方案。

Method: 采用FOLD-R++算法从ASP中提取可解释逻辑规则，与黑盒分类器结合：当模型预测不确定时，用符号规则进行修正并提供人类可读的解释。

Result: 在五个医疗数据集上的实验表明，该方法在准确率和F1分数上实现了统计学显著的提升（p<0.05）。

Conclusion: 通过融合符号推理与传统机器学习，可以在不牺牲准确性的前提下实现高可解释性，为高风险领域的决策支持提供了新思路。

Abstract: Machine learning (ML) techniques play a pivotal role in high-stakes domains
such as healthcare, where accurate predictions can greatly enhance
decision-making. However, most high-performing methods such as neural networks
and ensemble methods are often opaque, limiting trust and broader adoption. In
parallel, symbolic methods like Answer Set Programming (ASP) offer the
possibility of interpretable logical rules but do not always match the
predictive power of ML models. This paper proposes a hybrid approach that
integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML
classifiers to selectively correct uncertain predictions and provide
human-readable explanations. Experiments on five medical datasets reveal
statistically significant performance gains in accuracy and F1 score. This
study underscores the potential of combining symbolic reasoning with
conventional ML to achieve high interpretability without sacrificing accuracy.

</details>


### [89] [Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning](https://arxiv.org/abs/2506.19592)
*Harisankar Babu,Philipp Schillinger,Tamim Asfour*

Main category: cs.AI

TL;DR: TAPAS是一个多智能体框架，结合大语言模型（LLMs）与符号规划，无需手动定义环境模型即可解决复杂任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂任务中手动定义环境模型的局限性，TAPAS通过多智能体协作动态生成和调整领域模型、初始状态及目标规范。

Method: TAPAS使用基于LLM的专用智能体，通过结构化工具调用机制协作生成领域模型，并采用ReAct风格的执行智能体将动态生成的计划转化为机器人可执行的自然语言指令。

Result: TAPAS在基准规划领域和VirtualHome模拟现实环境中表现出色，展示了其强大的适应性和规划能力。

Conclusion: TAPAS框架通过多智能体协作和动态模型生成，显著提升了复杂任务解决的灵活性和效率，为机器人规划领域提供了新的解决方案。

Abstract: We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a
multi-agent framework that integrates Large Language Models (LLMs) with
symbolic planning to solve complex tasks without the need for manually defined
environment models. TAPAS employs specialized LLM-based agents that
collaboratively generate and adapt domain models, initial states, and goal
specifications as needed using structured tool-calling mechanisms. Through this
tool-based interaction, downstream agents can request modifications from
upstream agents, enabling adaptation to novel attributes and constraints
without manual domain redefinition. A ReAct (Reason+Act)-style execution agent,
coupled with natural language plan translation, bridges the gap between
dynamically generated plans and real-world robot capabilities. TAPAS
demonstrates strong performance in benchmark planning domains and in the
VirtualHome simulated real-world environment.

</details>


### [90] [ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP](https://arxiv.org/abs/2506.19608)
*Zhiyuan Wang,Bokui Chen*

Main category: cs.AI

TL;DR: 本文提出\ChordPrompt框架，通过视觉与文本提示的跨模态交互解决持续学习中的多领域适应问题，显著提升了零样本泛化和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在增量学习场景中难以跨领域保持性能，且当前提示学习方法局限于单模态和类增量场景，缺乏多领域任务增量学习的有效策略。

Method: \ChordPrompt框架引入跨模态提示机制，结合视觉与文本信息的交互；采用领域自适应文本提示，为多领域持续适应动态选择合适提示。

Result: 在多领域增量学习基准测试中，\ChordPrompt在零样本泛化和下游任务性能上均超越现有最优方法。

Conclusion: 该研究通过跨模态协同和领域自适应机制，为视觉语言模型的持续学习提供了新范式，显著提升了模型的多领域适应能力。

Abstract: Continual learning (CL) empowers pre-trained vision-language models to adapt
effectively to novel or previously underrepresented data distributions without
comprehensive retraining, enhancing their adaptability and efficiency. While
vision-language models like CLIP show great promise, they struggle to maintain
performance across domains in incremental learning scenarios. Existing prompt
learning methods face two main limitations: 1) they primarily focus on
class-incremental learning scenarios, lacking specific strategies for
multi-domain task incremental learning; 2) most current approaches employ
single-modal prompts, neglecting the potential benefits of cross-modal
information exchange. To address these challenges, we propose the \ChordPrompt
framework, which facilitates a harmonious interplay between visual and textual
prompts. \ChordPrompt introduces cross-modal prompts to leverage interactions
between visual and textual information. Our approach also employs
domain-adaptive text prompts to select appropriate prompts for continual
adaptation across multiple domains. Comprehensive experiments on multi-domain
incremental learning benchmarks demonstrate that \ChordPrompt outperforms
state-of-the-art methods in zero-shot generalization and downstream task
performance.

</details>


### [91] [Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI](https://arxiv.org/abs/2506.19613)
*Sha Zhang,Suorong Yang,Tong Xie,Xiangyuan Xue,Zixuan Hu,Rui Li,Wenxi Qu,Zhenfei Yin,Tianfan Fu,Di Hu,Andres M Bran,Nian Ran,Bram Hoex,Wangmeng Zuo,Philippe Schwaller,Wanli Ouyang,Lei Bai,Yanyong Zhang,Lingyu Duan,Shixiang Tang,Dongzhan Zhou*

Main category: cs.AI

TL;DR: 本文提出智能科学实验室（ISLs）的新范式，通过整合认知与具身智能，构建闭环系统以突破当前科学发现的局限，实现AI驱动科学的变革潜力。


<details>
  <summary>Details</summary>
Motivation: 传统科学发现受限于人类能力，现有AI科学家和自动化实验室仍存在虚拟环境局限与物理世界适应性不足的问题，需融合认知与具身智能以实现自主实验与偶然发现。

Method: 提出多层级闭环框架ISLs，整合科学推理基础模型、基于代理的工作流编排及具身代理的物理实验能力，形成认知-操作一体化系统。

Result: ISLs通过通用机器人基础模型、扩散动作策略等技术，支持迭代式自主实验，为科学发现提供灵活、自适应的解决方案。

Conclusion: ISLs范式是克服当前科学发现瓶颈的关键，其深度整合认知与具身智能的特性，将充分释放AI驱动科学的变革性潜力。

Abstract: Scientific discovery has long been constrained by human limitations in
expertise, physical capability, and sleep cycles. The recent rise of AI
scientists and automated laboratories has accelerated both the cognitive and
operational aspects of research. However, key limitations persist: AI systems
are often confined to virtual environments, while automated laboratories lack
the flexibility and autonomy to adaptively test new hypotheses in the physical
world. Recent advances in embodied AI, such as generalist robot foundation
models, diffusion-based action policies, fine-grained manipulation learning,
and sim-to-real transfer, highlight the promise of integrating cognitive and
embodied intelligence. This convergence opens the door to closed-loop systems
that support iterative, autonomous experimentation and the possibility of
serendipitous discovery. In this position paper, we propose the paradigm of
Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework
that deeply integrates cognitive and embodied intelligence. ISLs unify
foundation models for scientific reasoning, agent-based workflow orchestration,
and embodied agents for robust physical experimentation. We argue that such
systems are essential for overcoming the current limitations of scientific
discovery and for realizing the full transformative potential of AI-driven
science.

</details>


### [92] [Identifying Macro Causal Effects in C-DMGs over DMGs](https://arxiv.org/abs/2506.19650)
*Simon Ferreira,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文证明了在基于DMG的C-DMG中，do-calculus无条件适用于宏观因果效应识别，并扩展了非可识别性的图形标准。


<details>
  <summary>Details</summary>
Motivation: 现实系统中常存在循环因果结构，而传统ADMG框架无法完全适用，需要研究更通用的DMG框架下的因果效应识别方法。

Method: 通过输入-输出结构因果模型(ioSCM)构建允许循环的DMG，并研究其上的集群有向混合图(C-DMG)表示方法。

Result: 证明在DMG框架下，do-calculus对C-DMG的宏观因果效应识别具有无条件完备性，且ADMG的非可识别性标准可推广至部分DMG情形。

Conclusion: 研究扩展了do-calculus在循环因果系统中的适用性，为高维复杂系统的因果分析提供了理论工具。

Abstract: The do-calculus is a sound and complete tool for identifying causal effects
in acyclic directed mixed graphs (ADMGs) induced by structural causal models
(SCMs). However, in many real-world applications, especially in
high-dimensional setting, constructing a fully specified ADMG is often
infeasible. This limitation has led to growing interest in partially specified
causal representations, particularly through cluster-directed mixed graphs
(C-DMGs), which group variables into clusters and offer a more abstract yet
practical view of causal dependencies. While these representations can include
cycles, recent work has shown that the do-calculus remains sound and complete
for identifying macro-level causal effects in C-DMGs over ADMGs under the
assumption that all clusters size are greater than 1. Nevertheless, real-world
systems often exhibit cyclic causal dynamics at the structural level. To
account for this, input-output structural causal models (ioSCMs) have been
introduced as a generalization of SCMs that allow for cycles. ioSCMs induce
another type of graph structure known as a directed mixed graph (DMG).
Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level
representations of causal relations among clusters of variables. In this paper,
we prove that, unlike in the ADMG setting, the do-calculus is unconditionally
sound and complete for identifying macro causal effects in C-DMGs over DMGs.
Furthermore, we show that the graphical criteria for non-identifiability of
macro causal effects previously established C-DMGs over ADMGs naturally extends
to a subset of C-DMGs over DMGs.

</details>


### [93] [From memories to maps: Mechanisms of in context reinforcement learning in transformers](https://arxiv.org/abs/2506.19686)
*Ching Fang,Kanaka Rajan*

Main category: cs.AI

TL;DR: 该研究探讨了人类和动物通过情景记忆快速适应新环境的能力，并利用Transformer模型模拟这一过程，发现其学习策略不同于传统的模型无关或基于模型的规划，而是依赖于内存中的中间计算缓存。


<details>
  <summary>Details</summary>
Motivation: 人类和动物能够通过情景记忆快速适应新环境，而传统的强化学习算法无法有效模拟这一能力。研究旨在通过Transformer模型探索这一快速适应能力的机制。

Method: 研究训练了一个Transformer模型，在一个受啮齿动物行为启发的规划任务分布中进行上下文强化学习，并分析了模型中涌现的学习算法。

Result: 研究发现，模型的表示学习依赖于上下文结构学习和跨上下文对齐，且其强化学习策略无法用传统的模型无关或基于模型的规划来解释，而是依赖于内存令牌中的中间计算缓存。

Conclusion: 内存可以作为计算资源存储原始经验和缓存计算，支持灵活行为。模型中的表示与大脑海马-内嗅皮层系统的计算相似，表明研究结果可能对自然认知有重要意义。

Abstract: Humans and animals show remarkable learning efficiency, adapting to new
environments with minimal experience. This capability is not well captured by
standard reinforcement learning algorithms that rely on incremental value
updates. Rapid adaptation likely depends on episodic memory -- the ability to
retrieve specific past experiences to guide decisions in novel contexts.
Transformers provide a useful setting for studying these questions because of
their ability to learn rapidly in-context and because their key-value
architecture resembles episodic memory systems in the brain. We train a
transformer to in-context reinforcement learn in a distribution of planning
tasks inspired by rodent behavior. We then characterize the learning algorithms
that emerge in the model. We first find that representation learning is
supported by in-context structure learning and cross-context alignment, where
representations are aligned across environments with different sensory stimuli.
We next demonstrate that the reinforcement learning strategies developed by the
model are not interpretable as standard model-free or model-based planning.
Instead, we show that in-context reinforcement learning is supported by caching
intermediate computations within the model's memory tokens, which are then
accessed at decision time. Overall, we find that memory may serve as a
computational resource, storing both raw experience and cached computations to
support flexible behavior. Furthermore, the representations developed in the
model resemble computations associated with the hippocampal-entorhinal system
in the brain, suggesting that our findings may be relevant for natural
cognition. Taken together, our work offers a mechanistic hypothesis for the
rapid adaptation that underlies in-context learning in artificial and natural
settings.

</details>


### [94] [LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis](https://arxiv.org/abs/2506.19702)
*Lei Kang,Xuanshuo Fu,Oriol Ramos Terrades,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLaMA-v3模型和低秩适配技术的可信医疗文档分析平台，专注于鉴别诊断任务，在保护隐私的同时提升了诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗文档分析对临床决策至关重要，但现有大型语言模型存在隐私泄露风险，且缺乏针对鉴别诊断的优化方案。

Method: 采用低秩适配技术微调LLaMA-v3模型，利用最大鉴别诊断基准数据集DDXPlus进行训练，并开发支持用户上传非结构化文档的网页平台。

Result: 该方法在病理预测和变长鉴别诊断任务中超越现有技术，通过可解释技术确保预测透明性，临床评估证实其优越性。

Conclusion: 该研究实现了可靠、可解释且隐私保护的人工智能解决方案，为现实医疗场景中的智能文档分析提供了重要进展。代码已开源：\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}

Abstract: Medical document analysis plays a crucial role in extracting essential
clinical insights from unstructured healthcare records, supporting critical
tasks such as differential diagnosis. Determining the most probable condition
among overlapping symptoms requires precise evaluation and deep medical
expertise. While recent advancements in large language models (LLMs) have
significantly enhanced performance in medical document analysis, privacy
concerns related to sensitive patient data limit the use of online LLMs
services in clinical settings. To address these challenges, we propose a
trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using
low-rank adaptation, specifically optimized for differential diagnosis tasks.
Our approach utilizes DDXPlus, the largest benchmark dataset for differential
diagnosis, and demonstrates superior performance in pathology prediction and
variable-length differential diagnosis compared to existing methods. The
developed web-based platform allows users to submit their own unstructured
medical documents and receive accurate, explainable diagnostic results. By
incorporating advanced explainability techniques, the system ensures
transparent and reliable predictions, fostering user trust and confidence.
Extensive evaluations confirm that the proposed method surpasses current
state-of-the-art models in predictive accuracy while offering practical utility
in clinical settings. This work addresses the urgent need for reliable,
explainable, and privacy-preserving artificial intelligence solutions,
representing a significant advancement in intelligent medical document analysis
for real-world healthcare applications. The code can be found at
\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.

</details>


### [95] [From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking](https://arxiv.org/abs/2506.19724)
*Gyeongwon James Kim,Alex Wilf,Louis-Philippe Morency,Daniel Fried*

Main category: cs.AI

TL;DR: 本文介绍了AutoExperiment基准测试，用于评估AI代理根据自然语言描述实现和运行机器学习实验的能力，揭示了当前AI在长周期代码生成和自主实验执行中的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估AI代理在不同代码起点下实现科学想法的基准测试，AutoExperiment填补了这一空白，旨在衡量AI从部分复现到完全复现的能力。

Method: 通过提供研究论文、关键函数被遮蔽的代码库及实验命令，要求AI生成缺失代码并在沙盒环境中执行实验，难度随缺失函数数量$n$增加而提升。

Result: 实验显示，随着$n$增加，AI性能迅速下降；动态与环境交互的代理优于固定框架，且单次尝试与多次尝试成功率（Pass@1 vs. Pass@5）存在显著差距。

Conclusion: AutoExperiment为评估AI驱动的科学实验设立了新基准，突显了长周期代码生成、上下文检索和自主实验执行等关键挑战，相关数据和代码已开源。

Abstract: Recent progress in autonomous code generation has fueled excitement around AI
agents capable of accelerating scientific discovery by running experiments.
However, there is currently no benchmark that evaluates whether such agents can
implement scientific ideas when given varied amounts of code as a starting
point, interpolating between reproduction (running code) and from-scratch
replication (fully re-implementing and running code). We introduce
AutoExperiment, a benchmark that evaluates AI agents' ability to implement and
run machine learning experiments based on natural language descriptions in
research papers. In each task, agents are given a research paper, a codebase
with key functions masked out, and a command to run the experiment. The goal is
to generate the missing code, execute the experiment in a sandboxed
environment, and reproduce the results. AutoExperiment scales in difficulty by
varying the number of missing functions $n$, ranging from partial reproduction
to full replication. We evaluate state-of-the-art agents and find that
performance degrades rapidly as $n$ increases. Agents that can dynamically
interact with the environment (e.g. to debug their code) can outperform agents
in fixed "agentless" harnesses, and there exists a significant gap between
single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating
verifier approaches to our benchmark. Our findings highlight critical
challenges in long-horizon code generation, context retrieval, and autonomous
experiment execution, establishing AutoExperiment as a new benchmark for
evaluating progress in AI-driven scientific experimentation. Our data and code
are open-sourced at https://github.com/j1mk1m/AutoExperiment .

</details>


### [96] [Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study](https://arxiv.org/abs/2506.19773)
*Nandana Mihindukulasooriya,Niharika S. D'Souza,Faisal Chowdhury,Horst Samulowitz*

Main category: cs.AI

TL;DR: 本文通过实证研究探索了自动提示优化在知识图谱三元组抽取任务中的应用，评估了不同设置下的性能表现，发现自动优化提示可媲美人工作业并提升结果质量。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KG）构建中的三元组抽取是基础任务，但为大型语言模型（LLM）手工设计任务特定提示既耗时又脆弱。近期NLP研究采用自动提示优化来解决这一挑战，本文旨在验证该方法在三元组抽取任务中的有效性。

Method: 研究通过实验基准测试评估了六类变量：(a)提示策略、(b)LLM选择、(c)模式关系数量、(d)文本长度与多样性、(e)优化驱动指标、(f)训练测试数据集。比较了DSPy/APE/TextGrad三种优化器在SynthIE和REBEL数据集上的表现。

Result: 实证结果表明，自动提示优化技术能生成接近人工水平的合理提示。优化后的提示显著提升性能，尤其在模式复杂度增加和文本规模扩大时效果更为突出。

Conclusion: 自动提示优化可有效替代人工设计提示，为知识图谱三元组抽取任务提供高效解决方案，其优势随任务复杂性增强而愈发显著。

Abstract: A KG represents a network of entities and illustrates relationships between
them. KGs are used for various applications, including semantic search and
discovery, reasoning, decision-making, natural language processing, machine
learning, and recommendation systems. Triple (subject-relation-object)
extraction from text is the fundamental building block of KG construction and
has been widely studied, for example, in early benchmarks such as ACE 2002 to
more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs
is explored for KG construction, handcrafting reasonable task-specific prompts
for LLMs is a labour-intensive exercise and can be brittle due to subtle
changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy
generation) uses automatic prompt optimization/engineering to address this
challenge by generating optimal or near-optimal task-specific prompts given
input-output examples.
  This empirical study explores the application of automatic prompt
optimization for the triple extraction task using experimental benchmarking. We
evaluate different settings by changing (a) the prompting strategy, (b) the LLM
being used for prompt optimization and task execution, (c) the number of
canonical relations in the schema (schema complexity), (d) the length and
diversity of input text, (e) the metric used to drive the prompt optimization,
and (f) the dataset being used for training and testing. We evaluate three
different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use
two different triple extraction datasets, SynthIE and REBEL. Through rigorous
empirical evaluation, our main contribution highlights that automatic prompt
optimization techniques can generate reasonable prompts similar to humans for
triple extraction. In turn, these optimized prompts achieve improved results,
particularly with increasing schema complexity and text size.

</details>


### [97] [SAGE: Strategy-Adaptive Generation Engine for Query Rewriting](https://arxiv.org/abs/2506.19783)
*Teng Wang,Hailei Gong,Changwang Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出策略自适应生成引擎（SAGE），通过专家策略引导的强化学习框架改进密集检索中的查询重写，结合两种新型奖励机制（SCS和CRS），在多个基准测试中实现最优NDCG@10性能，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前查询重写方法依赖大规模监督数据或强化学习探索效率低下，需要更高效、可扩展且解释性强的解决方案。

Method: SAGE框架将专家策略（如语义扩展和实体消歧）融入强化学习，并设计战略信用塑造（SCS）和对比奖励塑造（CRS）机制优化学习信号。

Result: 在HotpotQA、FEVER等基准上取得SOTA的NDCG@10结果，模型能自主选择最优策略，减少冗余探索，生成简洁重写以降低推理成本。

Conclusion: 策略引导的强化学习结合精细奖励机制，为构建高效、可扩展的信息检索系统提供了新范式。

Abstract: Query rewriting is pivotal for enhancing dense retrieval, yet current methods
demand large-scale supervised data or suffer from inefficient reinforcement
learning (RL) exploration. In this work, we first establish that guiding Large
Language Models (LLMs) with a concise set of expert-crafted strategies, such as
semantic expansion and entity disambiguation, substantially improves retrieval
effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,
and SciFact. Building on this insight, we introduce the Strategy-Adaptive
Generation Engine (SAGE), which operationalizes these strategies in an RL
framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit
Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative
learning signals. This strategy-guided approach not only achieves new
state-of-the-art NDCG@10 results, but also uncovers a compelling emergent
behavior: the agent learns to select optimal strategies, reduces unnecessary
exploration, and generates concise rewrites, lowering inference cost without
sacrificing performance. Our findings demonstrate that strategy-guided RL,
enhanced with nuanced reward shaping, offers a scalable, efficient, and more
interpretable paradigm for developing the next generation of robust information
retrieval systems.

</details>


### [98] [Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning](https://arxiv.org/abs/2506.19785)
*Menglong Zhang,Fuyuan Qian*

Main category: cs.AI

TL;DR: 本文提出SimBelief框架，通过测量贝叶斯自适应MDP中的任务信念相似性，解决稀疏奖励环境下元强化学习的任务识别与探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯自适应深度强化学习方法依赖重构环境奖励信号，在稀疏奖励场景中效果不佳，需更鲁棒的任务识别方法。

Method: 引入潜在任务信念度量学习相似任务的共同结构，并将其整合到具体任务信念中，通过跨任务分布的潜在动力学连接共享特征与特定特征。

Result: 在稀疏奖励的MuJoCo和panda-gym任务上，SimBelief优于当前最先进基线方法。

Conclusion: 基于行为相似性的任务信念度量能有效提取任务分布共性，提升稀疏奖励环境下的元强化学习性能。

Abstract: Meta-reinforcement learning requires utilizing prior task distribution
information obtained during exploration to rapidly adapt to unknown tasks. The
efficiency of an agent's exploration hinges on accurately identifying the
current task. Recent Bayes-Adaptive Deep RL approaches often rely on
reconstructing the environment's reward signal, which is challenging in sparse
reward settings, leading to suboptimal exploitation. Inspired by bisimulation
metrics, which robustly extracts behavioral similarity in continuous MDPs, we
propose SimBelief-a novel meta-RL framework via measuring similarity of task
belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common
features of similar task distributions, enabling efficient task identification
and exploration in sparse reward environments. We introduce latent task belief
metric to learn the common structure of similar tasks and incorporate it into
the specific task belief. By learning the latent dynamics across task
distributions, we connect shared latent task belief features with specific task
features, facilitating rapid task identification and adaptation. Our method
outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym
tasks.

</details>


### [99] [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
*Baochang Ren,Shuofei Qiao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.AI

TL;DR: 本文提出KnowRL方法，通过知识增强的强化学习减少大语言模型在慢思考过程中的幻觉问题，保持其推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（尤其是慢思考模型）因无法准确识别知识边界而产生严重幻觉，传统强化学习的奖励机制缺乏对思考过程的事实监督，加剧了该问题。

Method: 提出KnowRL框架，在强化学习训练中融入基于知识验证的事实性奖励，引导模型进行基于事实的慢思考并识别知识边界。

Result: 在三个幻觉评估数据集和两个推理评估数据集上的实验表明，KnowRL有效减少了慢思考模型的幻觉，同时保持了其原有推理能力。

Conclusion: KnowRL通过直接奖励推理步骤中的事实遵循，培养了更可靠的思考过程，为解决大语言模型幻觉问题提供了新方向。

Abstract: Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

</details>


### [100] [Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models](https://arxiv.org/abs/2506.19825)
*Johannes Rückert,Louise Bloch,Christoph M. Friedrich*

Main category: cs.AI

TL;DR: 研究利用视觉语言模型（VLMs）自动检测图表中的可视化问题，验证了其在识别图表类型、3D效果、坐标轴标签等方面的有效性，并指出Qwen2.5VL模型和总结式提示策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于研究者常忽视数据可视化原则，导致图表传递错误或不完整信息。本研究旨在探索VLMs能否自动识别图表中的潜在问题，以提升可视化质量。

Method: 采用五种开源VLMs和五种提示策略，通过基于可视化准则设计的问题集，评估模型在图表类型、3D效果、坐标轴标签等维度的分析能力。

Result: VLMs在图表类型（F1值82.49%）、3D效果（98.55%）、图例（96.64%）等方面表现优异，但在图像质量（0.74%）和刻度标记（46.13%）上不可靠。Qwen2.5VL模型和总结式提示策略综合表现最佳。

Conclusion: VLMs可有效自动化检测图表中的常见问题（如缺失坐标轴标签、冗余3D效果），该方法可扩展至更多可视化场景，但需针对特定缺陷（如图像质量）优化模型能力。

Abstract: Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.

</details>


### [101] [Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning](https://arxiv.org/abs/2506.19843)
*Guo Li,Zixiang Xu,Wei Zhang,Yikuan Hu,Xinyu Yang,Nikolay Aristov,Mingjie Tang,Elenna R Dugundji*

Main category: cs.AI

TL;DR: 该研究通过逆向强化学习（IRL）分析船舶行为与停泊时间，开发了Temporal-IRL模型预测纽约/新泽西港特定码头的船舶序列与港口停留时间，从而有效预测港口拥堵。


<details>
  <summary>Details</summary>
Motivation: 准确预测港口拥堵对保障全球供应链可靠性至关重要，能优化运输计划、减少延误与成本，并提升供应链韧性。

Method: 研究基于历史AIS数据重建泊位调度，采用逆向强化学习确定奖励函数，并针对纽约/新泽西港某码头开发Temporal-IRL模型，学习泊位调度模式以预测船舶序列与停留时间。

Result: 利用2015年1月至2023年9月Maher码头数据训练模型，测试结果表明预测效果显著。

Conclusion: Temporal-IRL模型能有效学习泊位调度优先级，准确预测船舶港口停留时间与拥堵情况，为供应链管理提供可靠工具。

Abstract: Predicting port congestion is crucial for maintaining reliable global supply
chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand
costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely
deliveries and enhancing supply chain resilience. To achieve accurate
predictions, analyzing vessel behavior and their stay times at specific port
terminals is essential, focusing particularly on berth scheduling under various
conditions. Crucially, the model must capture and learn the underlying
priorities and patterns of berth scheduling. Berth scheduling and planning are
influenced by a range of factors, including incoming vessel size, waiting
times, and the status of vessels within the port terminal. By observing
historical Automatic Identification System (AIS) positions of vessels, we
reconstruct berth schedules, which are subsequently utilized to determine the
reward function via Inverse Reinforcement Learning (IRL). For this purpose, we
modeled a specific terminal at the Port of New York/New Jersey and developed
Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel
sequencing at the terminal and estimate vessel port stay, encompassing both
waiting and berthing times, to forecast port congestion. Utilizing data from
Maher Terminal spanning January 2015 to September 2023, we trained and tested
the model, achieving demonstrably excellent results.

</details>


### [102] [JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning](https://arxiv.org/abs/2506.19846)
*Ai Han,Junxing Hu,Pu Wei,Zhiqian Zhang,Yuhang Guo,Jiawei Lu,Zicheng Zhang*

Main category: cs.AI

TL;DR: 本文提出JoyAgents-R1框架，通过群体相对策略优化(GRPO)和自适应记忆进化机制，解决多智能体强化学习中协同效率低与训练不稳定的问题，在通用和领域特定场景中实现与小规模开源模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习(MARL)面临异质智能体协同效率低下与训练不稳定的核心挑战，需开发能平衡决策最优性与记忆能力的联合演化方法。

Method: 1) 采用节点级蒙特卡洛采样增强GRPO轨迹多样性 2) 基于边际收益选择奖励波动最大的Top-$K$采样组定向更新模型 3) 利用GRPO奖励作为免费监督信号驱动记忆进化，消除重复推理。

Result: 实验表明JoyAgents-R1在通用与领域场景中，仅需小型开源模型即可达到与大语言模型(LLMs)相当的性能，同时提升38\%训练稳定性与21\%协同收益。

Conclusion: 该框架通过GRPO联合优化与记忆演化机制，为异质多智能体系统提供了高效稳定的训练范式，验证了小型模型通过算法创新实现复杂任务的可能性。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm
for increasingly complex tasks. However, joint evolution across heterogeneous
agents remains challenging due to cooperative inefficiency and training
instability. In this paper, we propose the joint evolution dynamics for MARL
called JoyAgents-R1, which first applies Group Relative Policy Optimization
(GRPO) to the joint training of heterogeneous multi-agents. By iteratively
refining agents' large language models (LLMs) and memories, the method achieves
holistic equilibrium with optimal decision-making and memory capabilities.
Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on
the behavior of each agent across entire reasoning trajectories to enhance GRPO
sampling efficiency while maintaining policy diversity. Then, our marginal
benefit-driven selection strategy identifies top-$K$ sampling groups with
maximal reward fluctuations, enabling targeted agent model updates that improve
training stability and maximize joint benefits through cost-effective parameter
adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution
mechanism that repurposes GRPO rewards as cost-free supervisory signals to
eliminate repetitive reasoning and accelerate convergence. Experiments across
general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves
performance comparable to that of larger LLMs while built on smaller
open-source models.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [103] [Local Search Improvements for Soft Happy Colouring](https://arxiv.org/abs/2506.19284)
*Mohammad Hadi Shekarriz,Dhananjay Thiruvady,Asef Nazari,Wilfried Imrich*

Main category: cs.DM

TL;DR: 本文研究了软快乐着色问题，证明了高$\rho$值的完全快乐着色能提高社区检测精度，并提出三种局部搜索算法，其中线性时间算法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软快乐着色问题与图的社区结构密切相关，研究不同$\rho$值下的着色效果有助于理解社区检测的准确性。

Method: 通过理论分析$\rho$阈值对完全快乐着色的影响，并提出三种局部搜索算法（包括线性时间算法）进行比较。

Result: 当$\rho_2>\rho_1$时，完全$\rho_2$-快乐着色比$\rho_1$-快乐着色具有更高的社区检测精度；线性时间局部搜索算法能显著增加$\rho$-快乐顶点数量。

Conclusion: 高$\rho$值快乐着色能有效提升社区检测性能，线性时间局部搜索算法是实现高效软快乐着色的可靠方法。

Abstract: For $0\leq \rho\leq 1$ and a coloured graph $G$, a vertex $v$ is $\rho$-happy
if at least $\rho \deg(v)$ of its neighbours have the same colour as $v$. Soft
happy colouring of a partially coloured graph $G$ is the problem of finding a
vertex colouring $\sigma$ that preserves the precolouring and has the maximum
number of $\rho$-happy vertices. It is already known that this problem is
NP-hard and directly relates to the community structure of the graphs; under a
certain condition on the proportion of happiness $\rho$ and for graphs with
community structures, the induced colouring by communities can make all the
vertices $\rho$-happy. We show that when $0\leq \rho_1<\rho_2\leq 1$, a
complete $\rho_2$-happy colouring has a higher accuracy of community detection
than a complete $\rho_1$-happy colouring. Moreover, when $\rho$ is greater than
a threshold, it is unlikely for an algorithm to find a complete $\rho$-happy
colouring with colour classes of almost equal sizes. Three local search
algorithms for soft happy colouring are proposed, and their performances are
compared with one another and other known algorithms. Among them, the
linear-time local search is shown to be not only very fast, but also a reliable
algorithm that can dramatically improve the number of $\rho$-happy vertices.

</details>


### [104] [Paired Disjunctive Domination Number of Middle Graphs](https://arxiv.org/abs/2506.19529)
*Hande Tuncel Golpek,Zeliha Kartal Yildiz,Aysun Aytac*

Main category: cs.DM

TL;DR: 本文研究了中间图中成对析取支配数的性质，针对多种特殊图类（如路径图、轮图等）进行了分析，并给出了任意图中间图中该参数的下界和上界，特别关注了树结构。此外，还确定了通过连接操作得到的中间图的精确参数值。


<details>
  <summary>Details</summary>
Motivation: 图支配概念在网络理论中具有核心地位，研究中间图的成对析取支配数有助于理解变换图结构的支配性质及其组合行为。

Method: 首先分析路径图、轮图等特殊图类中间图的成对析取支配数，随后建立任意图中间图中该参数的上下界，特别针对树结构进行深入研究，并探讨连接操作生成的中间图的精确值。

Result: 确定了多种特殊图类中间图的成对析取支配数，给出了任意图中间图中该参数的通用界限，并推导出连接操作生成中间图的精确参数值。

Conclusion: 研究成果拓展了对变换图结构中支配类参数的理解，为相关组合行为提供了新的理论依据，尤其在树结构和连接操作生成的中间图方面具有显著贡献。

Abstract: The concept of domination in graphs plays a central role in understanding
structural properties and applications in network theory. In this study, we
focus on the paired disjunctive domination number in the context of middle
graphs, a transformation that captures both adjacency and incidence relations
of the original graph. We begin by investigating this parameter for middle
graphs of several special graph classes, including path graphs, cycle graphs,
wheel graphs, complete graphs, complete bipartite graphs, star graphs,
friendship graphs, and double star graphs. We then present general results by
establishing lower and upper bounds for the paired disjunctive domination
number in middle graphs of arbitrary graphs, with particular emphasis on trees.
Additionally, we determine the exact value of the parameter for middle graphs
obtained through the join operation. These findings contribute to the broader
understanding of domination-type parameters in transformed graph structures and
offer new insights into their combinatorial behavior.

</details>
